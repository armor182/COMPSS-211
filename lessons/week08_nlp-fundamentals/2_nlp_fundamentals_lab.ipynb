{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# NLP Fundamentals Lab\n",
    "\n",
    "* * * \n",
    "\n",
    "<div class=\"alert alert-success\">  \n",
    "    \n",
    "### Lab Objectives \n",
    "    \n",
    "Apply the NLP fundamentals you learned in the lesson to your own text dataset:\n",
    "* Preprocess your text data using appropriate cleaning techniques\n",
    "* Compare different tokenization approaches on your data\n",
    "* Create numerical representations using Bag-of-Words and TF-IDF\n",
    "* Build a text classifier if you have labeled data\n",
    "* Analyze and interpret your results\n",
    "</div>\n",
    "\n",
    "### Icons Used in This Lab\n",
    "üîî **Question**: A quick question to help you understand what's going on.<br>\n",
    "üí° **Tip**: How to do something a bit more efficiently or effectively.<br>\n",
    "‚ö†Ô∏è **Warning:** Heads-up about tricky stuff or common mistakes.<br>\n",
    "üìù **Your Task**: Something for you to implement or analyze.<br>\n",
    "\n",
    "### Sections\n",
    "1. [Data Import and Exploration](#section1)\n",
    "2. [Text Preprocessing](#section2)\n",
    "3. [Tokenization Comparison](#section3)\n",
    "4. [Numerical Representation](#section4)\n",
    "5. [Text Classification (Optional)](#section5)\n",
    "6. [Analysis and Reflection](#section6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install and import the necessary packages for text analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines to install packages if needed\n",
    "# %pip install NLTK\n",
    "# %pip install transformers\n",
    "# %pip install spaCy\n",
    "# %pip install scikit-learn\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from string import punctuation\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "\n",
    "# 1. Data Import and Exploration\n",
    "\n",
    "üìù **Your Task**: Import your text dataset and explore its structure.\n",
    "\n",
    "**Questions to consider:**\n",
    "- What format is your data in? (CSV, JSON, plain text files, etc.)\n",
    "- Which column(s) contain the main text you want to analyze?\n",
    "- What other information do you have (labels, categories, metadata)?\n",
    "- How many documents/texts do you have?\n",
    "- What does a typical text look like in your dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "# Example: df = pd.read_csv('your_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the structure of your data\n",
    "# Show first few rows, column info, basic statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a few example texts\n",
    "# Display 2-3 representative examples from your dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "üîî **Question**: What patterns do you notice in your text data? What preprocessing challenges might you encounter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "\n",
    "# 2. Text Preprocessing\n",
    "\n",
    "üìù **Your Task**: Clean and preprocess your text data based on its specific characteristics.\n",
    "\n",
    "**Common preprocessing steps to consider:**\n",
    "- Handle missing values (NaN, empty strings)\n",
    "- Remove or replace unwanted content (URLs, email addresses, special characters)\n",
    "- Lowercase text\n",
    "- Remove extra whitespace\n",
    "- Handle punctuation\n",
    "- Create placeholders for specific patterns in your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for and handle missing values\n",
    "# Count NaN values, empty strings, or other missing data indicators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove or filter out unusable rows\n",
    "# Remove rows with insufficient content or placeholder text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a basic preprocessing function\n",
    "def basic_preprocess(text):\n",
    "    \"\"\"\n",
    "    Apply basic preprocessing steps to text.\n",
    "    Adapt this function based on your data's characteristics.\n",
    "    \"\"\"\n",
    "    # Convert to string and lowercase\n",
    "    \n",
    "    # Remove extra whitespace using regex\n",
    "    \n",
    "    # Handle other data-specific patterns\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Test your function on an example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an advanced preprocessing function with regex patterns\n",
    "def advanced_preprocess(text):\n",
    "    \"\"\"\n",
    "    Apply domain-specific preprocessing.\n",
    "    Replace patterns specific to your dataset with placeholders.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Example patterns you might want to replace:\n",
    "    # URLs, email addresses, phone numbers, dates, etc.\n",
    "    \n",
    "    # Replace URLs with placeholder\n",
    "    # pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    # text = re.sub(pattern, ' URL ', text)\n",
    "    \n",
    "    # Replace numbers with placeholder\n",
    "    # pattern = r'\\\\d+'\n",
    "    # text = re.sub(pattern, ' NUMBER ', text)\n",
    "    \n",
    "    # Add your own domain-specific patterns here\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Test your advanced function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to your dataset\n",
    "# Create a new column with cleaned text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "\n",
    "# 3. Tokenization Comparison\n",
    "\n",
    "üìù **Your Task**: Compare different tokenization approaches on your data and understand their trade-offs.\n",
    "\n",
    "**Tokenizers to compare:**\n",
    "- NLTK (rule-based)\n",
    "- spaCy (linguistic model-based)\n",
    "- Modern transformer tokenizer (subword-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK resources if needed\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up tokenizers\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load spaCy model\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Load transformer tokenizer\n",
    "# tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Get stop words\n",
    "# stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create traditional preprocessing function (NLTK-based)\n",
    "def traditional_tokenize(text):\n",
    "    \"\"\"\n",
    "    Traditional NLP preprocessing: lowercase, remove punctuation, \n",
    "    tokenize, remove stopwords.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Lowercase\n",
    "    \n",
    "    # Remove punctuation\n",
    "    \n",
    "    # Tokenize with NLTK\n",
    "    \n",
    "    # Remove stop words\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spaCy preprocessing function\n",
    "def spacy_tokenize(text):\n",
    "    \"\"\"\n",
    "    SpaCy-based preprocessing with linguistic features.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Process with spaCy\n",
    "    \n",
    "    # Extract tokens, filter stop words, punctuation, spaces\n",
    "    \n",
    "    # Optional: use lemmatization instead of original text\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create modern tokenizer function\n",
    "def modern_tokenize(text):\n",
    "    \"\"\"\n",
    "    Modern subword tokenization.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize with transformer tokenizer\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tokenization approaches on example text\n",
    "# Pick a representative example from your dataset\n",
    "example_text = \"\"  # Replace with your example\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(example_text)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Traditional (NLTK):\")\n",
    "# Apply traditional_tokenize\n",
    "\n",
    "print(\"\\nSpaCy:\")\n",
    "# Apply spacy_tokenize\n",
    "\n",
    "print(\"\\nModern (Transformer):\")\n",
    "# Apply modern_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "üîî **Question**: What differences do you notice between the tokenization approaches? Which seems most appropriate for your data and task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply your chosen preprocessing approach to the full dataset\n",
    "# Create a column with the processed text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "\n",
    "# 4. Numerical Representation\n",
    "\n",
    "üìù **Your Task**: Convert your preprocessed text into numerical representations using Bag-of-Words and TF-IDF.\n",
    "\n",
    "**Key considerations:**\n",
    "- What vocabulary size makes sense for your dataset?\n",
    "- Should you filter out very rare or very common words?\n",
    "- How do the representations differ between Bag-of-Words and TF-IDF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Bag-of-Words representation\n",
    "# Initialize CountVectorizer with appropriate parameters\n",
    "count_vectorizer = None  # Your implementation\n",
    "\n",
    "# Fit and transform your text data\n",
    "bow_matrix = None  # Your implementation\n",
    "\n",
    "# Create DataFrame for easier inspection\n",
    "bow_df = None  # Your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the Bag-of-Words representation\n",
    "# Print matrix shape, vocabulary size, most frequent words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF representation\n",
    "# Initialize TfidfVectorizer with appropriate parameters\n",
    "tfidf_vectorizer = None  # Your implementation\n",
    "\n",
    "# Fit and transform your text data\n",
    "tfidf_matrix = None  # Your implementation\n",
    "\n",
    "# Create DataFrame for easier inspection\n",
    "tfidf_df = None  # Your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare a single document in both representations\n",
    "# Pick a document index to examine\n",
    "doc_index = 0\n",
    "\n",
    "# Show the original text\n",
    "\n",
    "# Show top words by Bag-of-Words count\n",
    "\n",
    "# Show top words by TF-IDF score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the most important words across your corpus\n",
    "# Create bar plots for top words in both representations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "üîî **Question**: How do the Bag-of-Words and TF-IDF representations differ for your data? Which seems more informative?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "\n",
    "# 5. Text Classification (Optional)\n",
    "\n",
    "üìù **Your Task**: If your dataset has labels or categories, build a text classifier.\n",
    "\n",
    "**Requirements:**\n",
    "- Your data must have a target variable (labels, categories, ratings, etc.)\n",
    "- You need at least 2 classes with reasonable representation\n",
    "- If you don't have labeled data, skip this section or create labels based on some text characteristics\n",
    "\n",
    "üí° **Tip**: If you don't have natural labels, you could create binary labels based on text length, presence of certain words, or other measurable characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if you have suitable labels for classification\n",
    "# Explore your target variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for classification\n",
    "# Filter to classes with sufficient samples, handle class imbalance if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features and target\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define X (text) and y (labels)\n",
    "X = None  # Your text data\n",
    "y = None  # Your labels\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = None  # Your implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build classification pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Create pipeline with TF-IDF and classifier\n",
    "classifier = None  # Your implementation\n",
    "\n",
    "# Train the model\n",
    "\n",
    "# Make predictions\n",
    "\n",
    "# Evaluate performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance\n",
    "# Extract and visualize the most important features for each class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on new examples\n",
    "# Create some test examples or use holdout data\n",
    "new_examples = []  # Add your test cases\n",
    "\n",
    "# Make predictions and show probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-39",
   "metadata": {},
   "source": [
    "<a id='section6'></a>\n",
    "\n",
    "# 6. Analysis and Reflection\n",
    "\n",
    "üìù **Your Task**: Reflect on your analysis and findings.\n",
    "\n",
    "**Questions to address:**\n",
    "- What insights did you gain about your text data?\n",
    "- Which preprocessing steps were most important for your dataset?\n",
    "- How well did the different approaches work?\n",
    "- What challenges did you encounter?\n",
    "- What would you do differently or explore further?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-40",
   "metadata": {},
   "source": [
    "## Summary of Findings\n",
    "\n",
    "**Dataset characteristics:**\n",
    "- [Describe your dataset: size, source, type of text]\n",
    "- [Key patterns or challenges you observed]\n",
    "\n",
    "**Preprocessing insights:**\n",
    "- [Which preprocessing steps were most important]\n",
    "- [Domain-specific challenges you addressed]\n",
    "\n",
    "**Tokenization comparison:**\n",
    "- [How different tokenizers performed on your data]\n",
    "- [Which approach you chose and why]\n",
    "\n",
    "**Numerical representation:**\n",
    "- [Differences between Bag-of-Words and TF-IDF for your data]\n",
    "- [Most informative features/words]\n",
    "\n",
    "**Classification results (if applicable):**\n",
    "- [Model performance and key predictive features]\n",
    "- [Insights about what the model learned]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-41",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "**Potential extensions:**\n",
    "- Experiment with different preprocessing approaches\n",
    "- Try other classification algorithms\n",
    "- Explore n-grams (bigrams, trigrams) in your vectorizers\n",
    "- Investigate misclassified examples\n",
    "- Compare with word embeddings or modern language models\n",
    "\n",
    "**Questions for further exploration:**\n",
    "- How would your results change with different preprocessing choices?\n",
    "- What other features could improve classification performance?\n",
    "- How do traditional methods compare to modern approaches on your data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-42",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## ‚ùó Key Takeaways\n",
    "\n",
    "* **Preprocessing is crucial**: The quality of your analysis depends heavily on appropriate text cleaning and preprocessing.\n",
    "* **Different tokenizers have trade-offs**: Rule-based, linguistic, and subword approaches each have strengths for different types of text.\n",
    "* **TF-IDF often outperforms simple counts**: By considering both term frequency and document frequency, TF-IDF typically provides more informative representations.\n",
    "* **Domain knowledge matters**: Understanding your specific text domain helps guide preprocessing decisions and interpret results.\n",
    "* **Feature analysis is valuable**: Examining which words are most predictive can provide insights about your data and task.\n",
    "* **Traditional methods are still useful**: While modern language models are powerful, classical NLP techniques remain valuable for many tasks.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kq97xdsrhf",
   "metadata": {},
   "source": [
    "## üöÄ Stretch Goals\n",
    "\n",
    "For students who complete the lab early and want to explore further:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5vtdt2xavy4",
   "metadata": {},
   "source": [
    "### 1. Experiment with N-grams\n",
    "\n",
    "Try using bigrams (two-word phrases) instead of just single words to capture more context:\n",
    "\n",
    "```python\n",
    "# Use ngram_range parameter in your vectorizer\n",
    "tfidf_bigram = TfidfVectorizer(ngram_range=(1,2), max_features=5000)\n",
    "X_bigram = tfidf_bigram.fit_transform(your_texts)\n",
    "\n",
    "# Find the most informative bigrams in your dataset\n",
    "```\n",
    "\n",
    "**Question**: Do bigrams improve your classification accuracy? Which bigrams are most meaningful for your data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ja6rah4x9fc",
   "metadata": {},
   "source": [
    "### 2. Named Entity Recognition (NER)\n",
    "\n",
    "**Challenge**: Extract structured information from unstructured text.\n",
    "\n",
    "**Tasks:**\n",
    "- Use spaCy's pre-trained NER model to identify entities (people, organizations, locations)\n",
    "- Visualize entity distributions in your corpus\n",
    "- Create a knowledge graph from extracted entities\n",
    "- Build custom entity patterns for domain-specific entities\n",
    "- Compare entity extraction across different text sources\n",
    "\n",
    "**Analysis ideas:**\n",
    "- Which entities appear most frequently?\n",
    "- How are different entity types distributed across your documents?\n",
    "- Can you find interesting co-occurrence patterns between entities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y3iumcefgj",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Extract and analyze named entities\n",
    "# import spacy\n",
    "# from collections import Counter\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# # Extract entities from your texts\n",
    "# all_entities = []\n",
    "# for text in processed_texts[:100]:  # Sample for speed\n",
    "#     doc = nlp(text)\n",
    "#     for ent in doc.ents:\n",
    "#         all_entities.append((ent.text, ent.label_))\n",
    "\n",
    "# # Analyze entity distribution\n",
    "# entity_counts = Counter(all_entities)\n",
    "# person_entities = [(ent, count) for (ent, label), count in entity_counts.items() if label == 'PERSON']\n",
    "# org_entities = [(ent, count) for (ent, label), count in entity_counts.items() if label == 'ORG']\n",
    "\n",
    "# print(f\"Top people mentioned: {person_entities[:5]}\")\n",
    "# print(f\"Top organizations: {org_entities[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k4tw5ewe4zc",
   "metadata": {},
   "source": [
    "### 3. Sentiment and Emotion Analysis\n",
    "\n",
    "**Challenge**: Go beyond classification to analyze sentiment and emotional content.\n",
    "\n",
    "**Tasks:**\n",
    "- Implement rule-based sentiment analysis using VADER or TextBlob\n",
    "- Create custom sentiment lexicons for your domain\n",
    "- Analyze sentiment distribution across different categories/labels\n",
    "- Build a sentiment classifier using your labeled data\n",
    "- Extract emotion-bearing phrases and patterns\n",
    "\n",
    "**Visualization ideas:**\n",
    "- Sentiment over time (if temporal data available)\n",
    "- Sentiment distribution by category\n",
    "- Word clouds colored by sentiment\n",
    "- Correlation between sentiment and other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cy6ph0drxog",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Basic sentiment analysis\n",
    "# from textblob import TextBlob\n",
    "# # or: from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# # Analyze sentiment for each text\n",
    "# sentiments = []\n",
    "# for text in your_texts[:100]:  # Sample for speed\n",
    "#     blob = TextBlob(text)\n",
    "#     sentiments.append(blob.sentiment.polarity)\n",
    "\n",
    "# # Visualize sentiment distribution\n",
    "# plt.hist(sentiments, bins=20, edgecolor='black')\n",
    "# plt.xlabel('Sentiment Polarity')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Sentiment Distribution in Your Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18003229",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
