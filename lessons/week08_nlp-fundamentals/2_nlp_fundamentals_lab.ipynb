{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: NLP Analysis of Reddit AITA Posts\n",
    "\n",
    "**COMPSS 211: Advanced Computing I**  \n",
    "**Time:** 90 minutes  \n",
    "**Due:** End of lab session\n",
    "\n",
    "---\n",
    "\n",
    "## Lab Overview\n",
    "\n",
    "In this lab, you'll apply the NLP techniques learned in today's lesson to analyze Reddit posts from r/AmItheAsshole (AITA). You'll explore how people describe interpersonal conflicts and uncover linguistic patterns in moral judgment discussions.\n",
    "\n",
    "### Learning Objectives\n",
    "- Apply text preprocessing pipelines to social media data\n",
    "- Use NLTK and spaCy for tokenization and text analysis\n",
    "- Create and analyze Bag-of-Words and TF-IDF representations\n",
    "- Perform basic topic analysis and visualization\n",
    "- Build a simple text classifier for Reddit posts\n",
    "\n",
    "### Deliverables\n",
    "- Completed Jupyter notebook with all exercises\n",
    "- Push your completed notebook to GitHub Classroom\n",
    "- Brief reflection (last cell) on what you learned\n",
    "\n",
    "### Grading Rubric\n",
    "- Data preprocessing pipeline (25%)\n",
    "- Tokenization and analysis (25%)\n",
    "- TF-IDF analysis (25%)\n",
    "- Visualization and interpretation (25%)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Loading (10 minutes)\n",
    "\n",
    "First, let's import the necessary libraries and load our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if needed\n",
    "# %pip install gdown wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NLP libraries\n",
    "import nltk\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data (run once)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "except:\n",
    "    # If model not found, download it\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Reddit AITA Dataset\n",
    "\n",
    "We'll download and load real Reddit data from r/AmItheAsshole, a subreddit where people post about interpersonal conflicts and ask for moral judgments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import os\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs('../../data', exist_ok=True)\n",
    "\n",
    "# Download the dataset\n",
    "file_id = \"1ct0UQ4Y4rvLYp-402Vb1C-0SXIZWEJ6h\"\n",
    "output_path = \"../../data/aita_pp.csv\"\n",
    "\n",
    "# Download if file doesn't exist\n",
    "if not os.path.exists(output_path):\n",
    "    print(\"Downloading dataset...\")\n",
    "    gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output_path, quiet=False)\n",
    "else:\n",
    "    print(\"Dataset already exists, loading...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "reddit_data = pd.read_csv(output_path)\n",
    "\n",
    "print(f\"Dataset shape: {reddit_data.shape}\")\n",
    "print(f\"\\nColumns: {list(reddit_data.columns)}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(reddit_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows to understand the data structure\n",
    "reddit_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data exploration\n",
    "print(\"Dataset Statistics:\")\n",
    "print(f\"Total posts: {len(reddit_data)}\")\n",
    "print(f\"\\nMissing values:\")\n",
    "print(reddit_data.isnull().sum())\n",
    "\n",
    "# Check for any categorical columns\n",
    "for col in reddit_data.columns:\n",
    "    if reddit_data[col].dtype == 'object':\n",
    "        unique_count = reddit_data[col].nunique()\n",
    "        if unique_count < 20:  # Only show if reasonable number of categories\n",
    "            print(f\"\\n{col} distribution:\")\n",
    "            print(reddit_data[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the main text column\n",
    "# Look for columns that might contain the main text\n",
    "text_columns = [col for col in reddit_data.columns if 'text' in col.lower() or 'body' in col.lower() or 'content' in col.lower()]\n",
    "if text_columns:\n",
    "    text_col = text_columns[0]\n",
    "    print(f\"Using '{text_col}' as the main text column\")\n",
    "else:\n",
    "    # If no obvious text column, use the first string column with long text\n",
    "    for col in reddit_data.columns:\n",
    "        if reddit_data[col].dtype == 'object':\n",
    "            avg_len = reddit_data[col].astype(str).str.len().mean()\n",
    "            if avg_len > 100:  # Assume posts are longer than 100 chars on average\n",
    "                text_col = col\n",
    "                print(f\"Using '{text_col}' as the main text column (avg length: {avg_len:.0f} chars)\")\n",
    "                break\n",
    "\n",
    "# Display sample posts\n",
    "print(\"\\nSample posts:\")\n",
    "for i in range(min(3, len(reddit_data))):\n",
    "    print(f\"\\nPost {i+1}:\")\n",
    "    print(reddit_data[text_col].iloc[i][:300] + \"...\" if len(str(reddit_data[text_col].iloc[i])) > 300 else reddit_data[text_col].iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Text Preprocessing Pipeline (20 minutes)\n",
    "\n",
    "### Exercise 1.1: Create a Comprehensive Preprocessing Function\n",
    "\n",
    "Build a preprocessing pipeline that handles the specific challenges of Reddit text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_reddit_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess Reddit post text.\n",
    "    \n",
    "    Steps to implement:\n",
    "    1. Convert to lowercase\n",
    "    2. Remove URLs (http/https links)\n",
    "    3. Remove subreddit mentions (r/subreddit)\n",
    "    4. Remove user mentions (u/username or /u/username)\n",
    "    5. Replace numbers with 'NUM' token\n",
    "    6. Remove extra whitespace\n",
    "    7. Remove special characters but keep apostrophes\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw Reddit post text\n",
    "    \n",
    "    Returns:\n",
    "        str: Preprocessed text\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handle None or non-string inputs\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Step 1: Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Step 2: Remove URLs\n",
    "    text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "    text = re.sub(r'www\\.\\S+', '', text)\n",
    "    \n",
    "    # Step 3: Remove subreddit mentions\n",
    "    text = re.sub(r'r/\\w+', '', text)\n",
    "    \n",
    "    # Step 4: Remove user mentions\n",
    "    text = re.sub(r'/u/\\w+', '', text)\n",
    "    text = re.sub(r'u/\\w+', '', text)\n",
    "    \n",
    "    # Step 5: Replace numbers with NUM token\n",
    "    text = re.sub(r'\\d+', ' NUM ', text)\n",
    "    \n",
    "    # Step 6: Remove special characters except apostrophes\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s']\", ' ', text)\n",
    "    \n",
    "    # Step 7: Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your preprocessing function\n",
    "test_text = \"Check out r/science! User u/john_doe shared this: https://example.com. It got 1500 upvotes!\"\n",
    "print(f\"Original: {test_text}\")\n",
    "print(f\"Processed: {preprocess_reddit_text(test_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to all posts\n",
    "reddit_data['processed_text'] = reddit_data[text_col].apply(preprocess_reddit_text)\n",
    "\n",
    "# Remove empty processed texts\n",
    "reddit_data = reddit_data[reddit_data['processed_text'].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "print(f\"Dataset after preprocessing: {reddit_data.shape}\")\n",
    "reddit_data[[text_col, 'processed_text']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2: Compare Tokenization Methods\n",
    "\n",
    "Compare how NLTK and spaCy tokenize Reddit posts differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def compare_tokenizers(text):\n",
    "    \"\"\"\n",
    "    Compare NLTK and spaCy tokenization.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with both tokenization results\n",
    "    \"\"\"\n",
    "    \n",
    "    # NLTK tokenization\n",
    "    nltk_tokens = word_tokenize(text)\n",
    "    \n",
    "    # spaCy tokenization\n",
    "    doc = nlp(text)\n",
    "    spacy_tokens = [token.text for token in doc]\n",
    "    \n",
    "    return {\n",
    "        'nltk': nltk_tokens,\n",
    "        'spacy': spacy_tokens,\n",
    "        'nltk_count': len(nltk_tokens),\n",
    "        'spacy_count': len(spacy_tokens)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tokenization on a sample post\n",
    "sample_post = reddit_data['processed_text'].iloc[0]\n",
    "comparison = compare_tokenizers(sample_post[:200])  # Use first 200 chars for readability\n",
    "\n",
    "print(f\"Sample text: {sample_post[:200]}...\\n\")\n",
    "print(f\"NLTK tokens ({comparison['nltk_count']}): {comparison['nltk'][:20]}...\\n\")\n",
    "print(f\"spaCy tokens ({comparison['spacy_count']}): {comparison['spacy'][:20]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Word Frequency and N-gram Analysis (20 minutes)\n",
    "\n",
    "### Exercise 2.1: Analyze Word Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def get_word_frequencies(texts, remove_stopwords=True, top_n=10):\n",
    "    \"\"\"\n",
    "    Get word frequencies from a list of texts.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of text strings\n",
    "        remove_stopwords (bool): Whether to remove stopwords\n",
    "        top_n (int): Number of top words to return\n",
    "    \n",
    "    Returns:\n",
    "        list: List of (word, frequency) tuples\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get English stopwords\n",
    "    stop_words = set(stopwords.words('english')) if remove_stopwords else set()\n",
    "    \n",
    "    # Tokenize all texts and count frequencies\n",
    "    all_words = []\n",
    "    for text in texts:\n",
    "        if pd.notna(text) and text:  # Check for valid text\n",
    "            tokens = word_tokenize(text.lower())\n",
    "            # Filter out stopwords and short words\n",
    "            words = [token for token in tokens \n",
    "                    if token not in stop_words \n",
    "                    and len(token) > 2 \n",
    "                    and token.isalpha()]\n",
    "            all_words.extend(words)\n",
    "    \n",
    "    word_freq = Counter(all_words)\n",
    "    return word_freq.most_common(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get overall word frequencies\n",
    "word_freq = get_word_frequencies(reddit_data['processed_text'].tolist(), top_n=15)\n",
    "\n",
    "print(\"Top 15 most frequent words in AITA posts:\")\n",
    "for word, freq in word_freq:\n",
    "    print(f\"  {word}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Visualize Word Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_frequencies(word_freq_list, title, color='steelblue'):\n",
    "    \"\"\"\n",
    "    Create a bar plot of word frequencies.\n",
    "    \n",
    "    Args:\n",
    "        word_freq_list (list): List of (word, frequency) tuples\n",
    "        title (str): Plot title\n",
    "        color (str): Bar color\n",
    "    \"\"\"\n",
    "    \n",
    "    words, frequencies = zip(*word_freq_list)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(range(len(words)), frequencies, color=color)\n",
    "    plt.yticks(range(len(words)), words)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot word frequencies\n",
    "plot_word_frequencies(word_freq, 'Top 15 Words in AITA Posts', color='coral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3: Create Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordcloud(texts, title, max_words=50):\n",
    "    \"\"\"\n",
    "    Create a word cloud from texts.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of text strings\n",
    "        title (str): Plot title\n",
    "        max_words (int): Maximum words in cloud\n",
    "    \"\"\"\n",
    "    \n",
    "    # Combine all texts\n",
    "    combined_text = ' '.join([text for text in texts if pd.notna(text)])\n",
    "    \n",
    "    # Create WordCloud\n",
    "    wordcloud = WordCloud(width=800, height=400, \n",
    "                          background_color='white',\n",
    "                          stopwords=set(stopwords.words('english')),\n",
    "                          max_words=max_words).generate(combined_text)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word cloud for the dataset\n",
    "# Sample if dataset is large to avoid memory issues\n",
    "sample_size = min(1000, len(reddit_data))\n",
    "sampled_texts = reddit_data['processed_text'].sample(n=sample_size, random_state=42).tolist()\n",
    "create_wordcloud(sampled_texts, 'AITA Posts Word Cloud')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.4: N-gram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams, trigrams\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def get_ngrams(texts, n=2, top_k=10):\n",
    "    \"\"\"\n",
    "    Get top n-grams from texts.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of texts\n",
    "        n (int): N-gram size (2 for bigrams, 3 for trigrams)\n",
    "        top_k (int): Number of top n-grams to return\n",
    "    \n",
    "    Returns:\n",
    "        list: List of (ngram, frequency) tuples\n",
    "    \"\"\"\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    all_ngrams = []\n",
    "    \n",
    "    for text in texts[:100]:  # Limit to first 100 texts for efficiency\n",
    "        if pd.notna(text) and text:\n",
    "            tokens = word_tokenize(text.lower())\n",
    "            # Filter tokens\n",
    "            tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "            \n",
    "            if n == 2:\n",
    "                text_ngrams = list(bigrams(tokens))\n",
    "            elif n == 3:\n",
    "                text_ngrams = list(trigrams(tokens))\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            all_ngrams.extend(text_ngrams)\n",
    "    \n",
    "    ngram_freq = Counter(all_ngrams)\n",
    "    return ngram_freq.most_common(top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get bigrams and trigrams\n",
    "bigrams_freq = get_ngrams(reddit_data['processed_text'].tolist(), n=2, top_k=10)\n",
    "trigrams_freq = get_ngrams(reddit_data['processed_text'].tolist(), n=3, top_k=10)\n",
    "\n",
    "print(\"Top 10 Bigrams:\")\n",
    "for ngram, freq in bigrams_freq:\n",
    "    print(f\"  {' '.join(ngram)}: {freq}\")\n",
    "\n",
    "print(\"\\nTop 10 Trigrams:\")\n",
    "for ngram, freq in trigrams_freq:\n",
    "    print(f\"  {' '.join(ngram)}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: TF-IDF Analysis (20 minutes)\n",
    "\n",
    "### Exercise 3.1: Create TF-IDF Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf_features(texts, max_features=100, ngram_range=(1, 2), min_df=2, max_df=0.95):\n",
    "    \"\"\"\n",
    "    Create TF-IDF features from texts.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of text strings\n",
    "        max_features (int): Maximum number of features\n",
    "        ngram_range (tuple): Range of n-grams to consider\n",
    "        min_df (int): Minimum document frequency\n",
    "        max_df (float): Maximum document frequency\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (TF-IDF matrix, vectorizer)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create and fit TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        ngram_range=ngram_range,\n",
    "        stop_words='english',\n",
    "        min_df=min_df,\n",
    "        max_df=max_df,\n",
    "        lowercase=True,\n",
    "        token_pattern=r'\\b[a-zA-Z]{2,}\\b'  # Only words with 2+ letters\n",
    "    )\n",
    "    \n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    return tfidf_matrix, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF features\n",
    "# Use a sample if dataset is large\n",
    "sample_size = min(2000, len(reddit_data))\n",
    "sample_data = reddit_data.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "\n",
    "tfidf_matrix, vectorizer = create_tfidf_features(sample_data['processed_text'].tolist())\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.todense(),\n",
    "    columns=vectorizer.get_feature_names_out(),\n",
    "    index=sample_data.index\n",
    ")\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {tfidf_df.shape}\")\n",
    "print(f\"\\nSample features: {list(tfidf_df.columns[:10])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2: Find Most Important Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_tfidf_terms(tfidf_df, top_n=15):\n",
    "    \"\"\"\n",
    "    Get terms with highest mean TF-IDF scores.\n",
    "    \n",
    "    Args:\n",
    "        tfidf_df (DataFrame): TF-IDF DataFrame\n",
    "        top_n (int): Number of top terms to return\n",
    "    \n",
    "    Returns:\n",
    "        Series: Top terms with their mean TF-IDF scores\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate mean TF-IDF scores across all documents\n",
    "    mean_tfidf = tfidf_df.mean(axis=0)\n",
    "    \n",
    "    return mean_tfidf.nlargest(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top TF-IDF terms\n",
    "top_terms = get_top_tfidf_terms(tfidf_df, top_n=15)\n",
    "\n",
    "# Visualize top terms\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_terms.sort_values().plot(kind='barh', color='darkgreen')\n",
    "plt.title('Top 15 Terms by Mean TF-IDF Score')\n",
    "plt.xlabel('Mean TF-IDF Score')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top terms by TF-IDF:\")\n",
    "for term, score in top_terms.items():\n",
    "    print(f\"  {term}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Text Classification (20 minutes)\n",
    "\n",
    "### Exercise 4.1: Create Labels for Classification\n",
    "\n",
    "Since we have Reddit posts, let's create a classification task based on post characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary classification task based on post length\n",
    "# Long posts vs short posts (this is just an example - you could use other criteria)\n",
    "sample_data['text_length'] = sample_data['processed_text'].str.len()\n",
    "median_length = sample_data['text_length'].median()\n",
    "sample_data['post_type'] = (sample_data['text_length'] > median_length).map({True: 'long_post', False: 'short_post'})\n",
    "\n",
    "print(f\"Median text length: {median_length:.0f} characters\")\n",
    "print(f\"\\nPost type distribution:\")\n",
    "print(sample_data['post_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.2: Build a Text Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(X, y, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Build and evaluate a text classifier.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Labels\n",
    "        test_size (float): Proportion of test set\n",
    "        random_state (int): Random seed\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (trained model, X_test, y_test, predictions)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Train a logistic regression classifier\n",
    "    model = LogisticRegression(max_iter=1000, random_state=random_state)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    train_accuracy = model.score(X_train, y_train)\n",
    "    test_accuracy = model.score(X_test, y_test)\n",
    "    \n",
    "    print(f\"Training Accuracy: {train_accuracy:.3f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.3f}\")\n",
    "    \n",
    "    return model, X_test, y_test, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and evaluate the classifier\n",
    "model, X_test, y_test, y_pred = build_classifier(\n",
    "    tfidf_matrix, \n",
    "    sample_data['post_type']\n",
    ")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.3: Analyze Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance(model, vectorizer, top_n=10):\n",
    "    \"\"\"\n",
    "    Get the most important features for classification.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained classifier\n",
    "        vectorizer: TF-IDF vectorizer\n",
    "        top_n (int): Number of top features to return\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with positive and negative features\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get feature names and coefficients\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    coef = model.coef_[0]\n",
    "    \n",
    "    # Create feature importance dataframe\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'coefficient': coef\n",
    "    }).sort_values('coefficient', ascending=False)\n",
    "    \n",
    "    # Get top positive and negative features\n",
    "    positive_features = feature_importance.head(top_n)[['feature', 'coefficient']].values.tolist()\n",
    "    negative_features = feature_importance.tail(top_n)[['feature', 'coefficient']].values.tolist()\n",
    "    \n",
    "    return {\n",
    "        'positive': positive_features,\n",
    "        'negative': negative_features\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and visualize feature importance\n",
    "important_features = get_feature_importance(model, vectorizer)\n",
    "\n",
    "print(\"Features most indicative of long posts:\")\n",
    "for feature, score in important_features['positive']:\n",
    "    print(f\"  {feature}: {score:.3f}\")\n",
    "\n",
    "print(\"\\nFeatures most indicative of short posts:\")\n",
    "for feature, score in important_features['negative']:\n",
    "    print(f\"  {feature}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.4: Test the Classifier on New Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_post_type(text, model, vectorizer, preprocess_func):\n",
    "    \"\"\"\n",
    "    Predict post type for new text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        model: Trained classifier\n",
    "        vectorizer: TF-IDF vectorizer\n",
    "        preprocess_func: Preprocessing function\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (predicted label, prediction probabilities)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Preprocess text\n",
    "    processed_text = preprocess_func(text)\n",
    "    \n",
    "    # Transform to TF-IDF features\n",
    "    features = vectorizer.transform([processed_text])\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(features)\n",
    "    probabilities = model.predict_proba(features)\n",
    "    \n",
    "    return prediction[0], probabilities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on new posts\n",
    "test_posts = [\n",
    "    \"AITA for not going?\",\n",
    "    \"So this is a long story but bear with me. It all started when I was in college and my roommate asked me to help with a project. I said yes initially but then realized it would take the entire weekend. The project was for a class I wasn't even in, and I had my own assignments due. But here's where it gets complicated...\",\n",
    "    \"My friend is mad at me.\",\n",
    "    \"I need some perspective on this situation. Last month, my sister planned a surprise party for our mother's 60th birthday. She asked everyone to contribute $100 for the venue and catering. I thought this was reasonable at first, but then I found out she chose the most expensive restaurant in town without consulting anyone.\"\n",
    "]\n",
    "\n",
    "for post in test_posts:\n",
    "    pred, probs = predict_post_type(post, model, vectorizer, preprocess_reddit_text)\n",
    "    print(f\"\\nPost: '{post[:50]}...'\")\n",
    "    print(f\"Predicted: {pred}\")\n",
    "    print(f\"Confidence: {max(probs):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Advanced Analysis with spaCy (10 minutes)\n",
    "\n",
    "### Exercise 5.1: Linguistic Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_linguistic_features(texts, sample_size=20):\n",
    "    \"\"\"\n",
    "    Analyze linguistic features using spaCy.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of texts\n",
    "        sample_size (int): Number of texts to sample\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of linguistic statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sample texts for efficiency\n",
    "    sampled_texts = texts[:sample_size] if len(texts) > sample_size else texts\n",
    "    \n",
    "    stats = {\n",
    "        'avg_sentence_length': [],\n",
    "        'noun_ratio': [],\n",
    "        'verb_ratio': [],\n",
    "        'adj_ratio': [],\n",
    "        'pronoun_ratio': []\n",
    "    }\n",
    "    \n",
    "    for text in sampled_texts:\n",
    "        if pd.notna(text) and len(text) > 0:\n",
    "            # Process with spaCy (limit length for efficiency)\n",
    "            doc = nlp(text[:1000])  # Process first 1000 chars\n",
    "            \n",
    "            # Count sentences\n",
    "            sentences = list(doc.sents)\n",
    "            if sentences:\n",
    "                avg_sent_len = sum(len(sent.text.split()) for sent in sentences) / len(sentences)\n",
    "                stats['avg_sentence_length'].append(avg_sent_len)\n",
    "            \n",
    "            # Count POS tags\n",
    "            pos_counts = Counter(token.pos_ for token in doc)\n",
    "            total_tokens = len(doc)\n",
    "            \n",
    "            if total_tokens > 0:\n",
    "                stats['noun_ratio'].append(pos_counts.get('NOUN', 0) / total_tokens)\n",
    "                stats['verb_ratio'].append(pos_counts.get('VERB', 0) / total_tokens)\n",
    "                stats['adj_ratio'].append(pos_counts.get('ADJ', 0) / total_tokens)\n",
    "                stats['pronoun_ratio'].append(pos_counts.get('PRON', 0) / total_tokens)\n",
    "    \n",
    "    # Calculate means\n",
    "    return {k: np.mean(v) if v else 0 for k, v in stats.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze linguistic features\n",
    "linguistic_features = analyze_linguistic_features(sample_data['processed_text'].tolist(), sample_size=50)\n",
    "\n",
    "# Create visualization\n",
    "feature_df = pd.DataFrame([linguistic_features]).T\n",
    "feature_df.columns = ['Value']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "feature_df.plot(kind='bar', color='purple', alpha=0.7)\n",
    "plt.title('Linguistic Features in AITA Posts')\n",
    "plt.ylabel('Average Value')\n",
    "plt.xlabel('Feature')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Linguistic Feature Analysis:\")\n",
    "for feature, value in linguistic_features.items():\n",
    "    print(f\"  {feature}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection and Submission (10 minutes)\n",
    "\n",
    "Complete the reflection below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Reflection\n",
    "\n",
    "**1. What was the most interesting finding from your analysis of the AITA posts?**\n",
    "\n",
    "_Your answer here_\n",
    "\n",
    "**2. Which preprocessing step had the biggest impact on your results?**\n",
    "\n",
    "_Your answer here_\n",
    "\n",
    "**3. How might you extend this analysis for a real research project about online discourse or moral judgment?**\n",
    "\n",
    "_Your answer here_\n",
    "\n",
    "**4. What challenges did you encounter with the real Reddit data and how did you solve them?**\n",
    "\n",
    "_Your answer here_\n",
    "\n",
    "**5. Did you use any AI assistance for this lab? If so, describe how and include your prompts:**\n",
    "\n",
    "_Your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus Challenges (Optional)\n",
    "\n",
    "If you finish early, try these additional challenges:\n",
    "\n",
    "1. **Sentiment Analysis**: Analyze the sentiment of AITA posts using TextBlob or VADER\n",
    "2. **Topic Modeling**: Apply LDA to discover main topics discussed in the posts\n",
    "3. **Judgment Prediction**: If the data has labels (YTA, NTA, etc.), build a classifier to predict judgments\n",
    "4. **Temporal Analysis**: If timestamps are available, analyze how language changes over time\n",
    "5. **Named Entity Recognition**: Use spaCy to extract and analyze people, places, and organizations mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space for bonus challenges\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
