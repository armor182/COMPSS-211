{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: NLP Analysis\n",
    "\n",
    "**COMPSS 211: Advanced Computing I**  \n",
    "**Time:** 90 minutes  \n",
    "**Due:** End of lab session\n",
    "\n",
    "---\n",
    "\n",
    "## Lab Overview\n",
    "\n",
    "In this lab, you'll apply the NLP techniques learned in today's lesson to analyze your own dataset.\n",
    "\n",
    "### Learning Objectives\n",
    "- Apply text preprocessing pipelines \n",
    "- Use NLTK and spaCy for tokenization and text analysis\n",
    "- Create and analyze Bag-of-Words and TF-IDF representations\n",
    "- Perform basic topic analysis and visualization\n",
    "- Build a simple text classifier \n",
    "\n",
    "### Deliverables\n",
    "- Completed Jupyter notebook with all exercises\n",
    "- Brief reflection (last cell) on what you learned\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Loading\n",
    "\n",
    "First, let's import the necessary libraries and load our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NLP libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required NLTK data (run once)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "except:\n",
    "    # If model not found, download it\n",
    "    !python -m spacy download en_core_web_sm\n",
    "    nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1e8XhmSwr81BuMs1hH1bKm4LaHh7DGCyr\n",
      "To: /Users/tomvannuenen/Library/CloudStorage/Dropbox/GitHub/DEV/COMPSS-211/data/changemyview_lg.csv\n",
      "100%|██████████| 21.8M/21.8M [00:00<00:00, 41.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "df = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (10003, 33)\n",
      "\n",
      "Columns: ['secure_media', 'thumbnail', 'over_18', 'stickied', 'media', 'ups', 'report_reasons', 'permalink', 'id', 'is_self', 'edited', 'domain', 'author', 'user_reports', 'selftext_html', 'score', 'gilded', 'num_comments', 'mod_reports', 'subreddit_id', 'url', 'retrieved_on', 'selftext', 'author_flair_css_class', 'downs', 'created_utc', 'author_flair_text', 'link_flair_css_class', 'link_flair_text', 'distinguished', 'banned_by', 'title', 'subreddit']\n",
      "\n",
      "Data types:\n",
      "secure_media               object\n",
      "thumbnail                  object\n",
      "over_18                    object\n",
      "stickied                   object\n",
      "media                     float64\n",
      "ups                       float64\n",
      "report_reasons             object\n",
      "permalink                  object\n",
      "id                         object\n",
      "is_self                    object\n",
      "edited                     object\n",
      "domain                     object\n",
      "author                     object\n",
      "user_reports               object\n",
      "selftext_html              object\n",
      "score                     float64\n",
      "gilded                    float64\n",
      "num_comments              float64\n",
      "mod_reports                object\n",
      "subreddit_id               object\n",
      "url                        object\n",
      "retrieved_on              float64\n",
      "selftext                   object\n",
      "author_flair_css_class     object\n",
      "downs                     float64\n",
      "created_utc               float64\n",
      "author_flair_text          object\n",
      "link_flair_css_class       object\n",
      "link_flair_text            object\n",
      "distinguished              object\n",
      "banned_by                 float64\n",
      "title                      object\n",
      "subreddit                  object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_col = df['PICK_YOUR_TEXT_COLUMN_NAME_HERE']  # Replace with actual text column name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Text Preprocessing Pipeline\n",
    "\n",
    "### Create a Comprehensive Preprocessing Function\n",
    "\n",
    "Build a preprocessing pipeline that handles the specific challenges of your text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text.\n",
    "    \n",
    "    Steps to implement:\n",
    "    1. Convert to lowercase\n",
    "    2. Remove URLs (http/https links)\n",
    "    3. Remove subreddit mentions (r/subreddit)\n",
    "    4. Remove user mentions (u/username or /u/username)\n",
    "    5. Replace numbers with 'NUM' token\n",
    "    6. Remove extra whitespace\n",
    "    7. Remove special characters but keep apostrophes\n",
    "    \n",
    "    Args:\n",
    "        text (str): Raw Reddit post text\n",
    "    \n",
    "    Returns:\n",
    "        str: Preprocessed text\n",
    "    \"\"\"\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your preprocessing function\n",
    "test_text = \"Check out r/science! User u/john_doe shared this: https://example.com. It got 1500 upvotes!\"\n",
    "print(f\"Original: {test_text}\")\n",
    "print(f\"Processed: {preprocess_reddit_text(test_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to your data\n",
    "df['processed_text'] = df[text_col].apply(preprocess_text)\n",
    "\n",
    "# Remove empty processed texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Tokenization Methods\n",
    "\n",
    "Compare how NLTK or spaCy tokenizes your posts differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenizer(text):\n",
    "    \"\"\"\n",
    "    Compare NLTK and spaCy tokenization.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with both tokenization results\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tokenization on a sample post\n",
    "sample_post = df['processed_text'].iloc[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Word Frequency and N-gram Analysis (20 minutes)\n",
    "\n",
    "### Analyze Word Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def get_word_frequencies(texts, remove_stopwords=True, top_n=10):\n",
    "    \"\"\"\n",
    "    Get word frequencies from a list of texts.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of text strings\n",
    "        remove_stopwords (bool): Whether to remove stopwords\n",
    "        top_n (int): Number of top words to return\n",
    "    \n",
    "    Returns:\n",
    "        list: List of (word, frequency) tuples\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get English stopwords\n",
    "    stop_words = set(stopwords.words('english')) if remove_stopwords else set()\n",
    "    \n",
    "    # Tokenize all texts and count frequencies\n",
    "\n",
    "    return #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print your  word frequencies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.2: Visualize Word Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_word_frequencies(word_freq_list, title, color='steelblue'):\n",
    "    \"\"\"\n",
    "    Create a bar plot of word frequencies.\n",
    "    \n",
    "    Args:\n",
    "        word_freq_list (list): List of (word, frequency) tuples\n",
    "        title (str): Plot title\n",
    "        color (str): Bar color\n",
    "    \"\"\"\n",
    "    \n",
    "    words, frequencies = zip(*word_freq_list)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(range(len(words)), frequencies, color=color)\n",
    "    plt.yticks(range(len(words)), words)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot word frequencies\n",
    "plot_word_frequencies(word_freq, 'Top 15 Words, color='coral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.3: Create Word Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordcloud(texts, title, max_words=50):\n",
    "    \"\"\"\n",
    "    Create a word cloud from texts.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of text strings\n",
    "        title (str): Plot title\n",
    "        max_words (int): Maximum words in cloud\n",
    "    \"\"\"\n",
    "    \n",
    "    # Combine all texts\n",
    "    combined_text = ' '.join([text for text in texts if pd.notna(text)])\n",
    "    \n",
    "    # Create WordCloud\n",
    "    wordcloud = WordCloud(width=800, height=400, \n",
    "                          background_color='white',\n",
    "                          stopwords=set(stopwords.words('english')),\n",
    "                          max_words=max_words).generate(combined_text)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word cloud for the dataset\n",
    "# Sample if dataset is large to avoid memory issues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: TF-IDF Analysis\n",
    "\n",
    "### Create TF-IDF Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf_features(texts, max_features=100, ngram_range=(1, 2), min_df=2, max_df=0.95):\n",
    "    \"\"\"\n",
    "    Create TF-IDF features from texts.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of text strings\n",
    "        max_features (int): Maximum number of features\n",
    "        ngram_range (tuple): Range of n-grams to consider\n",
    "        min_df (int): Minimum document frequency\n",
    "        max_df (float): Maximum document frequency\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (TF-IDF matrix, vectorizer)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create and fit TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        ngram_range=ngram_range,\n",
    "        stop_words='english',\n",
    "        min_df=min_df,\n",
    "        max_df=max_df,\n",
    "        lowercase=True,\n",
    "        token_pattern=r'\\b[a-zA-Z]{2,}\\b'  # Only words with 2+ letters\n",
    "    )\n",
    "    \n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "    \n",
    "    return tfidf_matrix, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF features\n",
    "# Use a sample if dataset is large\n",
    "sample_size = min(2000, len(reddit_data))\n",
    "sample_data = reddit_data.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "\n",
    "tfidf_matrix, vectorizer = create_tfidf_features(sample_data['processed_text'].tolist())\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.todense(),\n",
    "    columns=vectorizer.get_feature_names_out(),\n",
    "    index=sample_data.index\n",
    ")\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {tfidf_df.shape}\")\n",
    "print(f\"\\nSample features: {list(tfidf_df.columns[:10])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Most Important Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_tfidf_terms(tfidf_df, top_n=15):\n",
    "    \"\"\"\n",
    "    Get terms with highest mean TF-IDF scores.\n",
    "    \n",
    "    Args:\n",
    "        tfidf_df (DataFrame): TF-IDF DataFrame\n",
    "        top_n (int): Number of top terms to return\n",
    "    \n",
    "    Returns:\n",
    "        Series: Top terms with their mean TF-IDF scores\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate mean TF-IDF scores across all documents\n",
    "    mean_tfidf = tfidf_df.mean(axis=0)\n",
    "    \n",
    "    return mean_tfidf.nlargest(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top TF-IDF terms\n",
    "top_terms = get_top_tfidf_terms(tfidf_df, top_n=15)\n",
    "\n",
    "# Visualize top terms\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_terms.sort_values().plot(kind='barh', color='darkgreen')\n",
    "plt.title('Top 15 Terms by Mean TF-IDF Score')\n",
    "plt.xlabel('Mean TF-IDF Score')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top terms by TF-IDF:\")\n",
    "for term, score in top_terms.items():\n",
    "    print(f\"  {term}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Text Classification\n",
    "\n",
    "### Create Labels for Classification\n",
    "\n",
    "Since we have Reddit posts, let's create a classification task based on post characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary classification task based on post length\n",
    "# Long posts vs short posts (this is just an example - you could use other criteria)\n",
    "sample_data['text_length'] = sample_data['processed_text'].str.len()\n",
    "median_length = sample_data['text_length'].median()\n",
    "sample_data['post_type'] = (sample_data['text_length'] > median_length).map({True: 'long_post', False: 'short_post'})\n",
    "\n",
    "print(f\"Median text length: {median_length:.0f} characters\")\n",
    "print(f\"\\nPost type distribution:\")\n",
    "print(sample_data['post_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Text Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(X, y, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Build and evaluate a text classifier.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Labels\n",
    "        test_size (float): Proportion of test set\n",
    "        random_state (int): Random seed\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (trained model, X_test, y_test, predictions)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Train a logistic regression classifier\n",
    "    model = LogisticRegression(max_iter=1000, random_state=random_state)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    train_accuracy = model.score(X_train, y_train)\n",
    "    test_accuracy = model.score(X_test, y_test)\n",
    "    \n",
    "    print(f\"Training Accuracy: {train_accuracy:.3f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.3f}\")\n",
    "    \n",
    "    return model, X_test, y_test, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and evaluate the classifier\n",
    "model, X_test, y_test, y_pred = build_classifier(\n",
    "    tfidf_matrix, \n",
    "    sample_data['post_type']\n",
    ")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance(model, vectorizer, top_n=10):\n",
    "    \"\"\"\n",
    "    Get the most important features for classification.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained classifier\n",
    "        vectorizer: TF-IDF vectorizer\n",
    "        top_n (int): Number of top features to return\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary with positive and negative features\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get feature names and coefficients\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    coef = model.coef_[0]\n",
    "    \n",
    "    # Create feature importance dataframe\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'coefficient': coef\n",
    "    }).sort_values('coefficient', ascending=False)\n",
    "    \n",
    "    # Get top positive and negative features\n",
    "    positive_features = feature_importance.head(top_n)[['feature', 'coefficient']].values.tolist()\n",
    "    negative_features = feature_importance.tail(top_n)[['feature', 'coefficient']].values.tolist()\n",
    "    \n",
    "    return {\n",
    "        'positive': positive_features,\n",
    "        'negative': negative_features\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get and visualize feature importance\n",
    "important_features = get_feature_importance(model, vectorizer)\n",
    "\n",
    "print(\"Features most indicative of long posts:\")\n",
    "for feature, score in important_features['positive']:\n",
    "    print(f\"  {feature}: {score:.3f}\")\n",
    "\n",
    "print(\"\\nFeatures most indicative of short posts:\")\n",
    "for feature, score in important_features['negative']:\n",
    "    print(f\"  {feature}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Test the Classifier on New Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_post_type(text, model, vectorizer, preprocess_func):\n",
    "    \"\"\"\n",
    "    Predict post type for new text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        model: Trained classifier\n",
    "        vectorizer: TF-IDF vectorizer\n",
    "        preprocess_func: Preprocessing function\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (predicted label, prediction probabilities)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Preprocess text\n",
    "    processed_text = preprocess_func(text)\n",
    "    \n",
    "    # Transform to TF-IDF features\n",
    "    features = vectorizer.transform([processed_text])\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(features)\n",
    "    probabilities = model.predict_proba(features)\n",
    "    \n",
    "    return prediction[0], probabilities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on new posts\n",
    "test_posts = [\n",
    "    \"AITA for not going?\",\n",
    "    \"So this is a long story but bear with me. It all started when I was in college and my roommate asked me to help with a project. I said yes initially but then realized it would take the entire weekend. The project was for a class I wasn't even in, and I had my own assignments due. But here's where it gets complicated...\",\n",
    "    \"My friend is mad at me.\",\n",
    "    \"I need some perspective on this situation. Last month, my sister planned a surprise party for our mother's 60th birthday. She asked everyone to contribute $100 for the venue and catering. I thought this was reasonable at first, but then I found out she chose the most expensive restaurant in town without consulting anyone.\"\n",
    "]\n",
    "\n",
    "for post in test_posts:\n",
    "    pred, probs = predict_post_type(post, model, vectorizer, preprocess_reddit_text)\n",
    "    print(f\"\\nPost: '{post[:50]}...'\")\n",
    "    print(f\"Predicted: {pred}\")\n",
    "    print(f\"Confidence: {max(probs):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reflection and Submission\n",
    "\n",
    "Complete the reflection below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Reflection\n",
    "\n",
    "**1. What was the most interesting finding from your analysis of the AITA posts?**\n",
    "\n",
    "_Your answer here_\n",
    "\n",
    "**2. Which preprocessing step had the biggest impact on your results?**\n",
    "\n",
    "_Your answer here_\n",
    "\n",
    "**3. How might you extend this analysis for a real research project about online discourse or moral judgment?**\n",
    "\n",
    "_Your answer here_\n",
    "\n",
    "**4. What challenges did you encounter with the real Reddit data and how did you solve them?**\n",
    "\n",
    "_Your answer here_\n",
    "\n",
    "**5. Did you use any AI assistance for this lab? If so, describe how and include your prompts:**\n",
    "\n",
    "_Your answer here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🌟 Stretch Goals\n",
    "\n",
    "If you finish early, try these additional challenges:\n",
    "\n",
    "So far we’ve seen how tokenizers break text into tokens. But on Hugging Face, you can also find fine-tuned transformer models that build on tokenization to provide linguistic annotations. \n",
    "\n",
    "These annotations (like POS tags and NER labels) are not produced by the tokenizer itself — they come from models that were trained on labeled data for those tasks.\n",
    "\n",
    "1. POS tagging → Find a POS tagging model on Hugging Face (hint: try searching for “POS tagging” or use vblagoje/bert-english-uncased-finetuned-pos) and run it on your text.\n",
    "2. NER → Find a NER model (hint: try dslim/bert-base-NER) and run it on your text.\n",
    "3. Compare the outputs:\n",
    "    - How does POS tagging label the words?\n",
    "\t- What entities does the NER model detect?\n",
    "\t- How do these results differ from spaCy’s POS and NER?\n",
    "\n",
    "**💡 Reflection:** What do these annotations add on top of tokenization? How do they help structure the text for further analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space for stretch goals\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
