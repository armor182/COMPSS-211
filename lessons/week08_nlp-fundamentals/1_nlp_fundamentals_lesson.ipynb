{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# NLP Fundamentals\n",
    "\n",
    "* * * \n",
    "\n",
    "<div class=\"alert alert-success\">  \n",
    "    \n",
    "### Learning Objectives \n",
    "    \n",
    "* Learn common steps for preprocessing text data, as well as specific operations for preprocessing Reddit data.\n",
    "* Know commonly used NLP packages and what they are capable of.\n",
    "* Understand tokenizers, and how they have changed since the advent of Large Language Models.\n",
    "* Learn how to convert text data into numbers through a Bag-of-Words approach.\n",
    "* Understand the TF-IDF algorithm and how it complements the Bag-of-Words representation.\n",
    "* Implement Bag-of-Words and TF-IDF using the `sklearn` package and understand its parameter settings.\n",
    "* Use the numerical representations of text data to perform classification tasks.\n",
    "</div>\n",
    "\n",
    "### Icons Used in This Notebook\n",
    "üîî **Question**: A quick question to help you understand what's going on.<br>\n",
    "üí° **Tip**: How to do something a bit more efficiently or effectively.<br>\n",
    "‚ö†Ô∏è **Warning:** Heads-up about tricky stuff or common mistakes.<br>\n",
    "\n",
    "### Sections\n",
    "1. [Preprocessing](#section1)\n",
    "2. [Tokenization](#section2)\n",
    "3. [The Bag-of-Words Representation](#section3)\n",
    "4. [Term Frequency-Inverse Document Frequency](#section4)\n",
    "5. [Text Classification Using the TF-IDF Representation](#section5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "Today, we'll learn the building blocks for performing text analysis in Python using Reddit data from the popular subreddit r/AmItheAsshole (AITA). These techniques lie in the domain of Natural Language Processing (NLP). NLP is a field that deals with identifying and extracting patterns of language, primarily in written texts. Throughout the workshop, we'll interact with various packages for performing text analysis: starting from simple string methods to specific NLP packages, such as `nltk`, `spaCy`, and more recent ones on Large Language Models (`BERT`).\n",
    "\n",
    "Now, let's have these packages properly installed before diving into the materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines to install packages/model\n",
    "# %pip install NLTK\n",
    "# %pip install transformers\n",
    "# %pip install spaCy\n",
    "# %pip install scikit-learn\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from string import punctuation\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "\n",
    "# Preprocessing\n",
    "\n",
    "In the first part, we'll address the first step of text analysis. Our goal is to convert the raw, messy text data into a consistent format. This process is often called **preprocessing**, **text cleaning**, or **text normalization**.\n",
    "\n",
    "You'll notice that at the end of preprocessing, our data is still in a format that we can read and understand. Later in this workshop, we will begin our foray into converting the text data into a numerical representation‚Äîa format that can be more readily handled by computers. \n",
    "\n",
    "üîî **Question**: Let's pause for a minute to reflect on **your** previous experiences working on text data. \n",
    "- What is the format of the text data you have interacted with (plain text, CSV, or XML)?\n",
    "- Where does it come from (structured corpus, scraped from the web, survey data)?\n",
    "- Is it messy (i.e., is the data formatted consistently)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Common Processes\n",
    "\n",
    "Preprocessing is not something we can accomplish with a single line of code. We often start by familiarizing ourselves with the data, and along the way, we gain a clearer understanding of the granularity of preprocessing we want to apply.\n",
    "\n",
    "Typically, we begin by applying a set of commonly used processes to clean the data. These operations don't substantially alter the form or meaning of the data; they serve as a standardized procedure to reshape the data into a consistent format.\n",
    "\n",
    "The following processes, for examples, are commonly applied to preprocess English texts of various genres. These operations can be done using built-in Python functions, such as `string` methods, and Regular Expressions. \n",
    "- Lowercase the text\n",
    "- Remove punctuation marks\n",
    "- Remove extra whitespace characters\n",
    "- Remove stop words\n",
    "\n",
    "After the initial processing, we may choose to perform task-specific processes, the specifics of which often depend on the downstream task we want to perform and the nature of the text data (i.e., its stylistic and linguistic features).  \n",
    "\n",
    "Before we jump into these operations, let's take a look at our data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "### Import the Text Data\n",
    "\n",
    "The text data we'll be working with is a CSV file containing posts from the Reddit subreddit r/AmItheAsshole (AITA), where people post about moral dilemmas and ask for judgment from the community. \n",
    "\n",
    "Let's read the file `aita_top_subs.csv` into dataframe with `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Read the Reddit data\n",
    "df = pd.read_csv('../../data/aita_top_subs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the first five rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "The dataframe has one row per Reddit post. Let's examine the columns and identify the main text column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show column names and basic info\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "Let's take a look at an example post. Pick the one at index 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first post\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "üîî **Question**: What have you noticed about this formatting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8683ff",
   "metadata": {},
   "source": [
    "### Removing NaN\n",
    "\n",
    "Remember how to grab all NaN (Not a Number) values in Pandas? Let's see if our dataset has any, and remove them if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905c0744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get NaN values\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed05bda1",
   "metadata": {},
   "source": [
    "### Removing Unuseful Rows\n",
    "\n",
    "When you work with social media data, not every row will contain useful text. Sometimes you‚Äôll find entries that look unusual ‚Äî they aren‚Äôt empty, but they also aren‚Äôt really content you can analyze.\n",
    "\n",
    "Take a close look at your dataset and see if you can find these cases. Once you‚Äôve identified them, write code to remove those rows so that only meaningful text remains for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7096b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find unuseful rows\n",
    "# YOUR CODE HERE\n",
    "# Remove rows where 'selftext' is '[removed]' or '[deleted]'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "### Lowercasing\n",
    "\n",
    "In classic NLP, we often don't work in contexts where we can properly utilize all linguistic information.\n",
    "\n",
    "More often, the subsequent analysis we perform is **case-insensitive**. For instance, in frequency analysis, we want to account for various forms of the same word. Lowercasing the text data aids in this process and simplifies our analysis.\n",
    "\n",
    "We can easily achieve lowercasing with the string method [`.lower()`](https://docs.python.org/3/library/stdtypes.html#str.lower); see [documentation](https://docs.python.org/3/library/stdtypes.html#string-methods) for more useful functions.\n",
    "\n",
    "Let's apply it to an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the previous example post's text for demonstration\n",
    "# YOUR CODE HERE\n",
    "text = df['selftext'].iloc[2]\n",
    "\n",
    "# Convert it to lowercase\n",
    "# YOUR CODE HERE\n",
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "### Regex: Removing Whitespace\n",
    "\n",
    "Sometimes we might come across texts with extraneous whitespace, such as spaces, tabs, and newline characters, which is particularly common when the text is scrapped from web pages. Before we dive into the details, let's briefly introduce Regular Expressions (regex) and the `re` package. \n",
    "\n",
    "Regular expressions are a powerful way of searching for specific string patterns in large corpora. They have an infamously steep learning curve, but they can be very efficient when we get a handle on them. Many NLP packages heavily rely on regex under the hood. Regex testers, such as [regex101](https://regex101.com), are useful tools in both understanding and creating regex expressions.\n",
    "\n",
    "Our goal is not to provide a deep (or even shallow) dive into regex; instead, we want to expose you to them so that you are better prepared to do deep dives in the future!\n",
    "\n",
    "Let's look at an example Reddit post with potential whitespace issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example with extra whitespace (simulating messy Reddit data)\n",
    "messy_reddit_post = \"\"\"AITA for not going to my  sister's   wedding?\n",
    "\n",
    "So here's the situation...    My sister is getting married and I was supposed to be in the wedding party.\n",
    "But then she told me I couldn't bring my boyfriend    because \"it's family only\" which doesn't make sense.\"\"\"\n",
    "\n",
    "print(\"Original post:\")\n",
    "print(repr(messy_reddit_post))  # repr() shows the actual whitespace characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec85468e",
   "metadata": {},
   "source": [
    "First, look up what this piece of regex does: `r'\\s+'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# pattern in regex\n",
    "blankspace_pattern = r'\\s+'\n",
    "\n",
    "# Write a replacement for the pattern identified\n",
    "blankspace_repl = ' '\n",
    "\n",
    "# Replace whitespace(s) with ' '\n",
    "clean_text = re.sub(\n",
    "    pattern=blankspace_pattern,\n",
    "    repl=blankspace_repl,\n",
    "    string=messy_reddit_post\n",
    ")\n",
    "\n",
    "print(\"Cleaned text:\")\n",
    "print(repr(clean_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "### Removing Punctuation Marks\n",
    "\n",
    "Sometimes we are only interested in analyzing **alphanumeric characters** (i.e., the letters and numbers), in which case we might want to remove punctuation marks. \n",
    "\n",
    "The `string` module contains a list of predefined punctuation marks. Let's print them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in a predefined list of punctuation marks\n",
    "from string import punctuation\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    '''Remove punctuation marks in input text'''\n",
    "    \n",
    "    # Use string.punctuation to return text without punctuation\n",
    "    text_no_punct = ''.join([char for char in text if char not in punctuation])\n",
    "\n",
    "    return text_no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example Reddit post with punctuation\n",
    "reddit_example = \"AITA? My friend thinks I'm being 'dramatic' about this whole situation... What do you think?\"\n",
    "\n",
    "# Apply the function \n",
    "print(f\"No punctuation: {remove_punct(reddit_example)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b8d532",
   "metadata": {},
   "source": [
    "### Creating Placeholders\n",
    "Older NLP models (bag-of-words, TF-IDF) treated every unique word as a feature. That meant URLs, usernames, and numbers exploded the vocabulary with noisy, one-off tokens. To keep models simple and useful, we often replaced these with placeholders like URL, DIGIT, or USER."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a556ab",
   "metadata": {},
   "source": [
    "Write a function called `regex_remover` that that uses `re.sub` to:\n",
    "\n",
    "* Replace the following patterns with placeholders:\n",
    "    * URLs ‚Üí ` URL `\n",
    "    * Digits ‚Üí ` DIGIT `\n",
    "    * Subreddit mentions (r/subreddit) ‚Üí ` SUBREDDIT `\n",
    "    * User mentions (u/user or /u/user) ‚Üí ` USER `\n",
    "\n",
    "Here are some regex patterns to help you:\n",
    "- URLs: `r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+|www\\.\\S+'`\n",
    "- Digits: `r'\\d+'`\n",
    "- Subreddits: `r'r/\\w+'` \n",
    "- Users: `r'/u/\\w+|u/\\w+'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f280e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_remover(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Replace URLs, digits, subreddits, and user mentions with placeholders.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: replace URLs with ' URL '\n",
    "    pattern = # URL regex pattern\n",
    "    text = # re.sub(pattern, ' URL ', text\n",
    "\n",
    "    # Step 2: replace digits with ' DIGIT '\n",
    "    pattern = # Digit regex pattern\n",
    "    text = # re.sub(pattern, ' DIGIT ', text)\n",
    "\n",
    "    pattern = # Subreddit regex pattern\n",
    "    text = # re.sub(pattern, ' SUBREDDIT ', text)\n",
    "\n",
    "    # Step 4: replace user mentions with ' USER '\n",
    "    pattern = # User mention regex pattern\n",
    "    text = # re.sub(pattern, ' USER ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Example to test \n",
    "sample = \"Check out http://reddit.com/r/AmItheAsshole 123 times! Thanks u/username\"\n",
    "print(regex_remover(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ba598b",
   "metadata": {},
   "source": [
    "As you can see, in classical NLP, preprocessing pipelines often normalize text aggressively (lowercasing, replacing URLs, digits, usernames). In LLM-based NLP, we usually leave text as-is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "\n",
    "# Comparing Tokenizers\n",
    "\n",
    "One of the most important steps in text analysis is tokenization. This is the process of breaking a long sequence of text into word tokens. With these tokens available, we are ready to perform word-level analysis. For instance, we can filter out tokens that don't contribute to the core meaning of the text.\n",
    "\n",
    "In this section, we'll introduce how to perform tokenization using `nltk`, `spaCy`, and a Large Language Model (`bert`). The purpose is to expose you to different NLP packages, help you understand their functionalities, and demonstrate how to access key functions in each package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "### `nltk`\n",
    "\n",
    "The first package we'll be using is called **Natural Language Toolkit**, or `nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines to install these modules\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27390c60",
   "metadata": {},
   "source": [
    "NLTK is one of the earliest Python libraries for NLP. It provides simple, rule-based tools for tokenization, stemming, POS tagging, and more. Its tokenizers typically split text using whitespace, punctuation, or regex rules, which makes it easy to understand but sometimes brittle when handling messy, real-world text.\n",
    "\n",
    "NLTK's tokenizer:\n",
    "- Classic, rule/regex-based.\n",
    "- Splits on whitespace/punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word_tokenize \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Print a Reddit example\n",
    "reddit_text = \"AITA for not wanting to go to my brother's wedding? UPDATE: I talked to my family.\"\n",
    "\n",
    "# Apply the NLTK tokenizer\n",
    "nltk_tokens = word_tokenize(reddit_text)\n",
    "print(nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predefined stop words from nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Print the first 10 stopwords\n",
    "stop = stopwords.words('english')\n",
    "stop[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "### `spaCy`\n",
    "\n",
    "spaCy is a more modern NLP library designed for speed and practical use. Its tokenizer combines rules with statistical models to handle edge cases more robustly. It also integrates linguistic features out of the box, like part-of-speech tags, lemmas, dependency parses, and named entities, making it a common choice for production-level NLP workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46bcb3e",
   "metadata": {},
   "source": [
    "First, look up how to get verbatim text of tokens using SpaCy. Look up documentation, ask an LLM, Google it -- whatever you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Apply the pipeline to example Reddit text\n",
    "doc = nlp(reddit_text)\n",
    "\n",
    "# Get the verbatim texts of tokens\n",
    "spacy_tokens = [token.text for token in doc]\n",
    "\n",
    "# Compare with NLTK tokens\n",
    "print(f\"NLTK tokens: {nltk_tokens}\")\n",
    "print(f\"spaCy tokens: {spacy_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfccafd6",
   "metadata": {},
   "source": [
    "SpaCy adds linguistic knowledge (prefix/suffix rules, exceptions, POS tagging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1a4e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "rows = []\n",
    "for t in doc:\n",
    "    rows.append([\n",
    "        t.i,            # token index\n",
    "        t.text,         # surface\n",
    "        t.lemma_,       # lemma\n",
    "        t.pos_,         # coarse POS\n",
    "        t.tag_,         # fine POS\n",
    "        t.dep_,         # dependency label\n",
    "        t.head.text,    # syntactic head\n",
    "        t.is_stop,      # stopword?\n",
    "        t.shape_,       # orthographic shape\n",
    "        (t.idx, t.idx + len(t.text))  # char span\n",
    "    ])\n",
    "\n",
    "print(tabulate(rows, headers=[word for word in reddit_text.split()]))\n",
    "\n",
    "# sentence segmentation\n",
    "print(\"\\nSentences:\")\n",
    "for s in doc.sents:\n",
    "    print(\"-\", s.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da12919b",
   "metadata": {},
   "source": [
    "## BERT Tokenizer\n",
    "\n",
    "The BERT tokenizer is a subword tokenizer based on an algorithm called WordPiece. Instead of splitting text into whole words (like NLTK or spaCy), it breaks words into smaller units when the full word is not in its vocabulary. For example:\n",
    "- spaCy: [\"roommate\"]\n",
    "- BERT tokenizer: [\"room\", \"##mate\"]\n",
    "\n",
    "This design solves two problems that word-level tokenizers have:\n",
    "1. Out-of-vocabulary words: If a new word appears (‚Äúcryptomemes‚Äù), word-based tokenizers can‚Äôt handle it. Subword tokenization breaks it into familiar chunks (‚Äúcrypto‚Äù + ‚Äú##memes‚Äù).\n",
    "2. Vocabulary size: Instead of keeping hundreds of thousands of word forms, models only need ~30,000 subword units. This makes training more efficient.\n",
    "\n",
    "### bert-base-uncased tokenizer\n",
    "\n",
    "In practice, we usually don‚Äôt train tokenizers from scratch. Instead, we use a pretrained tokenizer that was released together with a model. For example, calling `AutoTokenizer.from_pretrained(\"bert-base-uncased\")` loads the WordPiece tokenizer associated with the BERT base model. \n",
    "\n",
    "This tokenizer has a fixed vocabulary of ~30k subword units that were learned during BERT‚Äôs pretraining. The important point is that the tokenizer and model are paired: the model only understands text that has been split into tokens exactly the way it saw during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6d4416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: The following code cell may take a while to run because it downloads large models.\n",
    "\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# 1) Subword tokenization (BERT WordPiece)\n",
    "tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "tokens = tok.tokenize(reddit_text)\n",
    "print(\"Subword tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "metadata": {},
   "source": [
    "## Comparing Tokenization\n",
    "\n",
    "Write **two** functions to tokenize and remove stop words from our full Reddit text data. \n",
    "\n",
    "1.\tUsing another tokenizer or preprocessing approach of their choice (e.g., spaCy, NLTK, regex, lowercasing, stopword removal, etc.)\n",
    "2.\tUsing a modern LLM tokenizer (e.g., Hugging Face AutoTokenizer or OpenAI‚Äôs tiktoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def og_preprocessor(raw_text):\n",
    "    # text ‚Üí tokenize ‚Üí whitespace ‚Üí URLS ‚Üí stopwords \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # Lowercase\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Remove punctuation\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Tokenize\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Remove stopwords\n",
    "    # YOUR CODE HERE\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "def modern_preprocessor(raw_text):\n",
    "    \n",
    "    # text ‚Üí subword tokenization\n",
    "    # YOUR CODE HERE\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2cd5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process row index 2 from df.selftext using both preprocessors\n",
    "text_example = df['selftext'].iloc[2]\n",
    "\n",
    "print(og_preprocessor(text_example))\n",
    "print(modern_preprocessor(text_example, stop))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d584130",
   "metadata": {},
   "source": [
    "üîî **Question**: What assumptions about language do these tokenizers make? What gets lost? What gets preserved?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f471cd42",
   "metadata": {},
   "source": [
    "## Apply to data\n",
    "FInally, let's apply our old-school preprocessor to our Reddit data -- we need the old approach for what we're about to do next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aebbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['selftext_clean'] = df['selftext'].apply(og_preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-41",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "\n",
    "# The Bag-of-Words Representation\n",
    "\n",
    "Now we move beyond preprocessing to converting text into numerical representations. We'll explore one of the most straightforward ways to generate a numeric representation from text: the **bag-of-words** (BoW). \n",
    "\n",
    "At the heart of the bag-of-words approach lies the assumption that the frequency of specific tokens is informative about the semantics and sentiment underlying the text.\n",
    "\n",
    "With a bag-of-words representation, we make heavy use of word frequency but not too much of word order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-50",
   "metadata": {},
   "source": [
    "## Document Term Matrix\n",
    "\n",
    "Now let's implement the idea of bag-of-words using `CountVectorizer` from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# A toy example containing four Reddit-like stories about different topics like weddings, work, and relationships\n",
    "test_reddit = ['I am planning my wedding and need help with the guest list.',\n",
    "               'My boss is being unreasonable about my work hours.',\n",
    "               'I just got engaged and am so excited for the future!',\n",
    "               'My partner and I are having communication issues.']\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform to create a DTM\n",
    "test_count = vectorizer.fit_transform(test_reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbfc78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DTM dataframe\n",
    "test_dtm = pd.DataFrame(data=test_count.todense(),\n",
    "                        columns=vectorizer.get_feature_names_out())\n",
    "test_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7b24e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the 15 most frequent words in the Reddit posts\n",
    "top_words = test_dtm.sum().sort_values(ascending=False).head(15)\n",
    "print(\"Top 15 most frequent words in Reddit posts:\")\n",
    "for word, freq in top_words.items():\n",
    "    print(f\"  {word}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-52",
   "metadata": {},
   "source": [
    "### DTM for Reddit Posts\n",
    "\n",
    "Let's create a Document Term Matrix for our reddit data, create a DTM dataframe, and grab most-frequent tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CountVectorizer object\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e3cd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DTM dataframe\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequent tokens\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-55",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "\n",
    "# Term Frequency-Inverse Document Frequency \n",
    "\n",
    "So far, we're relying on word frequency to give us information about a document. To remedy potential issues with very common words, we use a weighting scheme called **tf-idf (term frequency-inverse document frequency)**.\n",
    "\n",
    "We can create a tf-idf DTM using `sklearn`'s `TfidfVectorizer`.\n",
    "\n",
    "What happens is:\n",
    "- First, we build a vocabulary of all the tokens that appear in our dataset.\n",
    "- Each document (post) is turned into a row of numbers, one number per token in the vocabulary.\n",
    "- These numbers are TF-IDF scores: they tell us which words are important within that post.\n",
    "- Result = a big document‚Äìterm matrix (DTM). This is the input to our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "test_reddit = ['I am planning my wedding and need help with the guest list.',\n",
    "               'My boss is being unreasonable about my work hours.',\n",
    "               'I just got engaged and am so excited for the future!',\n",
    "               'My partner and I are having communication issues.']\n",
    "\n",
    "# Create a tfidf vectorizer\n",
    "vectorizer = TfidfVectorizer(lowercase=True,\n",
    "                             stop_words='english',\n",
    "                             max_features=None)\n",
    "\n",
    "# Fit and transform \n",
    "tf_dtm = vectorizer.fit_transform(test_reddit)\n",
    "\n",
    "# Create a tf-idf dataframe\n",
    "tfidf = pd.DataFrame(tf_dtm.todense(),\n",
    "                     columns=vectorizer.get_feature_names_out())\n",
    "tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-59",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "\n",
    "## Text Classification Using the TF-IDF Representation\n",
    "\n",
    "Now that we have a tf-idf representation of the text, we can do some simple classification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.flair_text.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562981f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Keep only YTA / NTA and drop NaNs\n",
    "mask = df[\"flair_text\"].isin([\"Asshole\", \"Not the A-hole\"])\n",
    "df_bin = df.loc[mask, [\"selftext_clean\", \"flair_text\"]].dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e3ab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of posts for each flair_text category\n",
    "flair_counts = df['flair_text'].value_counts()\n",
    "print(flair_counts[['Asshole', 'Not the A-hole']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdfdab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Features/labels\n",
    "X = df_bin[\"selftext_clean\"].astype(str)\n",
    "y = df_bin[\"flair_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac2c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 3) Train/test split (stratify keeps class balance)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b934e1",
   "metadata": {},
   "source": [
    "We can nowtrain a classifier using TF-IDF values, we do the following:\n",
    "- We choose an algorithm (like Logistic Regression or Linear SVM).\n",
    "- The classifier looks at the TF-IDF values across many posts classified as YTA or NTA.\n",
    "- During training, it learns which words tend to signal which labels.\n",
    "- Mathematically, this is stored as a set of coefficients: one number per vocabulary term, per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e125c118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 4) Pipeline: TF-IDF -> Logistic Regression\n",
    "clf = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(lowercase=True,\n",
    "                              stop_words=\"english\",\n",
    "                              min_df=2,\n",
    "                              max_df=0.95)),\n",
    "    (\"logreg\", LogisticRegression(max_iter=1000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19213868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "# 5) Train and evaluate\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", round(accuracy_score(y_test, preds), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31541828",
   "metadata": {},
   "source": [
    "How do coefficients work here? \n",
    "- We turn each post into a bag of TF-IDF features: one column per token, with a number that says how present and distinctive that token is in the post.\n",
    "- Logistic Regression learns one weight per token. A positive weight pushes the prediction toward YTA (here classes[1]), a negative weight pushes toward NTA (here classes[0]).\n",
    "\n",
    "For a post, the model computes a score:\n",
    "\n",
    "$$\\text{score} = b + \\sum_{j} w_j \\cdot \\text{tfidf}_j$$\n",
    "\n",
    "If the score is high (after passing through the sigmoid), it predicts YTA; if it‚Äôs low, NTA.\n",
    "\n",
    "The bar charts show the tokens with the biggest weights in each direction. They‚Äôre not TF-IDF values; they‚Äôre model weights learned from the training data that indicate which tokens are most predictive for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb512ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6a) Visualize top tokens for YTA (largest positive weights)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vec = clf.named_steps[\"tfidf\"]\n",
    "logreg = clf.named_steps[\"logreg\"]\n",
    "\n",
    "feature_names = np.array(vec.get_feature_names_out())\n",
    "weights = logreg.coef_[0]          # binary: positive -> classes_[1], negative -> classes_[0]\n",
    "classes = logreg.classes_          # e.g., ['NTA', 'YTA']\n",
    "top_k = 15\n",
    "\n",
    "# Top toward YTA (largest positive weights)\n",
    "yta_idx = np.argsort(weights)[-top_k:]\n",
    "yta_tokens = feature_names[yta_idx]\n",
    "yta_weights = weights[yta_idx]\n",
    "\n",
    "# Sort for a clean horizontal chart\n",
    "order = np.argsort(yta_weights)\n",
    "yta_tokens = yta_tokens[order]\n",
    "yta_weights = yta_weights[order]\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.barh(yta_tokens, yta_weights)\n",
    "plt.title(f\"Top tokens ‚Üí {classes[1]} (higher = more YTA-like)\")\n",
    "plt.xlabel(\"Logistic regression weight\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bcdce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6b) Visualize top tokens for NTA (most negative weights)\n",
    "nta_idx = np.argsort(weights)[:top_k]\n",
    "nta_tokens = feature_names[nta_idx]\n",
    "nta_weights = weights[nta_idx]   # these are negative numbers\n",
    "\n",
    "# Sort so the strongest (most negative) appears at the bottom/top nicely\n",
    "order = np.argsort(nta_weights)\n",
    "nta_tokens = nta_tokens[order]\n",
    "nta_weights = nta_weights[order]\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.barh(nta_tokens, nta_weights)\n",
    "plt.title(f\"Top tokens ‚Üí {classes[0]} (more negative = more {classes[0]}-like)\")\n",
    "plt.xlabel(\"Logistic regression weight\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a46e6ce",
   "metadata": {},
   "source": [
    "Make predictions\n",
    "- For a new post, we compute TF-IDF values for its words.\n",
    "- The classifier multiplies those numbers by its learned coefficients.\n",
    "- The result is a score for each class, and the highest score is the predicted label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81110ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example new reddit post\n",
    "new_post = [\"I told my roommate she is kind of annoying, and now she's kind of upset at me.\"]\n",
    "\n",
    "# Predict class\n",
    "pred = clf.predict(new_post)[0]\n",
    "proba = clf.predict_proba(new_post)[0]\n",
    "\n",
    "print(\"Prediction:\", pred)\n",
    "print(\"Probabilities:\", dict(zip(clf.classes_, np.round(proba, 3))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-62",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## ‚ùó Key Points\n",
    "\n",
    "* Preprocessing includes multiple steps, some of them are more common to text data regardlessly, and some are task-specific. Reddit data requires specific preprocessing for URLs, user mentions, and subreddit references.\n",
    "* Both `nltk` and `spaCy` could be used for tokenization and stop word removal. The latter is more powerful in providing various linguistic annotations. \n",
    "* A Bag-of-Words representation is a simple method to transform our text data to numbers. It focuses on word frequency but not word order. \n",
    "* A TF-IDF representation is a step further; it also considers if a certain word distinctively appears in one document or occurs uniformally across all documents. \n",
    "* With a numerical representation, we can perform a range of text classification task, such as post length categorization or potentially sentiment analysis with labeled data.\n",
    "* Reddit data provides rich opportunities for text analysis due to its conversational nature and community-specific language patterns.\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
