{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional: Handling large datasets - Practical techniques\n",
    "\n",
    "This additional notebook demonstrates various approaches for working with large datasets when computational resources are limited.\n",
    "\n",
    "## Techniques covered:\n",
    "1. Selective file loading\n",
    "2. Strategic sampling methods\n",
    "3. Data preprocessing and compression\n",
    "4. Memory-efficient data handling\n",
    "5. Creating manageable subsets for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install pandas pyarrow zstandard tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "import random\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Iterator, Dict, Any\n",
    "import sys\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Selective File Loading\n",
    "\n",
    "If your torrent contains multiple files or the data is organized chronologically, you can load only what you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_dataset_files(data_dir: str) -> list:\n",
    "    \"\"\"\n",
    "    List all data files in the directory to see what's available\n",
    "    before committing to loading everything.\n",
    "    \"\"\"\n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    if not data_path.exists():\n",
    "        print(f\"Directory {data_dir} not found\")\n",
    "        return []\n",
    "    \n",
    "    # Common Reddit data file patterns\n",
    "    file_patterns = ['*.json', '*.json.gz', '*.jsonl', '*.jsonl.gz', '*.zst']\n",
    "    \n",
    "    files = []\n",
    "    for pattern in file_patterns:\n",
    "        files.extend(data_path.glob(pattern))\n",
    "    \n",
    "    # Sort by size to identify large files\n",
    "    files_with_size = [(f, f.stat().st_size / (1024**3)) for f in files]  # Size in GB\n",
    "    files_with_size.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"Available files:\")\n",
    "    for file, size_gb in files_with_size:\n",
    "        print(f\"  {file.name}: {size_gb:.2f} GB\")\n",
    "    \n",
    "    return [f[0] for f in files_with_size]\n",
    "\n",
    "# Example usage\n",
    "# files = list_dataset_files('/path/to/reddit/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Memory-Efficient Iterator for Large Files\n",
    "\n",
    "Instead of loading entire files into memory, process them line-by-line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_reddit_posts(file_path: str, max_posts: int = None) -> Iterator[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Memory-efficient iterator for Reddit data files.\n",
    "    Handles both compressed (.gz) and uncompressed files.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the Reddit data file\n",
    "        max_posts: Maximum number of posts to yield (None = all)\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    \n",
    "    # Determine if file is compressed\n",
    "    open_func = gzip.open if file_path.endswith('.gz') else open\n",
    "    \n",
    "    try:\n",
    "        with open_func(file_path, 'rt', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if max_posts and count >= max_posts:\n",
    "                    break\n",
    "                    \n",
    "                try:\n",
    "                    post = json.loads(line.strip())\n",
    "                    yield post\n",
    "                    count += 1\n",
    "                except json.JSONDecodeError:\n",
    "                    continue  # Skip malformed lines\n",
    "                    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return\n",
    "\n",
    "# Example: Preview first 5 posts without loading entire file\n",
    "# for i, post in enumerate(iterate_reddit_posts('reddit_data.json', max_posts=5)):\n",
    "#     print(f\"Post {i+1}: {post.get('title', 'No title')[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Strategic Sampling Methods\n",
    "\n",
    "Create representative subsets of your data using various sampling strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample_posts(file_path: str, sample_rate: float = 0.1, output_path: str = None) -> list:\n",
    "    \"\"\"\n",
    "    Randomly sample a percentage of posts from a large file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to source file\n",
    "        sample_rate: Proportion of posts to keep (0.1 = 10%)\n",
    "        output_path: Optional path to save sampled data\n",
    "    \n",
    "    Returns:\n",
    "        List of sampled posts\n",
    "    \"\"\"\n",
    "    sampled_posts = []\n",
    "    \n",
    "    print(f\"Sampling {sample_rate*100:.1f}% of posts from {Path(file_path).name}...\")\n",
    "    \n",
    "    for post in tqdm(iterate_reddit_posts(file_path)):\n",
    "        if random.random() < sample_rate:\n",
    "            sampled_posts.append(post)\n",
    "    \n",
    "    print(f\"Sampled {len(sampled_posts):,} posts\")\n",
    "    \n",
    "    # Optionally save to file\n",
    "    if output_path:\n",
    "        with open(output_path, 'w') as f:\n",
    "            for post in sampled_posts:\n",
    "                f.write(json.dumps(post) + '\\n')\n",
    "        print(f\"Saved to {output_path}\")\n",
    "    \n",
    "    return sampled_posts\n",
    "\n",
    "# Example usage\n",
    "# sampled_data = random_sample_posts('large_reddit_file.json', sample_rate=0.1, \n",
    "#                                     output_path='sampled_10percent.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_stratified_sample(file_path: str, \n",
    "                          start_date: str = None, \n",
    "                          end_date: str = None,\n",
    "                          output_path: str = None) -> list:\n",
    "    \"\"\"\n",
    "    Sample posts from specific time periods (e.g., election cycles, major events).\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to source file\n",
    "        start_date: Start date in format 'YYYY-MM-DD' (None = no start limit)\n",
    "        end_date: End date in format 'YYYY-MM-DD' (None = no end limit)\n",
    "        output_path: Optional path to save filtered data\n",
    "    \n",
    "    Returns:\n",
    "        List of posts within the date range\n",
    "    \"\"\"\n",
    "    filtered_posts = []\n",
    "    \n",
    "    start_ts = datetime.strptime(start_date, '%Y-%m-%d').timestamp() if start_date else 0\n",
    "    end_ts = datetime.strptime(end_date, '%Y-%m-%d').timestamp() if end_date else float('inf')\n",
    "    \n",
    "    print(f\"Filtering posts between {start_date or 'beginning'} and {end_date or 'end'}...\")\n",
    "    \n",
    "    for post in tqdm(iterate_reddit_posts(file_path)):\n",
    "        # Reddit timestamps are usually in 'created_utc' field\n",
    "        post_time = post.get('created_utc', 0)\n",
    "        \n",
    "        if start_ts <= post_time <= end_ts:\n",
    "            filtered_posts.append(post)\n",
    "    \n",
    "    print(f\"Found {len(filtered_posts):,} posts in date range\")\n",
    "    \n",
    "    if output_path:\n",
    "        with open(output_path, 'w') as f:\n",
    "            for post in filtered_posts:\n",
    "                f.write(json.dumps(post) + '\\n')\n",
    "        print(f\"Saved to {output_path}\")\n",
    "    \n",
    "    return filtered_posts\n",
    "\n",
    "# Example: Get posts from 2024 election period\n",
    "# election_posts = time_stratified_sample('reddit_data.json', \n",
    "#                                         start_date='2024-01-01', \n",
    "#                                         end_date='2024-11-30',\n",
    "#                                         output_path='election_2024_posts.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engagement_filtered_sample(file_path: str, \n",
    "                               min_score: int = 10,\n",
    "                               min_comments: int = 5,\n",
    "                               output_path: str = None) -> list:\n",
    "    \"\"\"\n",
    "    Filter posts by engagement metrics (upvotes, comments).\n",
    "    Focuses on content that actually reached people.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to source file\n",
    "        min_score: Minimum upvote score\n",
    "        min_comments: Minimum number of comments\n",
    "        output_path: Optional path to save filtered data\n",
    "    \n",
    "    Returns:\n",
    "        List of high-engagement posts\n",
    "    \"\"\"\n",
    "    filtered_posts = []\n",
    "    \n",
    "    print(f\"Filtering posts with score >= {min_score} and comments >= {min_comments}...\")\n",
    "    \n",
    "    for post in tqdm(iterate_reddit_posts(file_path)):\n",
    "        score = post.get('score', 0)\n",
    "        num_comments = post.get('num_comments', 0)\n",
    "        \n",
    "        if score >= min_score and num_comments >= min_comments:\n",
    "            filtered_posts.append(post)\n",
    "    \n",
    "    print(f\"Found {len(filtered_posts):,} high-engagement posts\")\n",
    "    \n",
    "    if output_path:\n",
    "        with open(output_path, 'w') as f:\n",
    "            for post in filtered_posts:\n",
    "                f.write(json.dumps(post) + '\\n')\n",
    "        print(f\"Saved to {output_path}\")\n",
    "    \n",
    "    return filtered_posts\n",
    "\n",
    "# Example usage\n",
    "# popular_posts = engagement_filtered_sample('reddit_data.json', \n",
    "#                                            min_score=50, \n",
    "#                                            min_comments=10,\n",
    "#                                            output_path='popular_posts.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing and Compression\n",
    "\n",
    "Extract only essential fields and save in efficient formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_essential_fields(file_path: str, \n",
    "                            fields: list = None,\n",
    "                            output_path: str = None) -> list:\n",
    "    \"\"\"\n",
    "    Extract only the fields you need for training, reducing data size.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to source file\n",
    "        fields: List of field names to keep (default: common fields for misinformation analysis)\n",
    "        output_path: Optional path to save processed data\n",
    "    \n",
    "    Returns:\n",
    "        List of processed posts with only essential fields\n",
    "    \"\"\"\n",
    "    if fields is None:\n",
    "        # Default fields for political misinformation analysis\n",
    "        fields = [\n",
    "            'id',\n",
    "            'title',\n",
    "            'selftext',\n",
    "            'author',\n",
    "            'created_utc',\n",
    "            'score',\n",
    "            'num_comments',\n",
    "            'subreddit',\n",
    "            'url',\n",
    "            'domain'\n",
    "        ]\n",
    "    \n",
    "    processed_posts = []\n",
    "    \n",
    "    print(f\"Extracting fields: {', '.join(fields)}\")\n",
    "    \n",
    "    for post in tqdm(iterate_reddit_posts(file_path)):\n",
    "        # Extract only specified fields\n",
    "        processed_post = {field: post.get(field, '') for field in fields}\n",
    "        processed_posts.append(processed_post)\n",
    "    \n",
    "    print(f\"Processed {len(processed_posts):,} posts\")\n",
    "    \n",
    "    if output_path:\n",
    "        with open(output_path, 'w') as f:\n",
    "            for post in processed_posts:\n",
    "                f.write(json.dumps(post) + '\\n')\n",
    "        \n",
    "        # Show size reduction\n",
    "        original_size = Path(file_path).stat().st_size / (1024**2)  # MB\n",
    "        new_size = Path(output_path).stat().st_size / (1024**2)  # MB\n",
    "        reduction = (1 - new_size/original_size) * 100\n",
    "        \n",
    "        print(f\"Saved to {output_path}\")\n",
    "        print(f\"Size reduction: {original_size:.1f} MB → {new_size:.1f} MB ({reduction:.1f}% smaller)\")\n",
    "    \n",
    "    return processed_posts\n",
    "\n",
    "# Example usage\n",
    "# essential_data = extract_essential_fields('large_reddit_file.json',\n",
    "#                                           output_path='essential_fields_only.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_parquet(file_path: str, output_path: str = None):\n",
    "    \"\"\"\n",
    "    Convert JSON/JSONL to Parquet format for better compression and faster loading.\n",
    "    Parquet is a columnar format that's much more efficient than JSON.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to source JSON/JSONL file\n",
    "        output_path: Path for output Parquet file\n",
    "    \"\"\"\n",
    "    if output_path is None:\n",
    "        output_path = Path(file_path).with_suffix('.parquet')\n",
    "    \n",
    "    print(f\"Converting {file_path} to Parquet format...\")\n",
    "    \n",
    "    # Read data in chunks to avoid memory issues\n",
    "    chunk_size = 10000\n",
    "    chunks = []\n",
    "    \n",
    "    current_chunk = []\n",
    "    for i, post in enumerate(tqdm(iterate_reddit_posts(file_path))):\n",
    "        current_chunk.append(post)\n",
    "        \n",
    "        if len(current_chunk) >= chunk_size:\n",
    "            chunks.append(pd.DataFrame(current_chunk))\n",
    "            current_chunk = []\n",
    "    \n",
    "    # Add remaining posts\n",
    "    if current_chunk:\n",
    "        chunks.append(pd.DataFrame(current_chunk))\n",
    "    \n",
    "    # Combine and save\n",
    "    df = pd.concat(chunks, ignore_index=True)\n",
    "    df.to_parquet(output_path, compression='snappy', index=False)\n",
    "    \n",
    "    # Show size comparison\n",
    "    original_size = Path(file_path).stat().st_size / (1024**2)  # MB\n",
    "    new_size = Path(output_path).stat().st_size / (1024**2)  # MB\n",
    "    reduction = (1 - new_size/original_size) * 100\n",
    "    \n",
    "    print(f\"\\nConversion complete!\")\n",
    "    print(f\"Original: {original_size:.1f} MB\")\n",
    "    print(f\"Parquet: {new_size:.1f} MB ({reduction:.1f}% smaller)\")\n",
    "    print(f\"Saved to: {output_path}\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "# Example usage\n",
    "# parquet_file = convert_to_parquet('sampled_data.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Complete Pipeline: From Large File to Training-Ready Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_dataset(source_file: str,\n",
    "                           output_dir: str = './processed_data',\n",
    "                           sample_rate: float = 0.1,\n",
    "                           min_score: int = 10,\n",
    "                           start_date: str = None,\n",
    "                           end_date: str = None,\n",
    "                           fields: list = None) -> str:\n",
    "    \"\"\"\n",
    "    Complete pipeline to create a manageable training dataset from a large Reddit file.\n",
    "    \n",
    "    Steps:\n",
    "    1. Filter by date range (if specified)\n",
    "    2. Filter by engagement metrics\n",
    "    3. Random sampling\n",
    "    4. Extract essential fields\n",
    "    5. Convert to Parquet format\n",
    "    \n",
    "    Args:\n",
    "        source_file: Path to large source file\n",
    "        output_dir: Directory to save processed files\n",
    "        sample_rate: Proportion of posts to keep after filtering\n",
    "        min_score: Minimum upvote score for engagement filter\n",
    "        start_date: Start date for time filter (YYYY-MM-DD)\n",
    "        end_date: End date for time filter (YYYY-MM-DD)\n",
    "        fields: Fields to extract (None = defaults)\n",
    "    \n",
    "    Returns:\n",
    "        Path to final Parquet file\n",
    "    \"\"\"\n",
    "    # Create output directory\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"CREATING TRAINING DATASET\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Source: {source_file}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    print()\n",
    "    \n",
    "    # Step 1: Apply all filters\n",
    "    print(\"Step 1: Applying filters...\")\n",
    "    filtered_posts = []\n",
    "    \n",
    "    start_ts = datetime.strptime(start_date, '%Y-%m-%d').timestamp() if start_date else 0\n",
    "    end_ts = datetime.strptime(end_date, '%Y-%m-%d').timestamp() if end_date else float('inf')\n",
    "    \n",
    "    for post in tqdm(iterate_reddit_posts(source_file)):\n",
    "        # Time filter\n",
    "        if start_date or end_date:\n",
    "            post_time = post.get('created_utc', 0)\n",
    "            if not (start_ts <= post_time <= end_ts):\n",
    "                continue\n",
    "        \n",
    "        # Engagement filter\n",
    "        if post.get('score', 0) < min_score:\n",
    "            continue\n",
    "        \n",
    "        # Random sampling\n",
    "        if random.random() < sample_rate:\n",
    "            filtered_posts.append(post)\n",
    "    \n",
    "    print(f\"✓ Filtered to {len(filtered_posts):,} posts\\n\")\n",
    "    \n",
    "    # Step 2: Extract essential fields\n",
    "    print(\"Step 2: Extracting essential fields...\")\n",
    "    if fields is None:\n",
    "        fields = ['id', 'title', 'selftext', 'author', 'created_utc', \n",
    "                 'score', 'num_comments', 'subreddit', 'url', 'domain']\n",
    "    \n",
    "    processed_posts = [{field: post.get(field, '') for field in fields} \n",
    "                      for post in filtered_posts]\n",
    "    print(f\"✓ Extracted {len(fields)} fields\\n\")\n",
    "    \n",
    "    # Step 3: Save as Parquet\n",
    "    print(\"Step 3: Converting to Parquet format...\")\n",
    "    df = pd.DataFrame(processed_posts)\n",
    "    \n",
    "    output_file = output_path / 'training_data.parquet'\n",
    "    df.to_parquet(output_file, compression='snappy', index=False)\n",
    "    \n",
    "    file_size = output_file.stat().st_size / (1024**2)  # MB\n",
    "    print(f\"✓ Saved {len(df):,} posts to {output_file}\")\n",
    "    print(f\"✓ Final size: {file_size:.1f} MB\\n\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"=\"*60)\n",
    "    print(\"DATASET SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total posts: {len(df):,}\")\n",
    "    print(f\"Date range: {df['created_utc'].min()} to {df['created_utc'].max()}\")\n",
    "    print(f\"Subreddits: {df['subreddit'].nunique()}\")\n",
    "    print(f\"Average score: {df['score'].mean():.1f}\")\n",
    "    print(f\"Average comments: {df['num_comments'].mean():.1f}\")\n",
    "    print(f\"File size: {file_size:.1f} MB\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    return str(output_file)\n",
    "\n",
    "# Example: Create a training dataset from a large file\n",
    "# training_file = create_training_dataset(\n",
    "#     source_file='large_reddit_politics.json',\n",
    "#     output_dir='./training_data',\n",
    "#     sample_rate=0.15,  # Keep 15% after filtering\n",
    "#     min_score=20,      # Posts with at least 20 upvotes\n",
    "#     start_date='2023-01-01',\n",
    "#     end_date='2024-12-31'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Loading and Working with the Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(parquet_file: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load the processed training data.\n",
    "    Parquet loads much faster than JSON!\n",
    "    \"\"\"\n",
    "    print(f\"Loading {parquet_file}...\")\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "    print(f\"Loaded {len(df):,} posts\")\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "# df = load_training_data('./processed_data/training_data.parquet')\n",
    "# print(df.head())\n",
    "# print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyzing Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Quick analysis of your dataset to ensure it's representative.\n",
    "    \"\"\"\n",
    "    print(\"DATASET ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nTotal posts: {len(df):,}\")\n",
    "    print(f\"\\nDate range:\")\n",
    "    if 'created_utc' in df.columns:\n",
    "        df['date'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "        print(f\"  From: {df['date'].min()}\")\n",
    "        print(f\"  To: {df['date'].max()}\")\n",
    "    \n",
    "    print(f\"\\nTop 10 subreddits by post count:\")\n",
    "    if 'subreddit' in df.columns:\n",
    "        print(df['subreddit'].value_counts().head(10))\n",
    "    \n",
    "    print(f\"\\nEngagement statistics:\")\n",
    "    if 'score' in df.columns:\n",
    "        print(f\"  Score - Mean: {df['score'].mean():.1f}, Median: {df['score'].median():.1f}\")\n",
    "    if 'num_comments' in df.columns:\n",
    "        print(f\"  Comments - Mean: {df['num_comments'].mean():.1f}, Median: {df['num_comments'].median():.1f}\")\n",
    "    \n",
    "    print(f\"\\nText length statistics:\")\n",
    "    if 'selftext' in df.columns:\n",
    "        df['text_length'] = df['selftext'].str.len()\n",
    "        print(f\"  Mean: {df['text_length'].mean():.0f} characters\")\n",
    "        print(f\"  Median: {df['text_length'].median():.0f} characters\")\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Example usage\n",
    "# analyze_dataset(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Recommended Workflow:\n",
    "\n",
    "1. **Explore your data structure** using `list_dataset_files()` to see what you're working with\n",
    "2. **Choose your strategy**:\n",
    "   - If data is split by time: Use selective loading\n",
    "   - If you need speed: Use random sampling with lower rate\n",
    "   - If you need quality: Use engagement-based filtering\n",
    "   - If focused on events: Use time-stratified sampling\n",
    "3. **Run the complete pipeline** with `create_training_dataset()`\n",
    "4. **Validate your dataset** with `analyze_dataset()`\n",
    "\n",
    "### Expected Results:\n",
    "- **10% random sample** of 25GB file → ~2.5GB\n",
    "- **Essential fields only** → additional 50-70% reduction\n",
    "- **Parquet format** → additional 30-50% reduction\n",
    "- **Final size**: Typically 500MB - 1GB (manageable for most systems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlab2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
