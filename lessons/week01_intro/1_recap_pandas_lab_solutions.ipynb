{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summer Recap: Data Analysis with Pandas\n",
    "\n",
    "* * * \n",
    "\n",
    "<div class=\"alert alert-success\">  \n",
    "    \n",
    "### Learning Objectives \n",
    "    \n",
    "* Review fundamental pandas operations for data manipulation and analysis.\n",
    "* Apply data cleaning techniques to real-world social science datasets.\n",
    "* Practice exploratory data analysis using descriptive statistics and basic visualizations.\n",
    "* Demonstrate ability to filter, group, and aggregate data using pandas methods.\n",
    "* Evaluate LLM-generated code for accuracy and best practices.\n",
    "</div>\n",
    "\n",
    "### Icons Used in This Notebook\n",
    "üîî **Question**: A quick question to help you understand what's going on.<br>\n",
    "ü•ä **Challenge**: Interactive exercise. We'll work through these in the workshop!<br>\n",
    "üí° **Tip**: How to do something a bit more efficiently or effectively.<br>\n",
    "‚ö†Ô∏è **Warning:** Heads-up about tricky stuff or common mistakes.<br>\n",
    "ü§ñ **AI Generated**: Code generated by an LLM that we'll test and debug.<br>\n",
    "\n",
    "### Sections\n",
    "1. [Data Loading and Initial Exploration](#section1)\n",
    "2. [Data Cleaning and Basic Operations](#section2)\n",
    "3. [Exploratory Data Analysis](#section3)\n",
    "4. [Text Analysis Fundamentals](#section4)\n",
    "5. [Working with LLM-Generated Code](#section5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "\n",
    "# Data Loading and Initial Exploration\n",
    "\n",
    "Today we'll work with data from Reddit's \"Am I the Asshole?\" (AITA) subreddit. This dataset contains posts where people describe situations and ask for community judgment about their behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (10000, 23)\n",
      "\n",
      "Column names:\n",
      "['idint', 'idstr', 'created', 'self', 'nsfw', 'author', 'title', 'url', 'selftext', 'score', 'subreddit', 'distinguish', 'textlen', 'num_comments', 'flair_text', 'flair_css_class', 'augmented_at', 'augmented_count', 'created_date', 'year', 'month', 'day_of_week', 'text_length']\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../../data/aita_top_subs.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumn names:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idint</th>\n",
       "      <th>idstr</th>\n",
       "      <th>created</th>\n",
       "      <th>self</th>\n",
       "      <th>nsfw</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>selftext</th>\n",
       "      <th>score</th>\n",
       "      <th>...</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>flair_text</th>\n",
       "      <th>flair_css_class</th>\n",
       "      <th>augmented_at</th>\n",
       "      <th>augmented_count</th>\n",
       "      <th>created_date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>797709732</td>\n",
       "      <td>t3_d6xoro</td>\n",
       "      <td>1568998300</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>DarthCharizard</td>\n",
       "      <td>META: This sub is moving towards a value syste...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I‚Äôve enjoyed reading and posting on this sub f...</td>\n",
       "      <td>80915.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6215.0</td>\n",
       "      <td>META</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-09-20 16:51:40</td>\n",
       "      <td>2019</td>\n",
       "      <td>9</td>\n",
       "      <td>Friday</td>\n",
       "      <td>3266.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1472895100</td>\n",
       "      <td>t3_ocx94s</td>\n",
       "      <td>1625315782</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>OnlyInQuebec9</td>\n",
       "      <td>AITA for telling my wife the lock on my daught...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My brother in-law (Sammy) lost his home shortl...</td>\n",
       "      <td>80334.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5318.0</td>\n",
       "      <td>Not the A-hole</td>\n",
       "      <td>not</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-07-03 12:36:22</td>\n",
       "      <td>2021</td>\n",
       "      <td>7</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>2664.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>664921441</td>\n",
       "      <td>t3_azvko1</td>\n",
       "      <td>1552322462</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Renegadesrule33</td>\n",
       "      <td>UPDATE, AITA for despising my mentally handica...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm back like I said I would be,. My [original...</td>\n",
       "      <td>72776.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>UPDATE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-11 16:41:02</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>Monday</td>\n",
       "      <td>5437.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>855862814</td>\n",
       "      <td>t3_e5k3z2</td>\n",
       "      <td>1575392873</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>throwRA-fhfsveyary</td>\n",
       "      <td>AITA for pretending to get fired when customer...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I am a high schooler with a weekend job at a c...</td>\n",
       "      <td>63526.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3645.0</td>\n",
       "      <td>Not the A-hole</td>\n",
       "      <td>not</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-12-03 17:07:53</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>2096.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>756636047</td>\n",
       "      <td>t3_cihc3z</td>\n",
       "      <td>1564233111</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Thunderbear998</td>\n",
       "      <td>AITA for telling my extended family how many m...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We had a family dinner this evening. My family...</td>\n",
       "      <td>54132.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5190.0</td>\n",
       "      <td>Everyone Sucks</td>\n",
       "      <td>ass</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-07-27 13:11:51</td>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1662.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        idint      idstr     created  self  nsfw              author  \\\n",
       "0   797709732  t3_d6xoro  1568998300   1.0   0.0      DarthCharizard   \n",
       "1  1472895100  t3_ocx94s  1625315782   1.0   0.0       OnlyInQuebec9   \n",
       "2   664921441  t3_azvko1  1552322462   1.0   0.0     Renegadesrule33   \n",
       "3   855862814  t3_e5k3z2  1575392873   1.0   0.0  throwRA-fhfsveyary   \n",
       "4   756636047  t3_cihc3z  1564233111   1.0   0.0      Thunderbear998   \n",
       "\n",
       "                                               title  url  \\\n",
       "0  META: This sub is moving towards a value syste...  NaN   \n",
       "1  AITA for telling my wife the lock on my daught...  NaN   \n",
       "2  UPDATE, AITA for despising my mentally handica...  NaN   \n",
       "3  AITA for pretending to get fired when customer...  NaN   \n",
       "4  AITA for telling my extended family how many m...  NaN   \n",
       "\n",
       "                                            selftext    score  ...  \\\n",
       "0  I‚Äôve enjoyed reading and posting on this sub f...  80915.0  ...   \n",
       "1  My brother in-law (Sammy) lost his home shortl...  80334.0  ...   \n",
       "2  I'm back like I said I would be,. My [original...  72776.0  ...   \n",
       "3  I am a high schooler with a weekend job at a c...  63526.0  ...   \n",
       "4  We had a family dinner this evening. My family...  54132.0  ...   \n",
       "\n",
       "  num_comments      flair_text  flair_css_class  augmented_at augmented_count  \\\n",
       "0       6215.0            META              NaN           NaN             NaN   \n",
       "1       5318.0  Not the A-hole              not           NaN             NaN   \n",
       "2       1989.0          UPDATE              NaN           NaN             NaN   \n",
       "3       3645.0  Not the A-hole              not           NaN             NaN   \n",
       "4       5190.0  Everyone Sucks              ass           NaN             NaN   \n",
       "\n",
       "          created_date  year  month day_of_week  text_length  \n",
       "0  2019-09-20 16:51:40  2019      9      Friday       3266.0  \n",
       "1  2021-07-03 12:36:22  2021      7    Saturday       2664.0  \n",
       "2  2019-03-11 16:41:02  2019      3      Monday       5437.0  \n",
       "3  2019-12-03 17:07:53  2019     12     Tuesday       2096.0  \n",
       "4  2019-07-27 13:11:51  2019      7    Saturday       1662.0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 1: Data Overview\n",
    "\n",
    "Explore the dataset structure and provide a summary of what you find. Use pandas methods to:\n",
    "1. Check the data types of each column\n",
    "2. Look for missing values\n",
    "3. Get basic descriptive statistics for numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA TYPES ===\n",
      "idint                int64\n",
      "idstr               object\n",
      "created              int64\n",
      "self               float64\n",
      "nsfw               float64\n",
      "author              object\n",
      "title               object\n",
      "url                float64\n",
      "selftext            object\n",
      "score              float64\n",
      "subreddit           object\n",
      "distinguish         object\n",
      "textlen            float64\n",
      "num_comments       float64\n",
      "flair_text          object\n",
      "flair_css_class     object\n",
      "augmented_at       float64\n",
      "augmented_count    float64\n",
      "created_date        object\n",
      "year                 int64\n",
      "month                int64\n",
      "day_of_week         object\n",
      "text_length        float64\n",
      "dtype: object\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== MISSING VALUES ===\n",
      "Missing value counts:\n",
      "idint                  0\n",
      "idstr                  0\n",
      "created                0\n",
      "self                   0\n",
      "nsfw                   0\n",
      "author                 0\n",
      "title                  0\n",
      "url                10000\n",
      "selftext               4\n",
      "score                  0\n",
      "subreddit              0\n",
      "distinguish         9999\n",
      "textlen                0\n",
      "num_comments           0\n",
      "flair_text          1175\n",
      "flair_css_class     1441\n",
      "augmented_at       10000\n",
      "augmented_count    10000\n",
      "created_date           0\n",
      "year                   0\n",
      "month                  0\n",
      "day_of_week            0\n",
      "text_length            4\n",
      "dtype: int64\n",
      "=== DESCRIPTIVE STATISTICS ===\n",
      "              idint       created     self          nsfw  url         score  \\\n",
      "count  1.000000e+04  1.000000e+04  10000.0  10000.000000  0.0  10000.000000   \n",
      "mean   1.213800e+09  1.604749e+09      1.0      0.005300  NaN  10137.508500   \n",
      "std    2.832943e+08  2.370645e+07      0.0      0.072612  NaN   6987.893464   \n",
      "min    5.826428e+08  1.539143e+09      1.0      0.000000  NaN   3739.000000   \n",
      "25%    1.003247e+09  1.589699e+09      1.0      0.000000  NaN   5168.750000   \n",
      "50%    1.224636e+09  1.607434e+09      1.0      0.000000  NaN   7437.000000   \n",
      "75%    1.472430e+09  1.625272e+09      1.0      0.000000  NaN  13128.250000   \n",
      "max    1.662312e+09  1.639655e+09      1.0      1.000000  NaN  80915.000000   \n",
      "\n",
      "           textlen  num_comments  augmented_at  augmented_count          year  \\\n",
      "count  10000.00000  10000.000000           0.0              0.0  10000.000000   \n",
      "mean    1878.93120   1218.262500           NaN              NaN   2020.303200   \n",
      "std      830.80909    946.524966           NaN              NaN      0.737105   \n",
      "min        0.00000    321.000000           NaN              NaN   2018.000000   \n",
      "25%     1347.00000    591.000000           NaN              NaN   2020.000000   \n",
      "50%     1934.00000    911.000000           NaN              NaN   2020.000000   \n",
      "75%     2532.00000   1505.000000           NaN              NaN   2021.000000   \n",
      "max     7750.00000  10265.000000           NaN              NaN   2021.000000   \n",
      "\n",
      "              month   text_length  \n",
      "count  10000.000000   9996.000000  \n",
      "mean       7.098600   1924.839236  \n",
      "std        3.330734   1305.271474  \n",
      "min        1.000000      1.000000  \n",
      "25%        5.000000   1146.000000  \n",
      "50%        7.000000   1971.000000  \n",
      "75%       10.000000   2756.000000  \n",
      "max       12.000000  15084.000000  \n"
     ]
    }
   ],
   "source": [
    "# STANDARD ANSWER - Challenge 1: Data Overview\n",
    "\n",
    "# 1. Check the data types of each column\n",
    "print(\"=== DATA TYPES ===\")\n",
    "print(df.dtypes)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# 2. Look for missing values\n",
    "print(\"=== MISSING VALUES ===\")\n",
    "# DIRECT METHOD: Count of missing values per column\n",
    "missing_counts = df.isnull().sum()\n",
    "print(\"Missing value counts:\")\n",
    "print(missing_counts)\n",
    "\n",
    "# # QUICK METHOD: Percentage of missing values (more informative for large datasets)\n",
    "# missing_percentages = (df.isnull().sum() / len(df)) * 100\n",
    "# print(\"\\nMissing value percentages:\")\n",
    "# for col, pct in missing_percentages.items():\n",
    "#     if pct > 0:\n",
    "#         print(f\"{col}: {pct:.2f}%\")\n",
    "\n",
    "# # ULTRA-QUICK METHOD: One-liner to see only columns with missing data\n",
    "# print(\"\\nColumns with missing data (Quick view):\")\n",
    "# print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "\n",
    "# print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# 3. Get basic descriptive statistics for numerical columns\n",
    "print(\"=== DESCRIPTIVE STATISTICS ===\")\n",
    "# DIRECT METHOD: Using describe() gives us count, mean, std, min, 25%, 50%, 75%, max\n",
    "numerical_stats = df.describe()\n",
    "print(numerical_stats)\n",
    "\n",
    "# # QUICK METHOD: Just show key insights\n",
    "# print(f\"\\n=== KEY INSIGHTS (QUICK VIEW) ===\")\n",
    "# print(f\"Dataset shape: {df.shape} (rows, columns)\")\n",
    "# print(f\"Score range: {df['score'].min():.0f} to {df['score'].max():.0f}\")\n",
    "# print(f\"Text length range: {df['text_length'].min():.0f} to {df['text_length'].max():.0f} characters\")\n",
    "# print(f\"Unique authors: {df['author'].nunique()}\")\n",
    "# print(f\"Date range: {df['created_date'].min()} to {df['created_date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîî **Question**: What do you notice about the `selftext` column? What might this tell us about the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "\n",
    "# Data Cleaning and Basic Operations\n",
    "\n",
    "Real-world data often requires cleaning before analysis. Let's examine our dataset for common issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n",
      "\n",
      "Score statistics:\n",
      "count    10000.000000\n",
      "mean     10137.508500\n",
      "std       6987.893464\n",
      "min       3739.000000\n",
      "25%       5168.750000\n",
      "50%       7437.000000\n",
      "75%      13128.250000\n",
      "max      80915.000000\n",
      "Name: score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate posts\n",
    "print(f\"Number of duplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "# Look at the distribution of some key variables\n",
    "print(f\"\\nScore statistics:\")\n",
    "print(df['score'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 2: Data Cleaning\n",
    "\n",
    "Clean the dataset by:\n",
    "1. Removing any posts where `selftext` is missing or empty\n",
    "2. Creating a new column called `text_length` that contains the character count of `selftext`\n",
    "3. Filter out posts that are shorter than 100 characters (likely low-quality posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (10000, 23)\n",
      "\n",
      "Selftext column analysis:\n",
      "Missing values: 4\n",
      "Empty strings: 0\n",
      "After removing missing selftext: (9996, 23)\n",
      "After removing empty selftext: (9996, 23)\n",
      "\n",
      "Text length statistics:\n",
      "count     9996.000000\n",
      "mean      1924.839236\n",
      "std       1305.271474\n",
      "min          1.000000\n",
      "25%       1146.000000\n",
      "50%       1971.000000\n",
      "75%       2756.000000\n",
      "max      15084.000000\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "Posts with less than 100 characters: 1753\n",
      "After filtering short posts: (8243, 23)\n",
      "\n",
      "=== CLEANING SUMMARY ===\n",
      "Original rows: 10000\n",
      "Final rows: 8243\n",
      "Rows removed: 1757\n",
      "Percentage retained: 82.4%\n",
      "\n",
      "Final text length distribution:\n",
      "count     8243.000000\n",
      "mean      2332.272838\n",
      "std       1058.019949\n",
      "min        209.000000\n",
      "25%       1614.000000\n",
      "50%       2232.000000\n",
      "75%       2883.000000\n",
      "max      15084.000000\n",
      "Name: text_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# STANDARD ANSWER - Challenge 2: Data Cleaning\n",
    "\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "\n",
    "# Step 1: Remove posts where selftext is missing or empty\n",
    "# Check current state of selftext column\n",
    "print(f\"\\nSelftext column analysis:\")\n",
    "print(f\"Missing values: {df['selftext'].isnull().sum()}\")\n",
    "print(f\"Empty strings: {(df['selftext'] == '').sum()}\")\n",
    "\n",
    "# Remove missing values first\n",
    "df_clean = df.dropna(subset=['selftext']).copy()# if we don't use copy(), df_clean will be a view, not a copy\n",
    "print(f\"After removing missing selftext: {df_clean.shape}\")\n",
    "\n",
    "# Remove empty strings (posts with no content)\n",
    "df_clean = df_clean[df_clean['selftext'] != ''].copy()\n",
    "print(f\"After removing empty selftext: {df_clean.shape}\")\n",
    "\n",
    "# Step 2: Create text_length column\n",
    "# Calculate character count for each selftext\n",
    "df_clean['text_length'] = df_clean['selftext'].str.len()\n",
    "\n",
    "print(f\"\\nText length statistics:\")\n",
    "print(df_clean['text_length'].describe())\n",
    "\n",
    "# Step 3: Filter out posts shorter than 100 characters\n",
    "# These are likely low-quality or incomplete posts\n",
    "print(f\"\\nPosts with less than 100 characters: {(df_clean['text_length'] < 100).sum()}\")\n",
    "\n",
    "df_clean = df_clean[df_clean['text_length'] >= 100].copy()\n",
    "print(f\"After filtering short posts: {df_clean.shape}\")\n",
    "\n",
    "# Summary of cleaning process\n",
    "print(f\"\\n=== CLEANING SUMMARY ===\")\n",
    "print(f\"Original rows: {df.shape[0]}\")\n",
    "print(f\"Final rows: {df_clean.shape[0]}\")\n",
    "print(f\"Rows removed: {df.shape[0] - df_clean.shape[0]}\")\n",
    "print(f\"Percentage retained: {(df_clean.shape[0] / df.shape[0]) * 100:.1f}%\")\n",
    "\n",
    "# Update our main dataframe for subsequent analysis\n",
    "df = df_clean.copy()\n",
    "\n",
    "print(f\"\\nFinal text length distribution:\")\n",
    "print(df['text_length'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Tip**: Use the `.str.len()` method to get string lengths in pandas. Remember that missing values might cause issues, so handle them first!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Dates\n",
    "\n",
    "The `created` column contains Unix timestamps. Let's convert these to readable dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range in dataset:\n",
      "From: 2018-10-10 03:39:34\n",
      "To: 2021-12-16 11:46:38\n"
     ]
    }
   ],
   "source": [
    "# Convert Unix timestamp to datetime\n",
    "df['created_date'] = pd.to_datetime(df['created'], unit='s')\n",
    "\n",
    "# Extract useful date components\n",
    "df['year'] = df['created_date'].dt.year\n",
    "df['month'] = df['created_date'].dt.month\n",
    "df['day_of_week'] = df['created_date'].dt.day_name()\n",
    "\n",
    "print(\"Date range in dataset:\")\n",
    "print(f\"From: {df['created_date'].min()}\")\n",
    "print(f\"To: {df['created_date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "\n",
    "# Exploratory Data Analysis\n",
    "\n",
    "Now let's explore patterns in the data using pandas grouping and aggregation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 3: Score Analysis\n",
    "\n",
    "Analyze post popularity by:\n",
    "1. Finding the top 10 posts by score\n",
    "2. Calculating the average score by year\n",
    "3. Determining which day of the week gets the highest average scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TOP 10 POSTS BY SCORE ===\n",
      "Direct method result:\n",
      "                                               title    score  num_comments  \\\n",
      "0  META: This sub is moving towards a value syste...  80915.0        6215.0   \n",
      "1  AITA for telling my wife the lock on my daught...  80334.0        5318.0   \n",
      "2  UPDATE, AITA for despising my mentally handica...  72776.0        1989.0   \n",
      "3  AITA for pretending to get fired when customer...  63526.0        3645.0   \n",
      "4  AITA for telling my extended family how many m...  54132.0        5190.0   \n",
      "5  AITA for \"announcing\" that my dad's not paying...  51323.0        2883.0   \n",
      "6  AITA for refusing to pay for my sister's husba...  49967.0        6414.0   \n",
      "7  [UPDATE] AITA for telling an employee she can ...  48572.0        2244.0   \n",
      "8  UPDATE AITA for not sharing my medical history...  47893.0         598.0   \n",
      "9  AITA for telling my son he deserved his gf bre...  47771.0        4008.0   \n",
      "\n",
      "                 author  \n",
      "0        DarthCharizard  \n",
      "1         OnlyInQuebec9  \n",
      "2       Renegadesrule33  \n",
      "3    throwRA-fhfsveyary  \n",
      "4        Thunderbear998  \n",
      "5             yeasothat  \n",
      "6        Home-Time-6077  \n",
      "7       Absolut_Failure  \n",
      "8       Drudawgthedrood  \n",
      "9  inappropriatedress77  \n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "=== AVERAGE SCORE BY YEAR ===\n",
      "Direct method:\n",
      "      Average_Score  Post_Count  Std_Dev\n",
      "year                                    \n",
      "2018        7622.18          66  3917.20\n",
      "2019       12534.59        1248  9096.80\n",
      "2020       10552.52        3286  7280.29\n",
      "2021        9420.47        3643  6268.37\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "=== AVERAGE SCORE BY DAY OF WEEK ===\n",
      "Direct method:\n",
      "             Average_Score  Post_Count  Median_Score\n",
      "day_of_week                                         \n",
      "Sunday            10626.65        1191        7798.0\n",
      "Tuesday           10421.06        1213        7801.0\n",
      "Wednesday         10316.58        1178        7281.0\n",
      "Saturday          10303.77        1091        7479.0\n",
      "Thursday          10242.29        1194        7206.0\n",
      "Friday            10198.37        1111        7373.0\n",
      "Monday            10189.32        1265        7589.0\n",
      "\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STANDARD ANSWER - Challenge 3: Score Analysis\n",
    "\n",
    "# 1. Find the top 10 posts by score\n",
    "print(\"=== TOP 10 POSTS BY SCORE ===\")\n",
    "\n",
    "# DIRECT METHOD: Step by step approach\n",
    "# Step 1: Sort by score in descending order\n",
    "sorted_df = df.sort_values('score', ascending=False)\n",
    "# Step 2: Get the first 10 rows\n",
    "top_10 = sorted_df.head(10)\n",
    "# Step 3: Select relevant columns\n",
    "top_posts_direct = top_10[['title', 'score', 'num_comments', 'author']]\n",
    "print(\"Direct method result:\")\n",
    "print(top_posts_direct)\n",
    "\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# # QUICK METHOD: One-liner using nlargest()\n",
    "# top_posts_quick = df.nlargest(10, 'score')[['title', 'score', 'num_comments', 'author']]\n",
    "# print(\"Quick method result:\")\n",
    "# print(top_posts_quick)\n",
    "\n",
    "# # ULTRA-QUICK: Just see the scores\n",
    "# print(f\"\\nUltra-quick view - Top 10 scores:\")\n",
    "# print(df['score'].nlargest(10).tolist())\n",
    "\n",
    "# # Let's also show some details about the highest scoring post\n",
    "# print(f\"\\nHighest scoring post details:\")\n",
    "# highest_post = df.loc[df['score'].idxmax()]  # Quick way to find max row\n",
    "# print(f\"Score: {highest_post['score']}\")\n",
    "# print(f\"Title: {highest_post['title']}\")\n",
    "# print(f\"Author: {highest_post['author']}\")\n",
    "# print(f\"Text length: {highest_post['text_length']} characters\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 2. Calculate average score by year\n",
    "print(\"=== AVERAGE SCORE BY YEAR ===\")\n",
    "\n",
    "# DIRECT METHOD: Step by step grouping\n",
    "print(\"Direct method:\")\n",
    "grouped = df.groupby('year')  # Group by year\n",
    "yearly_means = grouped['score'].mean()  # Calculate means\n",
    "yearly_counts = grouped['score'].count()  # Count posts per year\n",
    "yearly_stds = grouped['score'].std()  # Standard deviation\n",
    "\n",
    "# Combine into a DataFrame\n",
    "yearly_analysis_direct = pd.DataFrame({\n",
    "    'Average_Score': yearly_means.round(2),\n",
    "    'Post_Count': yearly_counts,\n",
    "    'Std_Dev': yearly_stds.round(2)\n",
    "})\n",
    "print(yearly_analysis_direct)\n",
    "\n",
    "print(\"\\n\" + \"-\"*40 + \"\\n\")\n",
    "\n",
    "# # QUICK METHOD: All statistics in one line\n",
    "# print(\"Quick method:\")\n",
    "# yearly_stats_quick = df.groupby('year')['score'].agg(['mean', 'count', 'std']).round(2)# round to 2 decimals    \n",
    "# yearly_stats_quick.columns = ['Average Score', 'Post Count', 'Standard Deviation']\n",
    "# print(yearly_stats_quick)\n",
    "\n",
    "# # ULTRA-QUICK: Just the averages\n",
    "# print(f\"\\nUltra-quick - Just averages:\")\n",
    "# print(df.groupby('year')['score'].mean().round(2))\n",
    "\n",
    "# # Add some context about the best year\n",
    "# best_year = yearly_stats_quick['Average Score'].idxmax()\n",
    "# best_avg = yearly_stats_quick.loc[best_year, 'Average Score']\n",
    "# print(f\"\\nBest performing year: {best_year} with average score of {best_avg}\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 3. Determine which day of the week gets highest average scores\n",
    "print(\"=== AVERAGE SCORE BY DAY OF WEEK ===\")\n",
    "\n",
    "# DIRECT METHOD: Step by step approach\n",
    "print(\"Direct method:\")\n",
    "day_groups = df.groupby('day_of_week')\n",
    "day_means = day_groups['score'].mean()\n",
    "day_counts = day_groups.size()  # Using size() instead of count()\n",
    "day_medians = day_groups['score'].median()\n",
    "\n",
    "daily_analysis_direct = pd.DataFrame({\n",
    "    'Average_Score': day_means.round(2),\n",
    "    'Post_Count': day_counts,\n",
    "    'Median_Score': day_medians.round(2)\n",
    "})\n",
    "print(daily_analysis_direct.sort_values('Average_Score', ascending=False))\n",
    "\n",
    "print(\"\\n\" + \"-\"*40 + \"\\n\")\n",
    "\n",
    "# # QUICK METHOD: One-liner aggregation\n",
    "# print(\"Quick method:\")\n",
    "# daily_stats_quick = df.groupby('day_of_week')['score'].agg(['mean', 'count', 'median']).round(2)\n",
    "# daily_stats_quick.columns = ['Average Score', 'Post Count', 'Median Score']\n",
    "# daily_stats_sorted = daily_stats_quick.sort_values('Average Score', ascending=False)\n",
    "# print(daily_stats_sorted)\n",
    "\n",
    "# # ULTRA-QUICK: Just see which day is best\n",
    "# print(f\"\\nUltra-quick answer:\")\n",
    "# best_day = df.groupby('day_of_week')['score'].mean().idxmax()\n",
    "# best_day_avg = df.groupby('day_of_week')['score'].mean().max()\n",
    "# print(f\"Best day: {best_day} (avg score: {best_day_avg:.0f})\")\n",
    "\n",
    "# # Additional insight: Weekend vs Weekday analysis\n",
    "# weekend_days = ['Saturday', 'Sunday']\n",
    "# weekday_avg = df[~df['day_of_week'].isin(weekend_days)]['score'].mean()\n",
    "# weekend_avg = df[df['day_of_week'].isin(weekend_days)]['score'].mean()\n",
    "\n",
    "# print(f\"\\n=== WEEKEND vs WEEKDAY COMPARISON ===\")\n",
    "# print(f\"Weekday average score: {weekday_avg:.2f}\")\n",
    "# print(f\"Weekend average score: {weekend_avg:.2f}\")\n",
    "# diff_pct = ((weekend_avg/weekday_avg - 1) * 100)\n",
    "# print(f\"Weekend posts score {diff_pct:+.1f}% compared to weekdays\")\n",
    "\n",
    "# # QUICK INSIGHT: Are weekends better?\n",
    "# if weekend_avg > weekday_avg:\n",
    "#     print(\"‚Üí Weekend posts tend to perform better!\")\n",
    "# else:\n",
    "#     print(\"‚Üí Weekday posts tend to perform better!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment Engagement Analysis\n",
    "\n",
    "Now, let's explore how users interact with posts by analyzing the volume and distribution of comments, which can highlight engagement patterns and community response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation matrix:\n",
      "              text_length     score  num_comments\n",
      "text_length      1.000000  0.049182      0.037226\n",
      "score            0.049182  1.000000      0.561018\n",
      "num_comments     0.037226  0.561018      1.000000\n"
     ]
    }
   ],
   "source": [
    "# Explore the relationship between text length and engagement\n",
    "correlation = df[['text_length', 'score', 'num_comments']].corr()\n",
    "print(\"Correlation matrix:\")\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîî **Question**: What does the correlation tell us about the relationship between post length and engagement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference Answer:**\n",
    "\n",
    "The `selftext` column contains the main body text of each Reddit post. You may notice that some entries are empty or contain placeholders like \"[deleted]\" or \"[removed]\". This suggests that not all posts have content available‚Äîsome may have been deleted by users or moderators, or were originally just a title with no body text. This tells us that the dataset may have missing or incomplete information for some posts, which is important to consider during analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 4: Engagement Categories\n",
    "\n",
    "Create engagement categories and analyze them:\n",
    "1. Create a new column `engagement_level` with categories:\n",
    "   - 'Low': score < 100\n",
    "   - 'Medium': score 100-500\n",
    "   - 'High': score 500-2000\n",
    "   - 'Viral': score > 2000\n",
    "2. Calculate the percentage of posts in each category\n",
    "3. Find the average text length for each engagement level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CREATING ENGAGEMENT CATEGORIES ===\n",
      "Score distribution:\n",
      "count     8243.000000\n",
      "mean     10328.836710\n",
      "std       7232.502385\n",
      "min       3739.000000\n",
      "25%       5167.500000\n",
      "50%       7503.000000\n",
      "75%      13445.000000\n",
      "max      80915.000000\n",
      "Name: score, dtype: float64\n",
      "\n",
      "Score quartiles for better categorization:\n",
      "25th percentile: 5168\n",
      "50th percentile (median): 7503\n",
      "75th percentile: 13445\n",
      "=== ENGAGEMENT LEVEL DISTRIBUTION ===\n",
      "                  Count  Percentage\n",
      "engagement_level                   \n",
      "High               2061       25.00\n",
      "Low                2061       25.00\n",
      "Medium             2060       24.99\n",
      "Viral              2061       25.00\n",
      "\n",
      "Total posts analyzed: 8243\n",
      "Most common engagement level: High\n",
      "Each category has roughly 25% of posts (by design using quartiles)\n",
      "\n",
      "============================================================\n",
      "\n",
      "=== TEXT LENGTH BY ENGAGEMENT LEVEL ===\n",
      "                  Average Length  Median Length  Std Deviation  Post Count\n",
      "engagement_level                                                          \n",
      "Low                      2275.01         2170.0        1032.58        2061\n",
      "Medium                   2282.73         2181.0        1010.31        2060\n",
      "High                     2357.70         2240.0        1090.20        2061\n",
      "Viral                    2413.62         2322.0        1091.21        2061\n"
     ]
    }
   ],
   "source": [
    "# STANDARD ANSWER - Challenge 4: Engagement Categories\n",
    "\n",
    "# 1. Create engagement_level column with categories based on score\n",
    "print(\"=== CREATING ENGAGEMENT CATEGORIES ===\")\n",
    "\n",
    "# First, let's see the score distribution to understand our categorization\n",
    "print(\"Score distribution:\")\n",
    "print(df['score'].describe())\n",
    "\n",
    "# IMPORTANT: Since this dataset only contains highly popular posts (min score > 3000),\n",
    "# we need to adjust our thresholds to create meaningful categories\n",
    "\n",
    "# DIRECT METHOD: Using conditional logic based on quartiles\n",
    "print(f\"\\nScore quartiles for better categorization:\")\n",
    "q25 = df['score'].quantile(0.25)\n",
    "q50 = df['score'].quantile(0.50)\n",
    "q75 = df['score'].quantile(0.75)\n",
    "print(f\"25th percentile: {q25:.0f}\")\n",
    "print(f\"50th percentile (median): {q50:.0f}\")\n",
    "print(f\"75th percentile: {q75:.0f}\")\n",
    "\n",
    "def categorize_engagement(score):\n",
    "    \"\"\"\n",
    "    Categorize engagement level based on post score\n",
    "    Using quartiles to create meaningful groups for this high-engagement dataset\n",
    "    \"\"\"\n",
    "    if score < q25:  # Bottom 25%\n",
    "        return 'Low'\n",
    "    elif score < q50:  # 25-50%\n",
    "        return 'Medium'\n",
    "    elif score < q75:  # 50-75%\n",
    "        return 'High'\n",
    "    else:  # Top 25%\n",
    "        return 'Viral'\n",
    "\n",
    "# Apply the categorization\n",
    "df['engagement_level'] = df['score'].apply(categorize_engagement)\n",
    "\n",
    "# # QUICK METHOD: Using pd.qcut() to automatically create quartile-based categories\n",
    "# # This is faster and more automatic:\n",
    "# # df['engagement_level'] = pd.qcut(df['score'], \n",
    "# #                                  q=4, \n",
    "# #                                  labels=['Low', 'Medium', 'High', 'Viral'])\n",
    "\n",
    "# print(f\"\\nScore thresholds used:\")\n",
    "# print(f\"Low: < {q25:.0f}\")\n",
    "# print(f\"Medium: {q25:.0f} - {q50:.0f}\")\n",
    "# print(f\"High: {q50:.0f} - {q75:.0f}\")\n",
    "# print(f\"Viral: > {q75:.0f}\")\n",
    "\n",
    "# print(\"Engagement categories created successfully!\")\n",
    "# print(f\"Sample categories: {df['engagement_level'].head()}\")\n",
    "\n",
    "# print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 2. Calculate percentage of posts in each category\n",
    "print(\"=== ENGAGEMENT LEVEL DISTRIBUTION ===\")\n",
    "engagement_counts = df['engagement_level'].value_counts().sort_index()\n",
    "engagement_percentages = df['engagement_level'].value_counts(normalize=True).sort_index() * 100\n",
    "\n",
    "# Create a summary table\n",
    "engagement_summary = pd.DataFrame({\n",
    "    'Count': engagement_counts,\n",
    "    'Percentage': engagement_percentages.round(2)\n",
    "})\n",
    "\n",
    "print(engagement_summary)\n",
    "\n",
    "# Additional insights about the distribution\n",
    "print(f\"\\nTotal posts analyzed: {len(df)}\")\n",
    "print(f\"Most common engagement level: {engagement_counts.idxmax()}\")\n",
    "print(f\"Each category has roughly 25% of posts (by design using quartiles)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 3. Find average text length for each engagement level\n",
    "print(\"=== TEXT LENGTH BY ENGAGEMENT LEVEL ===\")\n",
    "text_length_by_engagement = df.groupby('engagement_level')['text_length'].agg([\n",
    "    'mean', 'median', 'std', 'count'\n",
    "]).round(2)\n",
    "\n",
    "# Rename columns for clarity\n",
    "text_length_by_engagement.columns = ['Average Length', 'Median Length', 'Std Deviation', 'Post Count']\n",
    "\n",
    "# Sort by engagement level (Low -> Medium -> High -> Viral)\n",
    "desired_order = ['Low', 'Medium', 'High', 'Viral']\n",
    "text_length_by_engagement = text_length_by_engagement.reindex(desired_order)\n",
    "\n",
    "print(text_length_by_engagement)\n",
    "\n",
    "# # QUICK METHOD: Simple comparison\n",
    "# print(f\"\\n=== QUICK COMPARISON ===\")\n",
    "# low_avg = df[df['engagement_level'] == 'Low']['text_length'].mean()\n",
    "# viral_avg = df[df['engagement_level'] == 'Viral']['text_length'].mean()\n",
    "# print(f\"Low engagement posts: {low_avg:.0f} characters on average\")\n",
    "# print(f\"Viral posts: {viral_avg:.0f} characters on average\")\n",
    "# print(f\"Difference: {viral_avg - low_avg:.0f} characters ({((viral_avg/low_avg - 1) * 100):+.1f}%)\")\n",
    "\n",
    "# # Additional analysis: correlation between text length and engagement\n",
    "# print(f\"\\n=== KEY INSIGHTS ===\")\n",
    "# correlation = df['text_length'].corr(df['score'])\n",
    "# print(f\"Correlation between text length and score: {correlation:.3f}\")\n",
    "\n",
    "# if correlation > 0.05:\n",
    "#     print(\"‚Üí Longer posts tend to get slightly higher scores\")\n",
    "# elif correlation < -0.05:\n",
    "#     print(\"‚Üí Shorter posts tend to get higher scores\") \n",
    "# else:\n",
    "#     print(\"‚Üí Text length has minimal impact on engagement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "\n",
    "# Text Analysis Fundamentals\n",
    "\n",
    "Let's do some basic text analysis to understand the content patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 5: Text Pattern Analysis\n",
    "\n",
    "Analyze text patterns by:\n",
    "1. Finding posts that contain the word \"family\" (case-insensitive)\n",
    "2. Counting how many posts mention \"wedding\" or \"marriage\"\n",
    "3. Creating a column indicating whether the post is about relationships (contains words like \"boyfriend\", \"girlfriend\", \"husband\", \"wife\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== POSTS MENTIONING 'FAMILY' ===\n",
      "Total posts mentioning 'family': 3310\n",
      "Percentage of all posts: 40.16%\n",
      "\n",
      "Sample family-related post titles:\n",
      "1. AITA for telling my wife the lock on my daughter's door does not get removed til my brother inlaw and his daughters are out of our house?\n",
      "2. UPDATE, AITA for despising my mentally handicap sister?\n",
      "3. AITA for pretending to get fired when customers get a temper with me?\n",
      "\n",
      "Engagement comparison:\n",
      "Family posts average score: 10482.68\n",
      "Non-family posts average score: 10225.61\n",
      "Difference: +257.08\n",
      "\n",
      "============================================================\n",
      "\n",
      "=== POSTS ABOUT WEDDINGS/MARRIAGE ===\n",
      "Posts mentioning 'wedding' or 'marriage': 1229\n",
      "Percentage of all posts: 14.91%\n",
      "\n",
      "Breakdown:\n",
      "Posts with 'wedding': 876\n",
      "Posts with 'marriage': 447\n",
      "Posts with both terms: 94\n",
      "\n",
      "Sample wedding/marriage post titles:\n",
      "1. AITA for calling my SIL a racist after she compared my cooking to \"making kung pao chicken\"?\n",
      "2. AITA for not letting my best friend have her wedding on my property after being uninvited?\n",
      "3. AITA for ruining both my parents marriages for disowning me?\n",
      "\n",
      "============================================================\n",
      "\n",
      "=== RELATIONSHIP POSTS ANALYSIS ===\n",
      "Posts about relationships: 4979\n",
      "Percentage of all posts: 60.40%\n",
      "\n",
      "Breakdown by relationship type:\n",
      "Boyfriend   :  928 posts\n",
      "Girlfriend  :  791 posts\n",
      "Husband     : 2024 posts\n",
      "Wife        : 1788 posts\n",
      "Partner     :  353 posts\n",
      "Fianc√©      :  286 posts\n",
      "Fianc√©e     :   81 posts\n",
      "Spouse      :   51 posts\n",
      "\n",
      "=== RELATIONSHIP POST INSIGHTS ===\n",
      "Relationship posts average score: 10311.08\n",
      "Non-relationship posts average score: 10355.92\n",
      "Relationship posts score -0.4% compared to others\n",
      "\n",
      "Most discussed relationship type: 'husband' (2024 posts)\n",
      "\n",
      "Sample relationship post titles:\n",
      "1. AITA for telling my wife the lock on my daughter's door does not get removed til my brother inlaw and his daughters are out of our house?\n",
      "2. AITA for telling my extended family how many men (roughly) my sister has slept with after she outed our youngest brother as a virgin?\n",
      "3. AITA for refusing to pay for my sister's husband's surgery with my inheritance/college money?\n"
     ]
    }
   ],
   "source": [
    "# STANDARD ANSWER - Challenge 5: Text Pattern Analysis\n",
    "\n",
    "# 1. Find posts that contain the word \"family\" (case-insensitive)\n",
    "print(\"=== POSTS MENTIONING 'FAMILY' ===\")\n",
    "\n",
    "# DIRECT METHOD: Create a boolean mask for posts containing \"family\"\n",
    "family_mask = df['selftext'].str.contains('family', case=False, na=False)\n",
    "family_posts = df[family_mask]\n",
    "\n",
    "# # ULTRA-QUICK METHOD: One-liner count\n",
    "# family_count = df['selftext'].str.contains('family', case=False, na=False).sum()\n",
    "# print(f\"Quick count: {family_count} posts mention 'family'\")\n",
    "\n",
    "# Detailed analysis\n",
    "print(f\"Total posts mentioning 'family': {family_posts.shape[0]}\")\n",
    "print(f\"Percentage of all posts: {(family_posts.shape[0] / len(df)) * 100:.2f}%\")\n",
    "\n",
    "# Show some examples\n",
    "print(f\"\\nSample family-related post titles:\")\n",
    "for i, title in enumerate(family_posts['title'].head(3), 1):\n",
    "    print(f\"{i}. {title}\")\n",
    "\n",
    "# Average engagement for family posts vs others\n",
    "family_avg_score = family_posts['score'].mean()\n",
    "non_family_avg_score = df[~family_mask]['score'].mean()\n",
    "print(f\"\\nEngagement comparison:\")\n",
    "print(f\"Family posts average score: {family_avg_score:.2f}\")\n",
    "print(f\"Non-family posts average score: {non_family_avg_score:.2f}\")\n",
    "print(f\"Difference: {family_avg_score - non_family_avg_score:+.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 2. Count posts that mention \"wedding\" or \"marriage\"\n",
    "print(\"=== POSTS ABOUT WEDDINGS/MARRIAGE ===\")\n",
    "\n",
    "# DIRECT METHOD: Use regex pattern with | (OR operator) to search for either word\n",
    "wedding_marriage_mask = df['selftext'].str.contains('wedding|marriage', case=False, na=False)\n",
    "wedding_marriage_posts = df[wedding_marriage_mask]\n",
    "\n",
    "# # QUICK METHOD: Multiple one-liners for comparison\n",
    "# wedding_count = df['selftext'].str.contains('wedding', case=False, na=False).sum()\n",
    "# marriage_count = df['selftext'].str.contains('marriage', case=False, na=False).sum()\n",
    "# both_count = (df['selftext'].str.contains('wedding', case=False, na=False) & \n",
    "#               df['selftext'].str.contains('marriage', case=False, na=False)).sum()\n",
    "\n",
    "print(f\"Posts mentioning 'wedding' or 'marriage': {wedding_marriage_posts.shape[0]}\")\n",
    "print(f\"Percentage of all posts: {(wedding_marriage_posts.shape[0] / len(df)) * 100:.2f}%\")\n",
    "\n",
    "# Get breakdown using direct method\n",
    "wedding_count = df[df['selftext'].str.contains('wedding', case=False, na=False)].shape[0]\n",
    "marriage_count = df[df['selftext'].str.contains('marriage', case=False, na=False)].shape[0]\n",
    "both_mask = (df['selftext'].str.contains('wedding', case=False, na=False) & \n",
    "             df['selftext'].str.contains('marriage', case=False, na=False))\n",
    "both_count = df[both_mask].shape[0]\n",
    "\n",
    "print(f\"\\nBreakdown:\")\n",
    "print(f\"Posts with 'wedding': {wedding_count}\")\n",
    "print(f\"Posts with 'marriage': {marriage_count}\")\n",
    "print(f\"Posts with both terms: {both_count}\")\n",
    "\n",
    "# Show sample titles\n",
    "print(f\"\\nSample wedding/marriage post titles:\")\n",
    "for i, title in enumerate(wedding_marriage_posts['title'].head(3), 1):\n",
    "    print(f\"{i}. {title}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 3. Create relationship indicator column\n",
    "print(\"=== RELATIONSHIP POSTS ANALYSIS ===\")\n",
    "\n",
    "# Define relationship terms to search for\n",
    "relationship_terms = ['boyfriend', 'girlfriend', 'husband', 'wife', 'partner', 'fianc√©', 'fianc√©e', 'spouse']\n",
    "\n",
    "# DIRECT METHOD: Create a regex pattern that matches any of these terms\n",
    "# Using word boundaries \\b to ensure we match complete words\n",
    "relationship_pattern = '|'.join([f'\\\\b{term}\\\\b' for term in relationship_terms])\n",
    "\n",
    "# Create the relationship indicator column\n",
    "df['is_relationship_post'] = df['selftext'].str.contains(\n",
    "    relationship_pattern, \n",
    "    case=False, \n",
    "    na=False, \n",
    "    regex=True\n",
    ")\n",
    "\n",
    "# Get relationship posts using direct method\n",
    "relationship_posts = df[df['is_relationship_post']]\n",
    "print(f\"Posts about relationships: {relationship_posts.shape[0]}\")\n",
    "print(f\"Percentage of all posts: {(relationship_posts.shape[0] / len(df)) * 100:.2f}%\")\n",
    "\n",
    "# Count each term using direct method\n",
    "print(f\"\\nBreakdown by relationship type:\")\n",
    "for term in relationship_terms:\n",
    "    term_mask = df['selftext'].str.contains(f'\\\\b{term}\\\\b', case=False, na=False, regex=True)\n",
    "    count = df[term_mask].shape[0]\n",
    "    if count > 0:  # Only show terms that appear in the dataset\n",
    "        print(f\"{term.capitalize():12}: {count:4d} posts\")\n",
    "\n",
    "# Compare engagement: relationship posts vs non-relationship posts\n",
    "rel_avg_score = relationship_posts['score'].mean()\n",
    "non_rel_posts = df[~df['is_relationship_post']]\n",
    "non_rel_avg_score = non_rel_posts['score'].mean()\n",
    "\n",
    "print(f\"\\n=== RELATIONSHIP POST INSIGHTS ===\")\n",
    "print(f\"Relationship posts average score: {rel_avg_score:.2f}\")\n",
    "print(f\"Non-relationship posts average score: {non_rel_avg_score:.2f}\")\n",
    "print(f\"Relationship posts score {((rel_avg_score/non_rel_avg_score - 1) * 100):+.1f}% compared to others\")\n",
    "\n",
    "# Find most common relationship type using direct method\n",
    "relationship_type_counts = {}\n",
    "for term in relationship_terms:\n",
    "    term_mask = df['selftext'].str.contains(f'\\\\b{term}\\\\b', case=False, na=False, regex=True)\n",
    "    count = df[term_mask].shape[0]\n",
    "    if count > 0:\n",
    "        relationship_type_counts[term] = count\n",
    "\n",
    "if relationship_type_counts:\n",
    "    most_common = max(relationship_type_counts.items(), key=lambda x: x[1])\n",
    "    print(f\"\\nMost discussed relationship type: '{most_common[0]}' ({most_common[1]} posts)\")\n",
    "\n",
    "# Sample relationship post titles\n",
    "print(f\"\\nSample relationship post titles:\")\n",
    "for i, title in enumerate(relationship_posts['title'].head(3), 1):\n",
    "    print(f\"{i}. {title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Tip**: Use the `.str.contains()` method with pandas to search for text patterns. The `case=False` parameter makes the search case-insensitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author Analysis\n",
    "\n",
    "Let's examine the authors. Do authors post multiple times? If so, who posts the most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most active authors:\n",
      "author\n",
      "[deleted]              5\n",
      "throw_away321654987    5\n",
      "apartmentroublee       4\n",
      "Jaer56                 4\n",
      "mychickenmyrules543    4\n",
      "fukhed69               3\n",
      "aWorkProblem0          3\n",
      "sweetassugarcoldas     3\n",
      "myredditusername28     3\n",
      "twinkleglitterstars    3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Analyze posting patterns by author\n",
    "author_stats = df['author'].value_counts().head(10)\n",
    "print(\"Top 10 most active authors:\")\n",
    "print(author_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 6: Final Analysis\n",
    "\n",
    "Combine multiple pandas operations to answer this question:\n",
    "**\"What are the characteristics of the most engaging posts about relationships?\"**\n",
    "\n",
    "Create an analysis that:\n",
    "1. Filters for relationship-related posts\n",
    "2. Groups them by engagement level\n",
    "3. Calculates average text length, comment count, and any other relevant metrics\n",
    "4. Presents a clear summary of your findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPREHENSIVE RELATIONSHIP POST ANALYSIS ===\n",
      "Research Question: What are the characteristics of the most engaging posts about relationships?\n",
      "================================================================================\n",
      "Total relationship posts in dataset: 4979\n",
      "Percentage of all posts: 60.40%\n",
      "\n",
      "================================================================================\n",
      "=== RELATIONSHIP POSTS BY ENGAGEMENT LEVEL ===\n",
      "                  Post_Count  Avg_Score  Median_Score  Score_StdDev  \\\n",
      "engagement_level                                                      \n",
      "Low                     1238    4409.76        4400.0        410.95   \n",
      "Medium                  1237    6253.78        6188.0        668.18   \n",
      "High                    1267   10019.01        9781.0       1718.94   \n",
      "Viral                   1237   20573.63       18803.0       6472.40   \n",
      "\n",
      "                  Avg_Comments  Median_Comments  Avg_Text_Length  \\\n",
      "engagement_level                                                   \n",
      "Low                     764.53            636.5          2368.41   \n",
      "Medium                  939.47            767.0          2375.30   \n",
      "High                   1329.38           1083.0          2407.85   \n",
      "Viral                  2149.87           1845.0          2525.17   \n",
      "\n",
      "                  Median_Text_Length       Earliest_Post         Latest_Post  \n",
      "engagement_level                                                              \n",
      "Low                           2248.0 2018-10-18 09:17:24 2021-12-16 07:23:08  \n",
      "Medium                        2298.0 2018-10-13 14:17:50 2021-12-16 11:46:38  \n",
      "High                          2299.0 2018-10-10 03:39:34 2021-12-15 19:34:38  \n",
      "Viral                         2401.0 2018-11-06 21:12:02 2021-12-15 19:36:37  \n",
      "\n",
      "================================================================================\n",
      "=== CHARACTERISTICS OF HIGH-ENGAGEMENT RELATIONSHIP POSTS ===\n",
      "High-engagement relationship posts: 2504\n",
      "Low-engagement relationship posts: 1238\n",
      "\n",
      "Average Score:\n",
      "  High-engagement: 15233.09\n",
      "  Low-engagement:  4409.76\n",
      "  Difference: +245.4%\n",
      "\n",
      "Average Comments:\n",
      "  High-engagement: 1734.71\n",
      "  Low-engagement:  764.53\n",
      "  Difference: +126.9%\n",
      "\n",
      "Average Text Length:\n",
      "  High-engagement: 2465.81\n",
      "  Low-engagement:  2368.41\n",
      "  Difference: +4.1%\n",
      "\n",
      "================================================================================\n",
      "=== POSTING PATTERNS FOR HIGH-ENGAGEMENT RELATIONSHIP POSTS ===\n",
      "High-engagement relationship posts by day of week:\n",
      "day_of_week\n",
      "Monday       409\n",
      "Sunday       390\n",
      "Tuesday      371\n",
      "Saturday     339\n",
      "Wednesday    335\n",
      "Thursday     332\n",
      "Friday       328\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Best day for relationship posts: Monday (409 posts)\n",
      "\n",
      "Performance by year:\n",
      "         score  num_comments\n",
      "year                        \n",
      "2018  12326.00       1686.44\n",
      "2019  17043.88       2374.46\n",
      "2020  15443.81       1466.61\n",
      "2021  14302.65       1724.51\n",
      "\n",
      "================================================================================\n",
      "=== CONTENT PATTERNS IN HIGH-ENGAGEMENT RELATIONSHIP POSTS ===\n",
      "Relationship type breakdown in high-engagement posts:\n",
      "  Husband     : 1031 posts ( 41.2%)\n",
      "  Wife        : 928 posts ( 37.1%)\n",
      "  Boyfriend   : 441 posts ( 17.6%)\n",
      "  Girlfriend  : 412 posts ( 16.5%)\n",
      "  Partner     : 162 posts (  6.5%)\n",
      "  Fianc√©      : 139 posts (  5.6%)\n",
      "  Fianc√©e     :  35 posts (  1.4%)\n",
      "\n",
      "================================================================================\n",
      "=== KEY FINDINGS SUMMARY ===\n",
      "\n",
      "Characteristics of the most engaging relationship posts:\n",
      "1. LENGTH: High-engagement posts are 97 characters longer on average\n",
      "2. ENGAGEMENT: Generate 1735 comments vs 765 for low-engagement posts\n",
      "3. TIMING: Most successful on Mondays\n",
      "4. CONTENT: 'husband' posts are most common in high-engagement category\n",
      "5. RARITY: Only 24.84% of relationship posts achieve viral status\n",
      "6. FAMILY CONNECTION: 44.8% of relationship posts also mention family dynamics\n",
      "\n",
      "================================================================================\n",
      "CONCLUSION: Most engaging relationship posts tend to be longer, more detailed\n",
      "narratives that provide sufficient context for community judgment, posted on\n",
      "weekends, with 'husband' relationships being most commonly discussed.\n"
     ]
    }
   ],
   "source": [
    "# STANDARD ANSWER - Challenge 6: Final Analysis\n",
    "# \"What are the characteristics of the most engaging posts about relationships?\"\n",
    "\n",
    "print(\"=== COMPREHENSIVE RELATIONSHIP POST ANALYSIS ===\")\n",
    "print(\"Research Question: What are the characteristics of the most engaging posts about relationships?\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Step 1: Filter for relationship-related posts\n",
    "relationship_posts = df[df['is_relationship_post'] == True].copy()\n",
    "print(f\"Total relationship posts in dataset: {len(relationship_posts)}\")\n",
    "print(f\"Percentage of all posts: {(len(relationship_posts) / len(df)) * 100:.2f}%\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "\n",
    "# Step 2: Group by engagement level and analyze characteristics\n",
    "print(\"=== RELATIONSHIP POSTS BY ENGAGEMENT LEVEL ===\")\n",
    "\n",
    "# Create comprehensive analysis by engagement level\n",
    "engagement_analysis = relationship_posts.groupby('engagement_level').agg({\n",
    "    'score': ['count', 'mean', 'median', 'std'],\n",
    "    'num_comments': ['mean', 'median'],\n",
    "    'text_length': ['mean', 'median'],\n",
    "    'created_date': ['min', 'max']  # Date range\n",
    "}).round(2)\n",
    "\n",
    "# Flatten column names for easier reading\n",
    "engagement_analysis.columns = [\n",
    "    'Post_Count', 'Avg_Score', 'Median_Score', 'Score_StdDev',\n",
    "    'Avg_Comments', 'Median_Comments',\n",
    "    'Avg_Text_Length', 'Median_Text_Length',\n",
    "    'Earliest_Post', 'Latest_Post'\n",
    "]\n",
    "\n",
    "# Sort by engagement level order\n",
    "level_order = ['Low', 'Medium', 'High', 'Viral']\n",
    "engagement_analysis = engagement_analysis.reindex(level_order)\n",
    "\n",
    "print(engagement_analysis)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "\n",
    "# Step 3: Deep dive into high-engagement relationship posts\n",
    "print(\"=== CHARACTERISTICS OF HIGH-ENGAGEMENT RELATIONSHIP POSTS ===\")\n",
    "\n",
    "high_engagement = relationship_posts[relationship_posts['engagement_level'].isin(['High', 'Viral'])]\n",
    "low_engagement = relationship_posts[relationship_posts['engagement_level'] == 'Low']\n",
    "\n",
    "print(f\"High-engagement relationship posts: {len(high_engagement)}\")\n",
    "print(f\"Low-engagement relationship posts: {len(low_engagement)}\")\n",
    "\n",
    "# Compare characteristics\n",
    "characteristics = {\n",
    "    'Average Score': {\n",
    "        'High-Engagement': high_engagement['score'].mean(),\n",
    "        'Low-Engagement': low_engagement['score'].mean()\n",
    "    },\n",
    "    'Average Comments': {\n",
    "        'High-Engagement': high_engagement['num_comments'].mean(),\n",
    "        'Low-Engagement': low_engagement['num_comments'].mean()\n",
    "    },\n",
    "    'Average Text Length': {\n",
    "        'High-Engagement': high_engagement['text_length'].mean(),\n",
    "        'Low-Engagement': low_engagement['text_length'].mean()\n",
    "    }\n",
    "}\n",
    "\n",
    "for metric, values in characteristics.items():\n",
    "    high_val = values['High-Engagement']\n",
    "    low_val = values['Low-Engagement']\n",
    "    diff_pct = ((high_val / low_val) - 1) * 100 if low_val > 0 else 0\n",
    "    print(f\"\\n{metric}:\")\n",
    "    print(f\"  High-engagement: {high_val:.2f}\")\n",
    "    print(f\"  Low-engagement:  {low_val:.2f}\")\n",
    "    print(f\"  Difference: {diff_pct:+.1f}%\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "\n",
    "# Step 4: Analyze posting patterns (day of week, time trends)\n",
    "print(\"=== POSTING PATTERNS FOR HIGH-ENGAGEMENT RELATIONSHIP POSTS ===\")\n",
    "\n",
    "# Day of week analysis\n",
    "day_analysis = high_engagement['day_of_week'].value_counts()\n",
    "print(\"High-engagement relationship posts by day of week:\")\n",
    "print(day_analysis.sort_values(ascending=False))\n",
    "\n",
    "# Most successful day\n",
    "best_day = day_analysis.idxmax()\n",
    "print(f\"\\nBest day for relationship posts: {best_day} ({day_analysis[best_day]} posts)\")\n",
    "\n",
    "# Year analysis (if multiple years in data)\n",
    "if high_engagement['year'].nunique() > 1:\n",
    "    year_analysis = high_engagement.groupby('year').agg({\n",
    "        'score': 'mean',\n",
    "        'num_comments': 'mean'\n",
    "    }).round(2)\n",
    "    print(f\"\\nPerformance by year:\")\n",
    "    print(year_analysis)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "\n",
    "# Step 5: Content analysis of high-engagement relationship posts\n",
    "print(\"=== CONTENT PATTERNS IN HIGH-ENGAGEMENT RELATIONSHIP POSTS ===\")\n",
    "\n",
    "# Analyze specific relationship terms in high-engagement posts\n",
    "relationship_terms = ['boyfriend', 'girlfriend', 'husband', 'wife', 'partner', 'fianc√©', 'fianc√©e']\n",
    "print(\"Relationship type breakdown in high-engagement posts:\")\n",
    "\n",
    "term_analysis = {}\n",
    "for term in relationship_terms:\n",
    "    pattern = f'\\\\b{term}\\\\b'\n",
    "    count = high_engagement['selftext'].str.contains(pattern, case=False, na=False, regex=True).sum()\n",
    "    if count > 0:\n",
    "        term_analysis[term] = count\n",
    "\n",
    "# Sort by frequency\n",
    "term_analysis = dict(sorted(term_analysis.items(), key=lambda x: x[1], reverse=True))\n",
    "for term, count in term_analysis.items():\n",
    "    pct = (count / len(high_engagement)) * 100\n",
    "    print(f\"  {term.capitalize():12}: {count:3d} posts ({pct:5.1f}%)\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "\n",
    "# Step 6: Key findings summary\n",
    "print(\"=== KEY FINDINGS SUMMARY ===\")\n",
    "print(\"\\nCharacteristics of the most engaging relationship posts:\")\n",
    "print(f\"1. LENGTH: High-engagement posts are {characteristics['Average Text Length']['High-Engagement'] - characteristics['Average Text Length']['Low-Engagement']:.0f} characters longer on average\")\n",
    "print(f\"2. ENGAGEMENT: Generate {characteristics['Average Comments']['High-Engagement']:.0f} comments vs {characteristics['Average Comments']['Low-Engagement']:.0f} for low-engagement posts\")\n",
    "print(f\"3. TIMING: Most successful on {best_day}s\")\n",
    "\n",
    "# Calculate content insights\n",
    "high_engagement_viral_pct = (len(relationship_posts[relationship_posts['engagement_level'] == 'Viral']) / len(relationship_posts)) * 100\n",
    "most_common_relationship = max(term_analysis.items(), key=lambda x: x[1])[0] if term_analysis else \"partner\"\n",
    "\n",
    "print(f\"4. CONTENT: '{most_common_relationship}' posts are most common in high-engagement category\")\n",
    "print(f\"5. RARITY: Only {high_engagement_viral_pct:.2f}% of relationship posts achieve viral status\")\n",
    "\n",
    "# Final insight: correlation with family posts\n",
    "family_relationship_overlap = relationship_posts['selftext'].str.contains('family', case=False, na=False).sum()\n",
    "overlap_pct = (family_relationship_overlap / len(relationship_posts)) * 100\n",
    "print(f\"6. FAMILY CONNECTION: {overlap_pct:.1f}% of relationship posts also mention family dynamics\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CONCLUSION: Most engaging relationship posts tend to be longer, more detailed\")\n",
    "print(\"narratives that provide sufficient context for community judgment, posted on\")\n",
    "print(f\"weekends, with '{most_common_relationship}' relationships being most commonly discussed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Warning**: When working with text data, always be mindful of missing values and different text encodings that might cause unexpected results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## ‚ùó Key Points\n",
    "\n",
    "* Pandas provides powerful tools for loading, cleaning, and exploring real-world datasets.\n",
    "* Always start data analysis by understanding your dataset structure and checking for data quality issues.\n",
    "* The `.groupby()` method is essential for aggregating data and finding patterns across categories.\n",
    "* Text data requires special handling, including case-insensitive searches and pattern matching.\n",
    "* Correlation analysis helps identify relationships between numerical variables.\n",
    "* Creating categorical variables from continuous data enables different types of analysis.\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
