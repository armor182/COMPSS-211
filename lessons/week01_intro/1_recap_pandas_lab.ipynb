{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summer Recap: Data Analysis with Pandas\n",
    "\n",
    "* * * \n",
    "\n",
    "<div class=\"alert alert-success\">  \n",
    "    \n",
    "### Learning Objectives \n",
    "    \n",
    "* Review fundamental pandas operations for data manipulation and analysis.\n",
    "* Apply data cleaning techniques to real-world social science datasets.\n",
    "* Practice exploratory data analysis using descriptive statistics and basic visualizations.\n",
    "* Demonstrate ability to filter, group, and aggregate data using pandas methods.\n",
    "* Evaluate LLM-generated code for accuracy and best practices.\n",
    "</div>\n",
    "\n",
    "### Icons Used in This Notebook\n",
    "üîî **Question**: A quick question to help you understand what's going on.<br>\n",
    "ü•ä **Challenge**: Interactive exercise. We'll work through these in the workshop!<br>\n",
    "üí° **Tip**: How to do something a bit more efficiently or effectively.<br>\n",
    "‚ö†Ô∏è **Warning:** Heads-up about tricky stuff or common mistakes.<br>\n",
    "ü§ñ **AI Generated**: Code generated by an LLM that we'll test and debug.<br>\n",
    "\n",
    "### Sections\n",
    "1. [Data Loading and Initial Exploration](#section1)\n",
    "2. [Data Cleaning and Basic Operations](#section2)\n",
    "3. [Exploratory Data Analysis](#section3)\n",
    "4. [Text Analysis Fundamentals](#section4)\n",
    "5. [Working with LLM-Generated Code](#section5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "\n",
    "# Data Loading and Initial Exploration\n",
    "\n",
    "Today we'll work with data from Reddit's \"Am I the Asshole?\" (AITA) subreddit. This dataset contains posts where people describe situations and ask for community judgment about their behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (10000, 23)\n",
      "\n",
      "Column names:\n",
      "['idint', 'idstr', 'created', 'self', 'nsfw', 'author', 'title', 'url', 'selftext', 'score', 'subreddit', 'distinguish', 'textlen', 'num_comments', 'flair_text', 'flair_css_class', 'augmented_at', 'augmented_count', 'created_date', 'year', 'month', 'day_of_week', 'text_length']\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../../data/aita_top_subs.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumn names:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idint</th>\n",
       "      <th>idstr</th>\n",
       "      <th>created</th>\n",
       "      <th>self</th>\n",
       "      <th>nsfw</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>selftext</th>\n",
       "      <th>score</th>\n",
       "      <th>...</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>flair_text</th>\n",
       "      <th>flair_css_class</th>\n",
       "      <th>augmented_at</th>\n",
       "      <th>augmented_count</th>\n",
       "      <th>created_date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>797709732</td>\n",
       "      <td>t3_d6xoro</td>\n",
       "      <td>1568998300</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>DarthCharizard</td>\n",
       "      <td>META: This sub is moving towards a value syste...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I‚Äôve enjoyed reading and posting on this sub f...</td>\n",
       "      <td>80915.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6215.0</td>\n",
       "      <td>META</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-09-20 16:51:40</td>\n",
       "      <td>2019</td>\n",
       "      <td>9</td>\n",
       "      <td>Friday</td>\n",
       "      <td>3266.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1472895100</td>\n",
       "      <td>t3_ocx94s</td>\n",
       "      <td>1625315782</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>OnlyInQuebec9</td>\n",
       "      <td>AITA for telling my wife the lock on my daught...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My brother in-law (Sammy) lost his home shortl...</td>\n",
       "      <td>80334.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5318.0</td>\n",
       "      <td>Not the A-hole</td>\n",
       "      <td>not</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-07-03 12:36:22</td>\n",
       "      <td>2021</td>\n",
       "      <td>7</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>2664.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>664921441</td>\n",
       "      <td>t3_azvko1</td>\n",
       "      <td>1552322462</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Renegadesrule33</td>\n",
       "      <td>UPDATE, AITA for despising my mentally handica...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm back like I said I would be,. My [original...</td>\n",
       "      <td>72776.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>UPDATE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-03-11 16:41:02</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>Monday</td>\n",
       "      <td>5437.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>855862814</td>\n",
       "      <td>t3_e5k3z2</td>\n",
       "      <td>1575392873</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>throwRA-fhfsveyary</td>\n",
       "      <td>AITA for pretending to get fired when customer...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I am a high schooler with a weekend job at a c...</td>\n",
       "      <td>63526.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3645.0</td>\n",
       "      <td>Not the A-hole</td>\n",
       "      <td>not</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-12-03 17:07:53</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>2096.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>756636047</td>\n",
       "      <td>t3_cihc3z</td>\n",
       "      <td>1564233111</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Thunderbear998</td>\n",
       "      <td>AITA for telling my extended family how many m...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We had a family dinner this evening. My family...</td>\n",
       "      <td>54132.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5190.0</td>\n",
       "      <td>Everyone Sucks</td>\n",
       "      <td>ass</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-07-27 13:11:51</td>\n",
       "      <td>2019</td>\n",
       "      <td>7</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1662.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        idint      idstr     created  self  nsfw              author  \\\n",
       "0   797709732  t3_d6xoro  1568998300   1.0   0.0      DarthCharizard   \n",
       "1  1472895100  t3_ocx94s  1625315782   1.0   0.0       OnlyInQuebec9   \n",
       "2   664921441  t3_azvko1  1552322462   1.0   0.0     Renegadesrule33   \n",
       "3   855862814  t3_e5k3z2  1575392873   1.0   0.0  throwRA-fhfsveyary   \n",
       "4   756636047  t3_cihc3z  1564233111   1.0   0.0      Thunderbear998   \n",
       "\n",
       "                                               title  url  \\\n",
       "0  META: This sub is moving towards a value syste...  NaN   \n",
       "1  AITA for telling my wife the lock on my daught...  NaN   \n",
       "2  UPDATE, AITA for despising my mentally handica...  NaN   \n",
       "3  AITA for pretending to get fired when customer...  NaN   \n",
       "4  AITA for telling my extended family how many m...  NaN   \n",
       "\n",
       "                                            selftext    score  ...  \\\n",
       "0  I‚Äôve enjoyed reading and posting on this sub f...  80915.0  ...   \n",
       "1  My brother in-law (Sammy) lost his home shortl...  80334.0  ...   \n",
       "2  I'm back like I said I would be,. My [original...  72776.0  ...   \n",
       "3  I am a high schooler with a weekend job at a c...  63526.0  ...   \n",
       "4  We had a family dinner this evening. My family...  54132.0  ...   \n",
       "\n",
       "  num_comments      flair_text  flair_css_class  augmented_at augmented_count  \\\n",
       "0       6215.0            META              NaN           NaN             NaN   \n",
       "1       5318.0  Not the A-hole              not           NaN             NaN   \n",
       "2       1989.0          UPDATE              NaN           NaN             NaN   \n",
       "3       3645.0  Not the A-hole              not           NaN             NaN   \n",
       "4       5190.0  Everyone Sucks              ass           NaN             NaN   \n",
       "\n",
       "          created_date  year  month day_of_week  text_length  \n",
       "0  2019-09-20 16:51:40  2019      9      Friday       3266.0  \n",
       "1  2021-07-03 12:36:22  2021      7    Saturday       2664.0  \n",
       "2  2019-03-11 16:41:02  2019      3      Monday       5437.0  \n",
       "3  2019-12-03 17:07:53  2019     12     Tuesday       2096.0  \n",
       "4  2019-07-27 13:11:51  2019      7    Saturday       1662.0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 1: Data Overview\n",
    "\n",
    "Explore the dataset structure and provide a summary of what you find. Use pandas methods to:\n",
    "1. Check the data types of each column\n",
    "2. Look for missing values\n",
    "3. Get basic descriptive statistics for numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA TYPES ===\n",
      "idint                int64\n",
      "idstr               object\n",
      "created              int64\n",
      "self               float64\n",
      "nsfw               float64\n",
      "author              object\n",
      "title               object\n",
      "url                float64\n",
      "selftext            object\n",
      "score              float64\n",
      "subreddit           object\n",
      "distinguish         object\n",
      "textlen            float64\n",
      "num_comments       float64\n",
      "flair_text          object\n",
      "flair_css_class     object\n",
      "augmented_at       float64\n",
      "augmented_count    float64\n",
      "created_date        object\n",
      "year                 int64\n",
      "month                int64\n",
      "day_of_week         object\n",
      "text_length        float64\n",
      "dtype: object\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== MISSING VALUES ===\n",
      "Missing value counts:\n",
      "idint                  0\n",
      "idstr                  0\n",
      "created                0\n",
      "self                   0\n",
      "nsfw                   0\n",
      "author                 0\n",
      "title                  0\n",
      "url                10000\n",
      "selftext               4\n",
      "score                  0\n",
      "subreddit              0\n",
      "distinguish         9999\n",
      "textlen                0\n",
      "num_comments           0\n",
      "flair_text          1175\n",
      "flair_css_class     1441\n",
      "augmented_at       10000\n",
      "augmented_count    10000\n",
      "created_date           0\n",
      "year                   0\n",
      "month                  0\n",
      "day_of_week            0\n",
      "text_length            4\n",
      "dtype: int64\n",
      "\n",
      "Missing value percentages:\n",
      "url: 100.00%\n",
      "selftext: 0.04%\n",
      "distinguish: 99.99%\n",
      "flair_text: 11.75%\n",
      "flair_css_class: 14.41%\n",
      "augmented_at: 100.00%\n",
      "augmented_count: 100.00%\n",
      "text_length: 0.04%\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== DESCRIPTIVE STATISTICS ===\n",
      "              idint       created     self          nsfw  url         score  \\\n",
      "count  1.000000e+04  1.000000e+04  10000.0  10000.000000  0.0  10000.000000   \n",
      "mean   1.213800e+09  1.604749e+09      1.0      0.005300  NaN  10137.508500   \n",
      "std    2.832943e+08  2.370645e+07      0.0      0.072612  NaN   6987.893464   \n",
      "min    5.826428e+08  1.539143e+09      1.0      0.000000  NaN   3739.000000   \n",
      "25%    1.003247e+09  1.589699e+09      1.0      0.000000  NaN   5168.750000   \n",
      "50%    1.224636e+09  1.607434e+09      1.0      0.000000  NaN   7437.000000   \n",
      "75%    1.472430e+09  1.625272e+09      1.0      0.000000  NaN  13128.250000   \n",
      "max    1.662312e+09  1.639655e+09      1.0      1.000000  NaN  80915.000000   \n",
      "\n",
      "           textlen  num_comments  augmented_at  augmented_count          year  \\\n",
      "count  10000.00000  10000.000000           0.0              0.0  10000.000000   \n",
      "mean    1878.93120   1218.262500           NaN              NaN   2020.303200   \n",
      "std      830.80909    946.524966           NaN              NaN      0.737105   \n",
      "min        0.00000    321.000000           NaN              NaN   2018.000000   \n",
      "25%     1347.00000    591.000000           NaN              NaN   2020.000000   \n",
      "50%     1934.00000    911.000000           NaN              NaN   2020.000000   \n",
      "75%     2532.00000   1505.000000           NaN              NaN   2021.000000   \n",
      "max     7750.00000  10265.000000           NaN              NaN   2021.000000   \n",
      "\n",
      "              month   text_length  \n",
      "count  10000.000000   9996.000000  \n",
      "mean       7.098600   1924.839236  \n",
      "std        3.330734   1305.271474  \n",
      "min        1.000000      1.000000  \n",
      "25%        5.000000   1146.000000  \n",
      "50%        7.000000   1971.000000  \n",
      "75%       10.000000   2756.000000  \n",
      "max       12.000000  15084.000000  \n",
      "\n",
      "=== ADDITIONAL INFO ===\n",
      "Dataset shape: (10000, 23) (rows, columns)\n",
      "Memory usage: 33.87 MB\n",
      "\n",
      "Unique authors: 9723\n",
      "Sample authors: ['DarthCharizard' 'OnlyInQuebec9' 'Renegadesrule33' 'throwRA-fhfsveyary'\n",
      " 'Thunderbear998']\n"
     ]
    }
   ],
   "source": [
    "# STANDARD ANSWER - Challenge 1: Data Overview\n",
    "\n",
    "# 1. Check the data types of each column\n",
    "print(\"=== DATA TYPES ===\")\n",
    "print(df.dtypes)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# 2. Look for missing values\n",
    "print(\"=== MISSING VALUES ===\")\n",
    "# Method 1: Count of missing values per column\n",
    "missing_counts = df.isnull().sum()\n",
    "print(\"Missing value counts:\")\n",
    "print(missing_counts)\n",
    "\n",
    "# Method 2: Percentage of missing values (more informative for large datasets)\n",
    "missing_percentages = (df.isnull().sum() / len(df)) * 100\n",
    "print(\"\\nMissing value percentages:\")\n",
    "for col, pct in missing_percentages.items():\n",
    "    if pct > 0:\n",
    "        print(f\"{col}: {pct:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# 3. Get basic descriptive statistics for numerical columns\n",
    "print(\"=== DESCRIPTIVE STATISTICS ===\")\n",
    "# Using describe() gives us count, mean, std, min, 25%, 50%, 75%, max\n",
    "numerical_stats = df.describe()\n",
    "print(numerical_stats)\n",
    "\n",
    "# Additional useful info: data shape and memory usage\n",
    "print(f\"\\n=== ADDITIONAL INFO ===\")\n",
    "print(f\"Dataset shape: {df.shape} (rows, columns)\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Show unique values for categorical columns (first few)\n",
    "print(f\"\\nUnique authors: {df['author'].nunique()}\")\n",
    "print(f\"Sample authors: {df['author'].unique()[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîî **Question**: What do you notice about the `selftext` column? What might this tell us about the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "\n",
    "# Data Cleaning and Basic Operations\n",
    "\n",
    "Real-world data often requires cleaning before analysis. Let's examine our dataset for common issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n",
      "\n",
      "Score statistics:\n",
      "count    10000.000000\n",
      "mean     10137.508500\n",
      "std       6987.893464\n",
      "min       3739.000000\n",
      "25%       5168.750000\n",
      "50%       7437.000000\n",
      "75%      13128.250000\n",
      "max      80915.000000\n",
      "Name: score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate posts\n",
    "print(f\"Number of duplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "# Look at the distribution of some key variables\n",
    "print(f\"\\nScore statistics:\")\n",
    "print(df['score'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 2: Data Cleaning\n",
    "\n",
    "Clean the dataset by:\n",
    "1. Removing any posts where `selftext` is missing or empty\n",
    "2. Creating a new column called `text_length` that contains the character count of `selftext`\n",
    "3. Filter out posts that are shorter than 100 characters (likely low-quality posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (10000, 23)\n",
      "\n",
      "Selftext column analysis:\n",
      "Missing values: 4\n",
      "Empty strings: 0\n",
      "After removing missing selftext: (9996, 23)\n",
      "After removing empty selftext: (9996, 23)\n",
      "\n",
      "Text length statistics:\n",
      "count     9996.000000\n",
      "mean      1924.839236\n",
      "std       1305.271474\n",
      "min          1.000000\n",
      "25%       1146.000000\n",
      "50%       1971.000000\n",
      "75%       2756.000000\n",
      "max      15084.000000\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "Posts with less than 100 characters: 1753\n",
      "After filtering short posts: (8243, 23)\n",
      "\n",
      "=== CLEANING SUMMARY ===\n",
      "Original rows: 10000\n",
      "Final rows: 8243\n",
      "Rows removed: 1757\n",
      "Percentage retained: 82.4%\n",
      "\n",
      "Final text length distribution:\n",
      "count     8243.000000\n",
      "mean      2332.272838\n",
      "std       1058.019949\n",
      "min        209.000000\n",
      "25%       1614.000000\n",
      "50%       2232.000000\n",
      "75%       2883.000000\n",
      "max      15084.000000\n",
      "Name: text_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# STANDARD ANSWER - Challenge 2: Data Cleaning\n",
    "\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "\n",
    "# Step 1: Remove posts where selftext is missing or empty\n",
    "# Check current state of selftext column\n",
    "print(f\"\\nSelftext column analysis:\")\n",
    "print(f\"Missing values: {df['selftext'].isnull().sum()}\")\n",
    "print(f\"Empty strings: {(df['selftext'] == '').sum()}\")\n",
    "\n",
    "# Remove missing values first\n",
    "df_clean = df.dropna(subset=['selftext']).copy()\n",
    "print(f\"After removing missing selftext: {df_clean.shape}\")\n",
    "\n",
    "# Remove empty strings (posts with no content)\n",
    "df_clean = df_clean[df_clean['selftext'] != ''].copy()\n",
    "print(f\"After removing empty selftext: {df_clean.shape}\")\n",
    "\n",
    "# Step 2: Create text_length column\n",
    "# Calculate character count for each selftext\n",
    "df_clean['text_length'] = df_clean['selftext'].str.len()\n",
    "\n",
    "print(f\"\\nText length statistics:\")\n",
    "print(df_clean['text_length'].describe())\n",
    "\n",
    "# Step 3: Filter out posts shorter than 100 characters\n",
    "# These are likely low-quality or incomplete posts\n",
    "print(f\"\\nPosts with less than 100 characters: {(df_clean['text_length'] < 100).sum()}\")\n",
    "\n",
    "df_clean = df_clean[df_clean['text_length'] >= 100].copy()\n",
    "print(f\"After filtering short posts: {df_clean.shape}\")\n",
    "\n",
    "# Summary of cleaning process\n",
    "print(f\"\\n=== CLEANING SUMMARY ===\")\n",
    "print(f\"Original rows: {df.shape[0]}\")\n",
    "print(f\"Final rows: {df_clean.shape[0]}\")\n",
    "print(f\"Rows removed: {df.shape[0] - df_clean.shape[0]}\")\n",
    "print(f\"Percentage retained: {(df_clean.shape[0] / df.shape[0]) * 100:.1f}%\")\n",
    "\n",
    "# Update our main dataframe for subsequent analysis\n",
    "df = df_clean.copy()\n",
    "\n",
    "print(f\"\\nFinal text length distribution:\")\n",
    "print(df['text_length'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Tip**: Use the `.str.len()` method to get string lengths in pandas. Remember that missing values might cause issues, so handle them first!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Dates\n",
    "\n",
    "The `created` column contains Unix timestamps. Let's convert these to readable dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date range in dataset:\n",
      "From: 2018-10-10 03:39:34\n",
      "To: 2021-12-16 11:46:38\n"
     ]
    }
   ],
   "source": [
    "# Convert Unix timestamp to datetime\n",
    "df['created_date'] = pd.to_datetime(df['created'], unit='s')\n",
    "\n",
    "# Extract useful date components\n",
    "df['year'] = df['created_date'].dt.year\n",
    "df['month'] = df['created_date'].dt.month\n",
    "df['day_of_week'] = df['created_date'].dt.day_name()\n",
    "\n",
    "print(\"Date range in dataset:\")\n",
    "print(f\"From: {df['created_date'].min()}\")\n",
    "print(f\"To: {df['created_date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "\n",
    "# Exploratory Data Analysis\n",
    "\n",
    "Now let's explore patterns in the data using pandas grouping and aggregation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 3: Score Analysis\n",
    "\n",
    "Analyze post popularity by:\n",
    "1. Finding the top 10 posts by score\n",
    "2. Calculating the average score by year\n",
    "3. Determining which day of the week gets the highest average scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TOP 10 POSTS BY SCORE ===\n",
      "                                               title    score  num_comments  \\\n",
      "0  META: This sub is moving towards a value syste...  80915.0        6215.0   \n",
      "1  AITA for telling my wife the lock on my daught...  80334.0        5318.0   \n",
      "2  UPDATE, AITA for despising my mentally handica...  72776.0        1989.0   \n",
      "3  AITA for pretending to get fired when customer...  63526.0        3645.0   \n",
      "4  AITA for telling my extended family how many m...  54132.0        5190.0   \n",
      "5  AITA for \"announcing\" that my dad's not paying...  51323.0        2883.0   \n",
      "6  AITA for refusing to pay for my sister's husba...  49967.0        6414.0   \n",
      "7  [UPDATE] AITA for telling an employee she can ...  48572.0        2244.0   \n",
      "8  UPDATE AITA for not sharing my medical history...  47893.0         598.0   \n",
      "9  AITA for telling my son he deserved his gf bre...  47771.0        4008.0   \n",
      "\n",
      "                 author  \n",
      "0        DarthCharizard  \n",
      "1         OnlyInQuebec9  \n",
      "2       Renegadesrule33  \n",
      "3    throwRA-fhfsveyary  \n",
      "4        Thunderbear998  \n",
      "5             yeasothat  \n",
      "6        Home-Time-6077  \n",
      "7       Absolut_Failure  \n",
      "8       Drudawgthedrood  \n",
      "9  inappropriatedress77  \n",
      "\n",
      "Highest scoring post details:\n",
      "Score: 80915.0\n",
      "Title: META: This sub is moving towards a value system that frequently doesn't align with the rest of the world\n",
      "Author: DarthCharizard\n",
      "Text length: 3266 characters\n",
      "\n",
      "============================================================\n",
      "\n",
      "=== AVERAGE SCORE BY YEAR ===\n",
      "      Average Score  Post Count  Standard Deviation\n",
      "year                                               \n",
      "2018        7622.18          66             3917.20\n",
      "2019       12534.59        1248             9096.80\n",
      "2020       10552.52        3286             7280.29\n",
      "2021        9420.47        3643             6268.37\n",
      "\n",
      "Best performing year: 2019 with average score of 12534.59\n",
      "\n",
      "============================================================\n",
      "\n",
      "=== AVERAGE SCORE BY DAY OF WEEK ===\n",
      "             Average Score  Post Count  Median Score\n",
      "day_of_week                                         \n",
      "Sunday            10626.65        1191        7798.0\n",
      "Tuesday           10421.06        1213        7801.0\n",
      "Wednesday         10316.58        1178        7281.0\n",
      "Saturday          10303.77        1091        7479.0\n",
      "Thursday          10242.29        1194        7206.0\n",
      "Friday            10198.37        1111        7373.0\n",
      "Monday            10189.32        1265        7589.0\n",
      "\n",
      "Best performing day: Sunday\n",
      "Average score on Sunday: 10626.65\n",
      "This is based on 1191 posts\n",
      "\n",
      "=== WEEKEND vs WEEKDAY COMPARISON ===\n",
      "Weekday average score: 10273.92\n",
      "Weekend average score: 10472.28\n",
      "Weekend posts score +1.9% compared to weekdays\n"
     ]
    }
   ],
   "source": [
    "# STANDARD ANSWER - Challenge 3: Score Analysis\n",
    "\n",
    "# 1. Find the top 10 posts by score\n",
    "print(\"=== TOP 10 POSTS BY SCORE ===\")\n",
    "top_posts = df.nlargest(10, 'score')[['title', 'score', 'num_comments', 'author']]\n",
    "print(top_posts)\n",
    "\n",
    "# Let's also show some details about the highest scoring post\n",
    "print(f\"\\nHighest scoring post details:\")\n",
    "highest_post = df.loc[df['score'].idxmax()]\n",
    "print(f\"Score: {highest_post['score']}\")\n",
    "print(f\"Title: {highest_post['title']}\")\n",
    "print(f\"Author: {highest_post['author']}\")\n",
    "print(f\"Text length: {highest_post['text_length']} characters\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 2. Calculate average score by year\n",
    "print(\"=== AVERAGE SCORE BY YEAR ===\")\n",
    "# Group by year and calculate mean score\n",
    "yearly_scores = df.groupby('year')['score'].agg(['mean', 'count', 'std']).round(2)\n",
    "yearly_scores.columns = ['Average Score', 'Post Count', 'Standard Deviation']\n",
    "print(yearly_scores)\n",
    "\n",
    "# Add some context about the year with highest average\n",
    "best_year = yearly_scores['Average Score'].idxmax()\n",
    "print(f\"\\nBest performing year: {best_year} with average score of {yearly_scores.loc[best_year, 'Average Score']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 3. Determine which day of the week gets highest average scores\n",
    "print(\"=== AVERAGE SCORE BY DAY OF WEEK ===\")\n",
    "# Group by day of week and calculate statistics\n",
    "daily_scores = df.groupby('day_of_week')['score'].agg(['mean', 'count', 'median']).round(2)\n",
    "daily_scores.columns = ['Average Score', 'Post Count', 'Median Score']\n",
    "\n",
    "# Sort by average score (descending) to see best performing days\n",
    "daily_scores_sorted = daily_scores.sort_values('Average Score', ascending=False)\n",
    "print(daily_scores_sorted)\n",
    "\n",
    "# Highlight the best day\n",
    "best_day = daily_scores_sorted.index[0]\n",
    "print(f\"\\nBest performing day: {best_day}\")\n",
    "print(f\"Average score on {best_day}: {daily_scores_sorted.loc[best_day, 'Average Score']}\")\n",
    "print(f\"This is based on {daily_scores_sorted.loc[best_day, 'Post Count']} posts\")\n",
    "\n",
    "# Additional insight: Weekend vs Weekday analysis\n",
    "weekend_days = ['Saturday', 'Sunday']\n",
    "weekday_avg = df[~df['day_of_week'].isin(weekend_days)]['score'].mean()\n",
    "weekend_avg = df[df['day_of_week'].isin(weekend_days)]['score'].mean()\n",
    "\n",
    "print(f\"\\n=== WEEKEND vs WEEKDAY COMPARISON ===\")\n",
    "print(f\"Weekday average score: {weekday_avg:.2f}\")\n",
    "print(f\"Weekend average score: {weekend_avg:.2f}\")\n",
    "print(f\"Weekend posts score {((weekend_avg/weekday_avg - 1) * 100):+.1f}% compared to weekdays\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment Engagement Analysis\n",
    "\n",
    "Now, let's explore how users interact with posts by analyzing the volume and distribution of comments, which can highlight engagement patterns and community response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation matrix:\n",
      "              text_length     score  num_comments\n",
      "text_length      1.000000  0.049182      0.037226\n",
      "score            0.049182  1.000000      0.561018\n",
      "num_comments     0.037226  0.561018      1.000000\n"
     ]
    }
   ],
   "source": [
    "# Explore the relationship between text length and engagement\n",
    "correlation = df[['text_length', 'score', 'num_comments']].corr()\n",
    "print(\"Correlation matrix:\")\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîî **Question**: What does the correlation tell us about the relationship between post length and engagement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 4: Engagement Categories\n",
    "\n",
    "Create engagement categories and analyze them:\n",
    "1. Create a new column `engagement_level` with categories:\n",
    "   - 'Low': score < 100\n",
    "   - 'Medium': score 100-500\n",
    "   - 'High': score 500-2000\n",
    "   - 'Viral': score > 2000\n",
    "2. Calculate the percentage of posts in each category\n",
    "3. Find the average text length for each engagement level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CREATING ENGAGEMENT CATEGORIES ===\n",
      "Score distribution:\n",
      "count     8243.000000\n",
      "mean     10328.836710\n",
      "std       7232.502385\n",
      "min       3739.000000\n",
      "25%       5167.500000\n",
      "50%       7503.000000\n",
      "75%      13445.000000\n",
      "max      80915.000000\n",
      "Name: score, dtype: float64\n",
      "Engagement categories created successfully!\n",
      "Sample categories: 0    Viral\n",
      "1    Viral\n",
      "2    Viral\n",
      "3    Viral\n",
      "4    Viral\n",
      "Name: engagement_level, dtype: object\n",
      "\n",
      "============================================================\n",
      "\n",
      "=== ENGAGEMENT LEVEL DISTRIBUTION ===\n",
      "                  Count  Percentage\n",
      "engagement_level                   \n",
      "Viral              8243       100.0\n",
      "\n",
      "Total posts analyzed: 8243\n",
      "Most common engagement level: Viral\n",
      "Viral posts (rare content): 8243 posts (100.00%)\n",
      "\n",
      "============================================================\n",
      "\n",
      "=== TEXT LENGTH BY ENGAGEMENT LEVEL ===\n",
      "                  Average Length  Median Length  Std Deviation  Post Count\n",
      "engagement_level                                                          \n",
      "Low                          NaN            NaN            NaN         NaN\n",
      "Medium                       NaN            NaN            NaN         NaN\n",
      "High                         NaN            NaN            NaN         NaN\n",
      "Viral                    2332.27         2232.0        1058.02      8243.0\n",
      "\n",
      "=== KEY INSIGHTS ===\n",
      "Correlation between text length and score: 0.049\n",
      "Viral posts are on average nan characters longer than low-engagement posts\n",
      "\n",
      "=== TEXT LENGTH RANGES BY ENGAGEMENT ===\n",
      "Low    engagement:   nan -    nan characters (range:    nan)\n",
      "Medium engagement:   nan -    nan characters (range:    nan)\n",
      "High   engagement:   nan -    nan characters (range:    nan)\n",
      "Viral  engagement:   209 -  15084 characters (range:  14875)\n"
     ]
    }
   ],
   "source": [
    "# STANDARD ANSWER - Challenge 4: Engagement Categories\n",
    "\n",
    "# 1. Create engagement_level column with categories based on score\n",
    "print(\"=== CREATING ENGAGEMENT CATEGORIES ===\")\n",
    "\n",
    "# First, let's see the score distribution to understand our categorization\n",
    "print(\"Score distribution:\")\n",
    "print(df['score'].describe())\n",
    "\n",
    "# Create engagement categories using pd.cut() or conditional logic\n",
    "# Using conditional logic for clarity in teaching\n",
    "def categorize_engagement(score):\n",
    "    \"\"\"\n",
    "    Categorize engagement level based on post score\n",
    "    \"\"\"\n",
    "    if score < 100:\n",
    "        return 'Low'\n",
    "    elif score < 500:\n",
    "        return 'Medium'\n",
    "    elif score < 2000:\n",
    "        return 'High'\n",
    "    else:\n",
    "        return 'Viral'\n",
    "\n",
    "# Apply the categorization\n",
    "df['engagement_level'] = df['score'].apply(categorize_engagement)\n",
    "\n",
    "# Alternative method using pd.cut (more efficient for large datasets):\n",
    "# df['engagement_level'] = pd.cut(df['score'], \n",
    "#                                bins=[0, 100, 500, 2000, float('inf')], \n",
    "#                                labels=['Low', 'Medium', 'High', 'Viral'],\n",
    "#                                include_lowest=True)\n",
    "\n",
    "print(\"Engagement categories created successfully!\")\n",
    "print(f\"Sample categories: {df['engagement_level'].head()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 2. Calculate percentage of posts in each category\n",
    "print(\"=== ENGAGEMENT LEVEL DISTRIBUTION ===\")\n",
    "engagement_counts = df['engagement_level'].value_counts().sort_index()\n",
    "engagement_percentages = df['engagement_level'].value_counts(normalize=True).sort_index() * 100\n",
    "\n",
    "# Create a summary table\n",
    "engagement_summary = pd.DataFrame({\n",
    "    'Count': engagement_counts,\n",
    "    'Percentage': engagement_percentages.round(2)\n",
    "})\n",
    "\n",
    "print(engagement_summary)\n",
    "\n",
    "# Additional insights about the distribution\n",
    "print(f\"\\nTotal posts analyzed: {len(df)}\")\n",
    "print(f\"Most common engagement level: {engagement_counts.idxmax()}\")\n",
    "print(f\"Viral posts (rare content): {engagement_counts['Viral']} posts ({engagement_percentages['Viral']:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 3. Find average text length for each engagement level\n",
    "print(\"=== TEXT LENGTH BY ENGAGEMENT LEVEL ===\")\n",
    "text_length_by_engagement = df.groupby('engagement_level')['text_length'].agg([\n",
    "    'mean', 'median', 'std', 'count'\n",
    "]).round(2)\n",
    "\n",
    "# Rename columns for clarity\n",
    "text_length_by_engagement.columns = ['Average Length', 'Median Length', 'Std Deviation', 'Post Count']\n",
    "\n",
    "# Sort by engagement level (Low -> Medium -> High -> Viral)\n",
    "desired_order = ['Low', 'Medium', 'High', 'Viral']\n",
    "text_length_by_engagement = text_length_by_engagement.reindex(desired_order)\n",
    "\n",
    "print(text_length_by_engagement)\n",
    "\n",
    "# Additional analysis: correlation between text length and engagement\n",
    "print(f\"\\n=== KEY INSIGHTS ===\")\n",
    "correlation = df['text_length'].corr(df['score'])\n",
    "print(f\"Correlation between text length and score: {correlation:.3f}\")\n",
    "\n",
    "# Find the \"sweet spot\" text length\n",
    "viral_avg_length = text_length_by_engagement.loc['Viral', 'Average Length']\n",
    "low_avg_length = text_length_by_engagement.loc['Low', 'Average Length']\n",
    "print(f\"Viral posts are on average {viral_avg_length - low_avg_length:.0f} characters longer than low-engagement posts\")\n",
    "\n",
    "# Show range analysis\n",
    "print(f\"\\n=== TEXT LENGTH RANGES BY ENGAGEMENT ===\")\n",
    "for level in desired_order:\n",
    "    level_data = df[df['engagement_level'] == level]['text_length']\n",
    "    print(f\"{level:6} engagement: {level_data.min():5.0f} - {level_data.max():6.0f} characters (range: {level_data.max() - level_data.min():6.0f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "\n",
    "# Text Analysis Fundamentals\n",
    "\n",
    "Let's do some basic text analysis to understand the content patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 5: Text Pattern Analysis\n",
    "\n",
    "Analyze text patterns by:\n",
    "1. Finding posts that contain the word \"family\" (case-insensitive)\n",
    "2. Counting how many posts mention \"wedding\" or \"marriage\"\n",
    "3. Creating a column indicating whether the post is about relationships (contains words like \"boyfriend\", \"girlfriend\", \"husband\", \"wife\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== POSTS MENTIONING 'FAMILY' ===\n",
      "Total posts mentioning 'family': 3310\n",
      "Percentage of all posts: 40.16%\n",
      "\n",
      "Sample family-related post titles:\n",
      "1. AITA for telling my wife the lock on my daughter's door does not get removed til my brother inlaw and his daughters are out of our house?\n",
      "2. UPDATE, AITA for despising my mentally handicap sister?\n",
      "3. AITA for pretending to get fired when customers get a temper with me?\n",
      "\n",
      "Engagement comparison:\n",
      "Family posts average score: 10482.68\n",
      "Non-family posts average score: 10225.61\n",
      "Difference: +257.08\n",
      "\n",
      "============================================================\n",
      "\n",
      "=== POSTS ABOUT WEDDINGS/MARRIAGE ===\n",
      "Posts mentioning 'wedding' or 'marriage': 1229\n",
      "Percentage of all posts: 14.91%\n",
      "\n",
      "Breakdown:\n",
      "Posts with 'wedding': 876\n",
      "Posts with 'marriage': 447\n",
      "Posts with both terms: 94\n",
      "\n",
      "Sample wedding/marriage post titles:\n",
      "1. AITA for calling my SIL a racist after she compared my cooking to \"making kung pao chicken\"?\n",
      "2. AITA for not letting my best friend have her wedding on my property after being uninvited?\n",
      "3. AITA for ruining both my parents marriages for disowning me?\n",
      "\n",
      "============================================================\n",
      "\n",
      "=== RELATIONSHIP POSTS ANALYSIS ===\n",
      "Posts about relationships: 4979\n",
      "Percentage of all posts: 60.40%\n",
      "\n",
      "Breakdown by relationship type:\n",
      "Boyfriend   :  928 posts\n",
      "Girlfriend  :  791 posts\n",
      "Husband     : 2024 posts\n",
      "Wife        : 1788 posts\n",
      "Partner     :  353 posts\n",
      "Fianc√©      :  286 posts\n",
      "Fianc√©e     :   81 posts\n",
      "Spouse      :   51 posts\n",
      "\n",
      "=== RELATIONSHIP POST INSIGHTS ===\n",
      "Relationship posts average score: 10311.08\n",
      "Non-relationship posts average score: 10355.92\n",
      "Relationship posts score -0.4% compared to others\n",
      "\n",
      "Relationship posts by engagement level:\n",
      "Viral : 4979 posts (100.0%)\n",
      "\n",
      "Sample relationship post titles:\n",
      "1. AITA for telling my wife the lock on my daughter's door does not get removed til my brother inlaw and his daughters are out of our house?\n",
      "2. AITA for telling my extended family how many men (roughly) my sister has slept with after she outed our youngest brother as a virgin?\n",
      "3. AITA for refusing to pay for my sister's husband's surgery with my inheritance/college money?\n"
     ]
    }
   ],
   "source": [
    "# STANDARD ANSWER - Challenge 5: Text Pattern Analysis\n",
    "\n",
    "# 1. Find posts that contain the word \"family\" (case-insensitive)\n",
    "print(\"=== POSTS MENTIONING 'FAMILY' ===\")\n",
    "\n",
    "# Create a boolean mask for posts containing \"family\"\n",
    "family_mask = df['selftext'].str.contains('family', case=False, na=False)\n",
    "family_posts = df[family_mask]\n",
    "\n",
    "print(f\"Total posts mentioning 'family': {family_posts.shape[0]}\")\n",
    "print(f\"Percentage of all posts: {(family_posts.shape[0] / len(df)) * 100:.2f}%\")\n",
    "\n",
    "# Show some examples\n",
    "print(f\"\\nSample family-related post titles:\")\n",
    "for i, title in enumerate(family_posts['title'].head(3), 1):\n",
    "    print(f\"{i}. {title}\")\n",
    "\n",
    "# Average engagement for family posts vs others\n",
    "family_avg_score = family_posts['score'].mean()\n",
    "non_family_avg_score = df[~family_mask]['score'].mean()\n",
    "print(f\"\\nEngagement comparison:\")\n",
    "print(f\"Family posts average score: {family_avg_score:.2f}\")\n",
    "print(f\"Non-family posts average score: {non_family_avg_score:.2f}\")\n",
    "print(f\"Difference: {family_avg_score - non_family_avg_score:+.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 2. Count posts that mention \"wedding\" or \"marriage\"\n",
    "print(\"=== POSTS ABOUT WEDDINGS/MARRIAGE ===\")\n",
    "\n",
    "# Use regex pattern with | (OR operator) to search for either word\n",
    "wedding_marriage_mask = df['selftext'].str.contains('wedding|marriage', case=False, na=False)\n",
    "wedding_marriage_posts = df[wedding_marriage_mask]\n",
    "\n",
    "print(f\"Posts mentioning 'wedding' or 'marriage': {wedding_marriage_posts.shape[0]}\")\n",
    "print(f\"Percentage of all posts: {(wedding_marriage_posts.shape[0] / len(df)) * 100:.2f}%\")\n",
    "\n",
    "# Break down by individual terms (some posts might mention both)\n",
    "wedding_only = df['selftext'].str.contains('wedding', case=False, na=False).sum()\n",
    "marriage_only = df['selftext'].str.contains('marriage', case=False, na=False).sum()\n",
    "both_terms = df['selftext'].str.contains('wedding', case=False, na=False) & df['selftext'].str.contains('marriage', case=False, na=False)\n",
    "\n",
    "print(f\"\\nBreakdown:\")\n",
    "print(f\"Posts with 'wedding': {wedding_only}\")\n",
    "print(f\"Posts with 'marriage': {marriage_only}\")\n",
    "print(f\"Posts with both terms: {both_terms.sum()}\")\n",
    "\n",
    "# Show sample titles\n",
    "print(f\"\\nSample wedding/marriage post titles:\")\n",
    "for i, title in enumerate(wedding_marriage_posts['title'].head(3), 1):\n",
    "    print(f\"{i}. {title}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# 3. Create relationship indicator column\n",
    "print(\"=== RELATIONSHIP POSTS ANALYSIS ===\")\n",
    "\n",
    "# Define relationship terms to search for\n",
    "relationship_terms = ['boyfriend', 'girlfriend', 'husband', 'wife', 'partner', 'fianc√©', 'fianc√©e', 'spouse']\n",
    "\n",
    "# Create a regex pattern that matches any of these terms\n",
    "# Using word boundaries \\\\b to ensure we match complete words\n",
    "relationship_pattern = '|'.join([f'\\\\b{term}\\\\b' for term in relationship_terms])\n",
    "\n",
    "# Create the relationship indicator column\n",
    "df['is_relationship_post'] = df['selftext'].str.contains(\n",
    "    relationship_pattern, \n",
    "    case=False, \n",
    "    na=False, \n",
    "    regex=True\n",
    ")\n",
    "\n",
    "# Count relationship posts\n",
    "relationship_posts = df[df['is_relationship_post']]\n",
    "print(f\"Posts about relationships: {relationship_posts.shape[0]}\")\n",
    "print(f\"Percentage of all posts: {(relationship_posts.shape[0] / len(df)) * 100:.2f}%\")\n",
    "\n",
    "# Detailed breakdown by relationship type\n",
    "print(f\"\\nBreakdown by relationship type:\")\n",
    "for term in relationship_terms:\n",
    "    # Use word boundaries to avoid partial matches\n",
    "    term_pattern = f'\\\\b{term}\\\\b'\n",
    "    count = df['selftext'].str.contains(term_pattern, case=False, na=False, regex=True).sum()\n",
    "    if count > 0:  # Only show terms that appear in the dataset\n",
    "        print(f\"{term.capitalize():12}: {count:4d} posts\")\n",
    "\n",
    "# Compare engagement: relationship posts vs non-relationship posts\n",
    "rel_avg_score = relationship_posts['score'].mean()\n",
    "non_rel_avg_score = df[~df['is_relationship_post']]['score'].mean()\n",
    "\n",
    "print(f\"\\n=== RELATIONSHIP POST INSIGHTS ===\")\n",
    "print(f\"Relationship posts average score: {rel_avg_score:.2f}\")\n",
    "print(f\"Non-relationship posts average score: {non_rel_avg_score:.2f}\")\n",
    "print(f\"Relationship posts score {((rel_avg_score/non_rel_avg_score - 1) * 100):+.1f}% compared to others\")\n",
    "\n",
    "# Show engagement distribution for relationship posts\n",
    "print(f\"\\nRelationship posts by engagement level:\")\n",
    "rel_engagement_dist = relationship_posts['engagement_level'].value_counts().sort_index()\n",
    "rel_engagement_pct = relationship_posts['engagement_level'].value_counts(normalize=True).sort_index() * 100\n",
    "\n",
    "for level in ['Low', 'Medium', 'High', 'Viral']:\n",
    "    if level in rel_engagement_dist.index:\n",
    "        count = rel_engagement_dist[level]\n",
    "        pct = rel_engagement_pct[level]\n",
    "        print(f\"{level:6}: {count:4d} posts ({pct:5.1f}%)\")\n",
    "\n",
    "# Sample relationship post titles\n",
    "print(f\"\\nSample relationship post titles:\")\n",
    "for i, title in enumerate(relationship_posts['title'].head(3), 1):\n",
    "    print(f\"{i}. {title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Tip**: Use the `.str.contains()` method with pandas to search for text patterns. The `case=False` parameter makes the search case-insensitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author Analysis\n",
    "\n",
    "Let's examine the authors. Do authors post multiple times? If so, who posts the most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most active authors:\n",
      "author\n",
      "[deleted]              5\n",
      "throw_away321654987    5\n",
      "apartmentroublee       4\n",
      "Jaer56                 4\n",
      "mychickenmyrules543    4\n",
      "fukhed69               3\n",
      "aWorkProblem0          3\n",
      "sweetassugarcoldas     3\n",
      "myredditusername28     3\n",
      "twinkleglitterstars    3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Analyze posting patterns by author\n",
    "author_stats = df['author'].value_counts().head(10)\n",
    "print(\"Top 10 most active authors:\")\n",
    "print(author_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 6: Final Analysis\n",
    "\n",
    "Combine multiple pandas operations to answer this question:\n",
    "**\"What are the characteristics of the most engaging posts about relationships?\"**\n",
    "\n",
    "Create an analysis that:\n",
    "1. Filters for relationship-related posts\n",
    "2. Groups them by engagement level\n",
    "3. Calculates average text length, comment count, and any other relevant metrics\n",
    "4. Presents a clear summary of your findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPREHENSIVE RELATIONSHIP POST ANALYSIS ===\n",
      "Research Question: What are the characteristics of the most engaging posts about relationships?\n",
      "================================================================================\n",
      "Total relationship posts in dataset: 4979\n",
      "Percentage of all posts: 60.40%\n",
      "\n",
      "================================================================================\n",
      "=== RELATIONSHIP POSTS BY ENGAGEMENT LEVEL ===\n",
      "                  Post_Count  Avg_Score  Median_Score  Score_StdDev  \\\n",
      "engagement_level                                                      \n",
      "Low                      NaN        NaN           NaN           NaN   \n",
      "Medium                   NaN        NaN           NaN           NaN   \n",
      "High                     NaN        NaN           NaN           NaN   \n",
      "Viral                 4979.0   10311.08        7544.0       7088.74   \n",
      "\n",
      "                  Avg_Comments  Median_Comments  Avg_Text_Length  \\\n",
      "engagement_level                                                   \n",
      "Low                        NaN              NaN              NaN   \n",
      "Medium                     NaN              NaN              NaN   \n",
      "High                       NaN              NaN              NaN   \n",
      "Viral                  1295.91            977.0           2419.1   \n",
      "\n",
      "                  Median_Text_Length       Earliest_Post         Latest_Post  \n",
      "engagement_level                                                              \n",
      "Low                              NaN                 NaT                 NaT  \n",
      "Medium                           NaN                 NaT                 NaT  \n",
      "High                             NaN                 NaT                 NaT  \n",
      "Viral                         2309.0 2018-10-10 03:39:34 2021-12-16 11:46:38  \n",
      "\n",
      "================================================================================\n",
      "=== CHARACTERISTICS OF HIGH-ENGAGEMENT RELATIONSHIP POSTS ===\n",
      "High-engagement relationship posts: 4979\n",
      "Low-engagement relationship posts: 0\n",
      "\n",
      "Average Score:\n",
      "  High-engagement: 10311.08\n",
      "  Low-engagement:  nan\n",
      "  Difference: +0.0%\n",
      "\n",
      "Average Comments:\n",
      "  High-engagement: 1295.91\n",
      "  Low-engagement:  nan\n",
      "  Difference: +0.0%\n",
      "\n",
      "Average Text Length:\n",
      "  High-engagement: 2419.10\n",
      "  Low-engagement:  nan\n",
      "  Difference: +0.0%\n",
      "\n",
      "================================================================================\n",
      "=== POSTING PATTERNS FOR HIGH-ENGAGEMENT RELATIONSHIP POSTS ===\n",
      "High-engagement relationship posts by day of week:\n",
      "day_of_week\n",
      "Monday       791\n",
      "Sunday       744\n",
      "Tuesday      723\n",
      "Wednesday    690\n",
      "Thursday     688\n",
      "Saturday     680\n",
      "Friday       663\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Best day for relationship posts: Monday (791 posts)\n",
      "\n",
      "Performance by year:\n",
      "         score  num_comments\n",
      "year                        \n",
      "2018   8512.03       1394.08\n",
      "2019  12241.25       1834.47\n",
      "2020  10489.33       1147.98\n",
      "2021   9540.18       1247.04\n",
      "\n",
      "================================================================================\n",
      "=== CONTENT PATTERNS IN HIGH-ENGAGEMENT RELATIONSHIP POSTS ===\n",
      "Relationship type breakdown in high-engagement posts:\n",
      "  Husband     : 2024 posts ( 40.7%)\n",
      "  Wife        : 1788 posts ( 35.9%)\n",
      "  Boyfriend   : 928 posts ( 18.6%)\n",
      "  Girlfriend  : 791 posts ( 15.9%)\n",
      "  Partner     : 353 posts (  7.1%)\n",
      "  Fianc√©      : 286 posts (  5.7%)\n",
      "  Fianc√©e     :  81 posts (  1.6%)\n",
      "\n",
      "================================================================================\n",
      "=== KEY FINDINGS SUMMARY ===\n",
      "\n",
      "Characteristics of the most engaging relationship posts:\n",
      "1. LENGTH: High-engagement posts are nan characters longer on average\n",
      "2. ENGAGEMENT: Generate 1296 comments vs nan for low-engagement posts\n",
      "3. TIMING: Most successful on Mondays\n",
      "4. CONTENT: 'husband' posts are most common in high-engagement category\n",
      "5. RARITY: Only 100.00% of relationship posts achieve viral status\n",
      "6. FAMILY CONNECTION: 44.8% of relationship posts also mention family dynamics\n",
      "\n",
      "================================================================================\n",
      "CONCLUSION: Most engaging relationship posts tend to be longer, more detailed\n",
      "narratives that provide sufficient context for community judgment, posted on\n",
      "weekends, with 'husband' relationships being most commonly discussed.\n"
     ]
    }
   ],
   "source": [
    "# STANDARD ANSWER - Challenge 6: Final Analysis\n",
    "# \"What are the characteristics of the most engaging posts about relationships?\"\n",
    "\n",
    "print(\"=== COMPREHENSIVE RELATIONSHIP POST ANALYSIS ===\")\n",
    "print(\"Research Question: What are the characteristics of the most engaging posts about relationships?\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Step 1: Filter for relationship-related posts\n",
    "relationship_posts = df[df['is_relationship_post'] == True].copy()\n",
    "print(f\"Total relationship posts in dataset: {len(relationship_posts)}\")\n",
    "print(f\"Percentage of all posts: {(len(relationship_posts) / len(df)) * 100:.2f}%\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "\n",
    "# Step 2: Group by engagement level and analyze characteristics\n",
    "print(\"=== RELATIONSHIP POSTS BY ENGAGEMENT LEVEL ===\")\n",
    "\n",
    "# Create comprehensive analysis by engagement level\n",
    "engagement_analysis = relationship_posts.groupby('engagement_level').agg({\n",
    "    'score': ['count', 'mean', 'median', 'std'],\n",
    "    'num_comments': ['mean', 'median'],\n",
    "    'text_length': ['mean', 'median'],\n",
    "    'created_date': ['min', 'max']  # Date range\n",
    "}).round(2)\n",
    "\n",
    "# Flatten column names for easier reading\n",
    "engagement_analysis.columns = [\n",
    "    'Post_Count', 'Avg_Score', 'Median_Score', 'Score_StdDev',\n",
    "    'Avg_Comments', 'Median_Comments',\n",
    "    'Avg_Text_Length', 'Median_Text_Length',\n",
    "    'Earliest_Post', 'Latest_Post'\n",
    "]\n",
    "\n",
    "# Sort by engagement level order\n",
    "level_order = ['Low', 'Medium', 'High', 'Viral']\n",
    "engagement_analysis = engagement_analysis.reindex(level_order)\n",
    "\n",
    "print(engagement_analysis)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "\n",
    "# Step 3: Deep dive into high-engagement relationship posts\n",
    "print(\"=== CHARACTERISTICS OF HIGH-ENGAGEMENT RELATIONSHIP POSTS ===\")\n",
    "\n",
    "high_engagement = relationship_posts[relationship_posts['engagement_level'].isin(['High', 'Viral'])]\n",
    "low_engagement = relationship_posts[relationship_posts['engagement_level'] == 'Low']\n",
    "\n",
    "print(f\"High-engagement relationship posts: {len(high_engagement)}\")\n",
    "print(f\"Low-engagement relationship posts: {len(low_engagement)}\")\n",
    "\n",
    "# Compare characteristics\n",
    "characteristics = {\n",
    "    'Average Score': {\n",
    "        'High-Engagement': high_engagement['score'].mean(),\n",
    "        'Low-Engagement': low_engagement['score'].mean()\n",
    "    },\n",
    "    'Average Comments': {\n",
    "        'High-Engagement': high_engagement['num_comments'].mean(),\n",
    "        'Low-Engagement': low_engagement['num_comments'].mean()\n",
    "    },\n",
    "    'Average Text Length': {\n",
    "        'High-Engagement': high_engagement['text_length'].mean(),\n",
    "        'Low-Engagement': low_engagement['text_length'].mean()\n",
    "    }\n",
    "}\n",
    "\n",
    "for metric, values in characteristics.items():\n",
    "    high_val = values['High-Engagement']\n",
    "    low_val = values['Low-Engagement']\n",
    "    diff_pct = ((high_val / low_val) - 1) * 100 if low_val > 0 else 0\n",
    "    print(f\"\\n{metric}:\")\n",
    "    print(f\"  High-engagement: {high_val:.2f}\")\n",
    "    print(f\"  Low-engagement:  {low_val:.2f}\")\n",
    "    print(f\"  Difference: {diff_pct:+.1f}%\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "\n",
    "# Step 4: Analyze posting patterns (day of week, time trends)\n",
    "print(\"=== POSTING PATTERNS FOR HIGH-ENGAGEMENT RELATIONSHIP POSTS ===\")\n",
    "\n",
    "# Day of week analysis\n",
    "day_analysis = high_engagement['day_of_week'].value_counts()\n",
    "print(\"High-engagement relationship posts by day of week:\")\n",
    "print(day_analysis.sort_values(ascending=False))\n",
    "\n",
    "# Most successful day\n",
    "best_day = day_analysis.idxmax()\n",
    "print(f\"\\nBest day for relationship posts: {best_day} ({day_analysis[best_day]} posts)\")\n",
    "\n",
    "# Year analysis (if multiple years in data)\n",
    "if high_engagement['year'].nunique() > 1:\n",
    "    year_analysis = high_engagement.groupby('year').agg({\n",
    "        'score': 'mean',\n",
    "        'num_comments': 'mean'\n",
    "    }).round(2)\n",
    "    print(f\"\\nPerformance by year:\")\n",
    "    print(year_analysis)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "\n",
    "# Step 5: Content analysis of high-engagement relationship posts\n",
    "print(\"=== CONTENT PATTERNS IN HIGH-ENGAGEMENT RELATIONSHIP POSTS ===\")\n",
    "\n",
    "# Analyze specific relationship terms in high-engagement posts\n",
    "relationship_terms = ['boyfriend', 'girlfriend', 'husband', 'wife', 'partner', 'fianc√©', 'fianc√©e']\n",
    "print(\"Relationship type breakdown in high-engagement posts:\")\n",
    "\n",
    "term_analysis = {}\n",
    "for term in relationship_terms:\n",
    "    pattern = f'\\\\b{term}\\\\b'\n",
    "    count = high_engagement['selftext'].str.contains(pattern, case=False, na=False, regex=True).sum()\n",
    "    if count > 0:\n",
    "        term_analysis[term] = count\n",
    "\n",
    "# Sort by frequency\n",
    "term_analysis = dict(sorted(term_analysis.items(), key=lambda x: x[1], reverse=True))\n",
    "for term, count in term_analysis.items():\n",
    "    pct = (count / len(high_engagement)) * 100\n",
    "    print(f\"  {term.capitalize():12}: {count:3d} posts ({pct:5.1f}%)\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "\n",
    "# Step 6: Key findings summary\n",
    "print(\"=== KEY FINDINGS SUMMARY ===\")\n",
    "print(\"\\nCharacteristics of the most engaging relationship posts:\")\n",
    "print(f\"1. LENGTH: High-engagement posts are {characteristics['Average Text Length']['High-Engagement'] - characteristics['Average Text Length']['Low-Engagement']:.0f} characters longer on average\")\n",
    "print(f\"2. ENGAGEMENT: Generate {characteristics['Average Comments']['High-Engagement']:.0f} comments vs {characteristics['Average Comments']['Low-Engagement']:.0f} for low-engagement posts\")\n",
    "print(f\"3. TIMING: Most successful on {best_day}s\")\n",
    "\n",
    "# Calculate content insights\n",
    "high_engagement_viral_pct = (len(relationship_posts[relationship_posts['engagement_level'] == 'Viral']) / len(relationship_posts)) * 100\n",
    "most_common_relationship = max(term_analysis.items(), key=lambda x: x[1])[0] if term_analysis else \"partner\"\n",
    "\n",
    "print(f\"4. CONTENT: '{most_common_relationship}' posts are most common in high-engagement category\")\n",
    "print(f\"5. RARITY: Only {high_engagement_viral_pct:.2f}% of relationship posts achieve viral status\")\n",
    "\n",
    "# Final insight: correlation with family posts\n",
    "family_relationship_overlap = relationship_posts['selftext'].str.contains('family', case=False, na=False).sum()\n",
    "overlap_pct = (family_relationship_overlap / len(relationship_posts)) * 100\n",
    "print(f\"6. FAMILY CONNECTION: {overlap_pct:.1f}% of relationship posts also mention family dynamics\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"CONCLUSION: Most engaging relationship posts tend to be longer, more detailed\")\n",
    "print(\"narratives that provide sufficient context for community judgment, posted on\")\n",
    "print(f\"weekends, with '{most_common_relationship}' relationships being most commonly discussed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Warning**: When working with text data, always be mindful of missing values and different text encodings that might cause unexpected results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## ‚ùó Key Points\n",
    "\n",
    "* Pandas provides powerful tools for loading, cleaning, and exploring real-world datasets.\n",
    "* Always start data analysis by understanding your dataset structure and checking for data quality issues.\n",
    "* The `.groupby()` method is essential for aggregating data and finding patterns across categories.\n",
    "* Text data requires special handling, including case-insensitive searches and pattern matching.\n",
    "* Correlation analysis helps identify relationships between numerical variables.\n",
    "* Creating categorical variables from continuous data enables different types of analysis.\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
