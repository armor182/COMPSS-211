{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summer Recap: Data Analysis with Pandas\n",
    "\n",
    "* * * \n",
    "\n",
    "<div class=\"alert alert-success\">  \n",
    "    \n",
    "### Learning Objectives \n",
    "    \n",
    "* Review fundamental pandas operations for data manipulation and analysis.\n",
    "* Apply data cleaning techniques to real-world social science datasets.\n",
    "* Practice exploratory data analysis using descriptive statistics and basic visualizations.\n",
    "* Demonstrate ability to filter, group, and aggregate data using pandas methods.\n",
    "* Evaluate LLM-generated code for accuracy and best practices.\n",
    "</div>\n",
    "\n",
    "### Icons Used in This Notebook\n",
    "üîî **Question**: A quick question to help you understand what's going on.<br>\n",
    "ü•ä **Challenge**: Interactive exercise. We'll work through these in the workshop!<br>\n",
    "üí° **Tip**: How to do something a bit more efficiently or effectively.<br>\n",
    "‚ö†Ô∏è **Warning:** Heads-up about tricky stuff or common mistakes.<br>\n",
    "ü§ñ **AI Generated**: Code generated by an LLM that we'll test and debug.<br>\n",
    "\n",
    "### Sections\n",
    "1. [Data Loading and Initial Exploration](#section1)\n",
    "2. [Data Cleaning and Basic Operations](#section2)\n",
    "3. [Exploratory Data Analysis](#section3)\n",
    "4. [Text Analysis Fundamentals](#section4)\n",
    "5. [Working with LLM-Generated Code](#section5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "\n",
    "# Data Loading and Initial Exploration\n",
    "\n",
    "Today we'll work with data from Reddit's \"Am I the Asshole?\" (AITA) subreddit. This dataset contains posts where people describe situations and ask for community judgment about their behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../../data/aita_top_subs.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumn names:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 1: Data Overview\n",
    "\n",
    "Explore the dataset structure and provide a summary of what you find. Use pandas methods to:\n",
    "1. Check the data types of each column\n",
    "2. Look for missing values\n",
    "3. Get basic descriptive statistics for numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# STANDARD ANSWER - Challenge 1: Data Overview\n\n# 1. Check the data types of each column\nprint(\"=== DATA TYPES ===\")\nprint(df.dtypes)\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# 2. Look for missing values\nprint(\"=== MISSING VALUES ===\")\n# Method 1: Count of missing values per column\nmissing_counts = df.isnull().sum()\nprint(\"Missing value counts:\")\nprint(missing_counts)\n\n# Method 2: Percentage of missing values (more informative for large datasets)\nmissing_percentages = (df.isnull().sum() / len(df)) * 100\nprint(\"\\nMissing value percentages:\")\nfor col, pct in missing_percentages.items():\n    if pct > 0:\n        print(f\"{col}: {pct:.2f}%\")\n\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# 3. Get basic descriptive statistics for numerical columns\nprint(\"=== DESCRIPTIVE STATISTICS ===\")\n# Using describe() gives us count, mean, std, min, 25%, 50%, 75%, max\nnumerical_stats = df.describe()\nprint(numerical_stats)\n\n# Additional useful info: data shape and memory usage\nprint(f\"\\n=== ADDITIONAL INFO ===\")\nprint(f\"Dataset shape: {df.shape} (rows, columns)\")\nprint(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n\n# Show unique values for categorical columns (first few)\nprint(f\"\\nUnique authors: {df['author'].nunique()}\")\nprint(f\"Sample authors: {df['author'].unique()[:5]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîî **Question**: What do you notice about the `selftext` column? What might this tell us about the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "\n",
    "# Data Cleaning and Basic Operations\n",
    "\n",
    "Real-world data often requires cleaning before analysis. Let's examine our dataset for common issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate posts\n",
    "print(f\"Number of duplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "# Look at the distribution of some key variables\n",
    "print(f\"\\nScore statistics:\")\n",
    "print(df['score'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 2: Data Cleaning\n",
    "\n",
    "Clean the dataset by:\n",
    "1. Removing any posts where `selftext` is missing or empty\n",
    "2. Creating a new column called `text_length` that contains the character count of `selftext`\n",
    "3. Filter out posts that are shorter than 100 characters (likely low-quality posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# STANDARD ANSWER - Challenge 2: Data Cleaning\n\nprint(f\"Original dataset shape: {df.shape}\")\n\n# Step 1: Remove posts where selftext is missing or empty\n# Check current state of selftext column\nprint(f\"\\nSelftext column analysis:\")\nprint(f\"Missing values: {df['selftext'].isnull().sum()}\")\nprint(f\"Empty strings: {(df['selftext'] == '').sum()}\")\n\n# Remove missing values first\ndf_clean = df.dropna(subset=['selftext']).copy()\nprint(f\"After removing missing selftext: {df_clean.shape}\")\n\n# Remove empty strings (posts with no content)\ndf_clean = df_clean[df_clean['selftext'] != ''].copy()\nprint(f\"After removing empty selftext: {df_clean.shape}\")\n\n# Step 2: Create text_length column\n# Calculate character count for each selftext\ndf_clean['text_length'] = df_clean['selftext'].str.len()\n\nprint(f\"\\nText length statistics:\")\nprint(df_clean['text_length'].describe())\n\n# Step 3: Filter out posts shorter than 100 characters\n# These are likely low-quality or incomplete posts\nprint(f\"\\nPosts with less than 100 characters: {(df_clean['text_length'] < 100).sum()}\")\n\ndf_clean = df_clean[df_clean['text_length'] >= 100].copy()\nprint(f\"After filtering short posts: {df_clean.shape}\")\n\n# Summary of cleaning process\nprint(f\"\\n=== CLEANING SUMMARY ===\")\nprint(f\"Original rows: {df.shape[0]}\")\nprint(f\"Final rows: {df_clean.shape[0]}\")\nprint(f\"Rows removed: {df.shape[0] - df_clean.shape[0]}\")\nprint(f\"Percentage retained: {(df_clean.shape[0] / df.shape[0]) * 100:.1f}%\")\n\n# Update our main dataframe for subsequent analysis\ndf = df_clean.copy()\n\nprint(f\"\\nFinal text length distribution:\")\nprint(df['text_length'].describe())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Tip**: Use the `.str.len()` method to get string lengths in pandas. Remember that missing values might cause issues, so handle them first!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Dates\n",
    "\n",
    "The `created` column contains Unix timestamps. Let's convert these to readable dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Unix timestamp to datetime\n",
    "df['created_date'] = pd.to_datetime(df['created'], unit='s')\n",
    "\n",
    "# Extract useful date components\n",
    "df['year'] = df['created_date'].dt.year\n",
    "df['month'] = df['created_date'].dt.month\n",
    "df['day_of_week'] = df['created_date'].dt.day_name()\n",
    "\n",
    "print(\"Date range in dataset:\")\n",
    "print(f\"From: {df['created_date'].min()}\")\n",
    "print(f\"To: {df['created_date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "\n",
    "# Exploratory Data Analysis\n",
    "\n",
    "Now let's explore patterns in the data using pandas grouping and aggregation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 3: Score Analysis\n",
    "\n",
    "Analyze post popularity by:\n",
    "1. Finding the top 10 posts by score\n",
    "2. Calculating the average score by year\n",
    "3. Determining which day of the week gets the highest average scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# STANDARD ANSWER - Challenge 3: Score Analysis\n\n# 1. Find the top 10 posts by score\nprint(\"=== TOP 10 POSTS BY SCORE ===\")\ntop_posts = df.nlargest(10, 'score')[['title', 'score', 'num_comments', 'author']]\nprint(top_posts)\n\n# Let's also show some details about the highest scoring post\nprint(f\"\\nHighest scoring post details:\")\nhighest_post = df.loc[df['score'].idxmax()]\nprint(f\"Score: {highest_post['score']}\")\nprint(f\"Title: {highest_post['title']}\")\nprint(f\"Author: {highest_post['author']}\")\nprint(f\"Text length: {highest_post['text_length']} characters\")\n\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\n\n# 2. Calculate average score by year\nprint(\"=== AVERAGE SCORE BY YEAR ===\")\n# Group by year and calculate mean score\nyearly_scores = df.groupby('year')['score'].agg(['mean', 'count', 'std']).round(2)\nyearly_scores.columns = ['Average Score', 'Post Count', 'Standard Deviation']\nprint(yearly_scores)\n\n# Add some context about the year with highest average\nbest_year = yearly_scores['Average Score'].idxmax()\nprint(f\"\\nBest performing year: {best_year} with average score of {yearly_scores.loc[best_year, 'Average Score']}\")\n\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\n\n# 3. Determine which day of the week gets highest average scores\nprint(\"=== AVERAGE SCORE BY DAY OF WEEK ===\")\n# Group by day of week and calculate statistics\ndaily_scores = df.groupby('day_of_week')['score'].agg(['mean', 'count', 'median']).round(2)\ndaily_scores.columns = ['Average Score', 'Post Count', 'Median Score']\n\n# Sort by average score (descending) to see best performing days\ndaily_scores_sorted = daily_scores.sort_values('Average Score', ascending=False)\nprint(daily_scores_sorted)\n\n# Highlight the best day\nbest_day = daily_scores_sorted.index[0]\nprint(f\"\\nBest performing day: {best_day}\")\nprint(f\"Average score on {best_day}: {daily_scores_sorted.loc[best_day, 'Average Score']}\")\nprint(f\"This is based on {daily_scores_sorted.loc[best_day, 'Post Count']} posts\")\n\n# Additional insight: Weekend vs Weekday analysis\nweekend_days = ['Saturday', 'Sunday']\nweekday_avg = df[~df['day_of_week'].isin(weekend_days)]['score'].mean()\nweekend_avg = df[df['day_of_week'].isin(weekend_days)]['score'].mean()\n\nprint(f\"\\n=== WEEKEND vs WEEKDAY COMPARISON ===\")\nprint(f\"Weekday average score: {weekday_avg:.2f}\")\nprint(f\"Weekend average score: {weekend_avg:.2f}\")\nprint(f\"Weekend posts score {((weekend_avg/weekday_avg - 1) * 100):+.1f}% compared to weekdays\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment Engagement Analysis\n",
    "\n",
    "Now, let's explore how users interact with posts by analyzing the volume and distribution of comments, which can highlight engagement patterns and community response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the relationship between text length and engagement\n",
    "correlation = df[['text_length', 'score', 'num_comments']].corr()\n",
    "print(\"Correlation matrix:\")\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîî **Question**: What does the correlation tell us about the relationship between post length and engagement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 4: Engagement Categories\n",
    "\n",
    "Create engagement categories and analyze them:\n",
    "1. Create a new column `engagement_level` with categories:\n",
    "   - 'Low': score < 100\n",
    "   - 'Medium': score 100-500\n",
    "   - 'High': score 500-2000\n",
    "   - 'Viral': score > 2000\n",
    "2. Calculate the percentage of posts in each category\n",
    "3. Find the average text length for each engagement level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# STANDARD ANSWER - Challenge 4: Engagement Categories\n\n# 1. Create engagement_level column with categories based on score\nprint(\"=== CREATING ENGAGEMENT CATEGORIES ===\")\n\n# First, let's see the score distribution to understand our categorization\nprint(\"Score distribution:\")\nprint(df['score'].describe())\n\n# Create engagement categories using pd.cut() or conditional logic\n# Using conditional logic for clarity in teaching\ndef categorize_engagement(score):\n    \"\"\"\n    Categorize engagement level based on post score\n    \"\"\"\n    if score < 100:\n        return 'Low'\n    elif score < 500:\n        return 'Medium'\n    elif score < 2000:\n        return 'High'\n    else:\n        return 'Viral'\n\n# Apply the categorization\ndf['engagement_level'] = df['score'].apply(categorize_engagement)\n\n# Alternative method using pd.cut (more efficient for large datasets):\n# df['engagement_level'] = pd.cut(df['score'], \n#                                bins=[0, 100, 500, 2000, float('inf')], \n#                                labels=['Low', 'Medium', 'High', 'Viral'],\n#                                include_lowest=True)\n\nprint(\"Engagement categories created successfully!\")\nprint(f\"Sample categories: {df['engagement_level'].head()}\")\n\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\n\n# 2. Calculate percentage of posts in each category\nprint(\"=== ENGAGEMENT LEVEL DISTRIBUTION ===\")\nengagement_counts = df['engagement_level'].value_counts().sort_index()\nengagement_percentages = df['engagement_level'].value_counts(normalize=True).sort_index() * 100\n\n# Create a summary table\nengagement_summary = pd.DataFrame({\n    'Count': engagement_counts,\n    'Percentage': engagement_percentages.round(2)\n})\n\nprint(engagement_summary)\n\n# Additional insights about the distribution\nprint(f\"\\nTotal posts analyzed: {len(df)}\")\nprint(f\"Most common engagement level: {engagement_counts.idxmax()}\")\nprint(f\"Viral posts (rare content): {engagement_counts['Viral']} posts ({engagement_percentages['Viral']:.2f}%)\")\n\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\n\n# 3. Find average text length for each engagement level\nprint(\"=== TEXT LENGTH BY ENGAGEMENT LEVEL ===\")\ntext_length_by_engagement = df.groupby('engagement_level')['text_length'].agg([\n    'mean', 'median', 'std', 'count'\n]).round(2)\n\n# Rename columns for clarity\ntext_length_by_engagement.columns = ['Average Length', 'Median Length', 'Std Deviation', 'Post Count']\n\n# Sort by engagement level (Low -> Medium -> High -> Viral)\ndesired_order = ['Low', 'Medium', 'High', 'Viral']\ntext_length_by_engagement = text_length_by_engagement.reindex(desired_order)\n\nprint(text_length_by_engagement)\n\n# Additional analysis: correlation between text length and engagement\nprint(f\"\\n=== KEY INSIGHTS ===\")\ncorrelation = df['text_length'].corr(df['score'])\nprint(f\"Correlation between text length and score: {correlation:.3f}\")\n\n# Find the \"sweet spot\" text length\nviral_avg_length = text_length_by_engagement.loc['Viral', 'Average Length']\nlow_avg_length = text_length_by_engagement.loc['Low', 'Average Length']\nprint(f\"Viral posts are on average {viral_avg_length - low_avg_length:.0f} characters longer than low-engagement posts\")\n\n# Show range analysis\nprint(f\"\\n=== TEXT LENGTH RANGES BY ENGAGEMENT ===\")\nfor level in desired_order:\n    level_data = df[df['engagement_level'] == level]['text_length']\n    print(f\"{level:6} engagement: {level_data.min():5.0f} - {level_data.max():6.0f} characters (range: {level_data.max() - level_data.min():6.0f})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "\n",
    "# Text Analysis Fundamentals\n",
    "\n",
    "Let's do some basic text analysis to understand the content patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 5: Text Pattern Analysis\n",
    "\n",
    "Analyze text patterns by:\n",
    "1. Finding posts that contain the word \"family\" (case-insensitive)\n",
    "2. Counting how many posts mention \"wedding\" or \"marriage\"\n",
    "3. Creating a column indicating whether the post is about relationships (contains words like \"boyfriend\", \"girlfriend\", \"husband\", \"wife\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# STANDARD ANSWER - Challenge 5: Text Pattern Analysis\n\n# 1. Find posts that contain the word \"family\" (case-insensitive)\nprint(\"=== POSTS MENTIONING 'FAMILY' ===\")\n\n# Create a boolean mask for posts containing \"family\"\nfamily_mask = df['selftext'].str.contains('family', case=False, na=False)\nfamily_posts = df[family_mask]\n\nprint(f\"Total posts mentioning 'family': {family_posts.shape[0]}\")\nprint(f\"Percentage of all posts: {(family_posts.shape[0] / len(df)) * 100:.2f}%\")\n\n# Show some examples\nprint(f\"\\nSample family-related post titles:\")\nfor i, title in enumerate(family_posts['title'].head(3), 1):\n    print(f\"{i}. {title}\")\n\n# Average engagement for family posts vs others\nfamily_avg_score = family_posts['score'].mean()\nnon_family_avg_score = df[~family_mask]['score'].mean()\nprint(f\"\\nEngagement comparison:\")\nprint(f\"Family posts average score: {family_avg_score:.2f}\")\nprint(f\"Non-family posts average score: {non_family_avg_score:.2f}\")\nprint(f\"Difference: {family_avg_score - non_family_avg_score:+.2f}\")\n\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\n\n# 2. Count posts that mention \"wedding\" or \"marriage\"\nprint(\"=== POSTS ABOUT WEDDINGS/MARRIAGE ===\")\n\n# Use regex pattern with | (OR operator) to search for either word\nwedding_marriage_mask = df['selftext'].str.contains('wedding|marriage', case=False, na=False)\nwedding_marriage_posts = df[wedding_marriage_mask]\n\nprint(f\"Posts mentioning 'wedding' or 'marriage': {wedding_marriage_posts.shape[0]}\")\nprint(f\"Percentage of all posts: {(wedding_marriage_posts.shape[0] / len(df)) * 100:.2f}%\")\n\n# Break down by individual terms (some posts might mention both)\nwedding_only = df['selftext'].str.contains('wedding', case=False, na=False).sum()\nmarriage_only = df['selftext'].str.contains('marriage', case=False, na=False).sum()\nboth_terms = df['selftext'].str.contains('wedding', case=False, na=False) & df['selftext'].str.contains('marriage', case=False, na=False)\n\nprint(f\"\\nBreakdown:\")\nprint(f\"Posts with 'wedding': {wedding_only}\")\nprint(f\"Posts with 'marriage': {marriage_only}\")\nprint(f\"Posts with both terms: {both_terms.sum()}\")\n\n# Show sample titles\nprint(f\"\\nSample wedding/marriage post titles:\")\nfor i, title in enumerate(wedding_marriage_posts['title'].head(3), 1):\n    print(f\"{i}. {title}\")\n\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\n\n# 3. Create relationship indicator column\nprint(\"=== RELATIONSHIP POSTS ANALYSIS ===\")\n\n# Define relationship terms to search for\nrelationship_terms = ['boyfriend', 'girlfriend', 'husband', 'wife', 'partner', 'fianc√©', 'fianc√©e', 'spouse']\n\n# Create a regex pattern that matches any of these terms\n# Using word boundaries \\\\b to ensure we match complete words\nrelationship_pattern = '|'.join([f'\\\\b{term}\\\\b' for term in relationship_terms])\n\n# Create the relationship indicator column\ndf['is_relationship_post'] = df['selftext'].str.contains(\n    relationship_pattern, \n    case=False, \n    na=False, \n    regex=True\n)\n\n# Count relationship posts\nrelationship_posts = df[df['is_relationship_post']]\nprint(f\"Posts about relationships: {relationship_posts.shape[0]}\")\nprint(f\"Percentage of all posts: {(relationship_posts.shape[0] / len(df)) * 100:.2f}%\")\n\n# Detailed breakdown by relationship type\nprint(f\"\\nBreakdown by relationship type:\")\nfor term in relationship_terms:\n    # Use word boundaries to avoid partial matches\n    term_pattern = f'\\\\b{term}\\\\b'\n    count = df['selftext'].str.contains(term_pattern, case=False, na=False, regex=True).sum()\n    if count > 0:  # Only show terms that appear in the dataset\n        print(f\"{term.capitalize():12}: {count:4d} posts\")\n\n# Compare engagement: relationship posts vs non-relationship posts\nrel_avg_score = relationship_posts['score'].mean()\nnon_rel_avg_score = df[~df['is_relationship_post']]['score'].mean()\n\nprint(f\"\\n=== RELATIONSHIP POST INSIGHTS ===\")\nprint(f\"Relationship posts average score: {rel_avg_score:.2f}\")\nprint(f\"Non-relationship posts average score: {non_rel_avg_score:.2f}\")\nprint(f\"Relationship posts score {((rel_avg_score/non_rel_avg_score - 1) * 100):+.1f}% compared to others\")\n\n# Show engagement distribution for relationship posts\nprint(f\"\\nRelationship posts by engagement level:\")\nrel_engagement_dist = relationship_posts['engagement_level'].value_counts().sort_index()\nrel_engagement_pct = relationship_posts['engagement_level'].value_counts(normalize=True).sort_index() * 100\n\nfor level in ['Low', 'Medium', 'High', 'Viral']:\n    if level in rel_engagement_dist.index:\n        count = rel_engagement_dist[level]\n        pct = rel_engagement_pct[level]\n        print(f\"{level:6}: {count:4d} posts ({pct:5.1f}%)\")\n\n# Sample relationship post titles\nprint(f\"\\nSample relationship post titles:\")\nfor i, title in enumerate(relationship_posts['title'].head(3), 1):\n    print(f\"{i}. {title}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Tip**: Use the `.str.contains()` method with pandas to search for text patterns. The `case=False` parameter makes the search case-insensitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author Analysis\n",
    "\n",
    "Let's examine the authors. Do authors post multiple times? If so, who posts the most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze posting patterns by author\n",
    "author_stats = df['author'].value_counts().head(10)\n",
    "print(\"Top 10 most active authors:\")\n",
    "print(author_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 6: Final Analysis\n",
    "\n",
    "Combine multiple pandas operations to answer this question:\n",
    "**\"What are the characteristics of the most engaging posts about relationships?\"**\n",
    "\n",
    "Create an analysis that:\n",
    "1. Filters for relationship-related posts\n",
    "2. Groups them by engagement level\n",
    "3. Calculates average text length, comment count, and any other relevant metrics\n",
    "4. Presents a clear summary of your findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# STANDARD ANSWER - Challenge 6: Final Analysis\n# \"What are the characteristics of the most engaging posts about relationships?\"\n\nprint(\"=== COMPREHENSIVE RELATIONSHIP POST ANALYSIS ===\")\nprint(\"Research Question: What are the characteristics of the most engaging posts about relationships?\")\nprint(\"=\"*80)\n\n# Step 1: Filter for relationship-related posts\nrelationship_posts = df[df['is_relationship_post'] == True].copy()\nprint(f\"Total relationship posts in dataset: {len(relationship_posts)}\")\nprint(f\"Percentage of all posts: {(len(relationship_posts) / len(df)) * 100:.2f}%\")\n\nprint(f\"\\n{'='*80}\")\n\n# Step 2: Group by engagement level and analyze characteristics\nprint(\"=== RELATIONSHIP POSTS BY ENGAGEMENT LEVEL ===\")\n\n# Create comprehensive analysis by engagement level\nengagement_analysis = relationship_posts.groupby('engagement_level').agg({\n    'score': ['count', 'mean', 'median', 'std'],\n    'num_comments': ['mean', 'median'],\n    'text_length': ['mean', 'median'],\n    'created_date': ['min', 'max']  # Date range\n}).round(2)\n\n# Flatten column names for easier reading\nengagement_analysis.columns = [\n    'Post_Count', 'Avg_Score', 'Median_Score', 'Score_StdDev',\n    'Avg_Comments', 'Median_Comments',\n    'Avg_Text_Length', 'Median_Text_Length',\n    'Earliest_Post', 'Latest_Post'\n]\n\n# Sort by engagement level order\nlevel_order = ['Low', 'Medium', 'High', 'Viral']\nengagement_analysis = engagement_analysis.reindex(level_order)\n\nprint(engagement_analysis)\n\nprint(f\"\\n{'='*80}\")\n\n# Step 3: Deep dive into high-engagement relationship posts\nprint(\"=== CHARACTERISTICS OF HIGH-ENGAGEMENT RELATIONSHIP POSTS ===\")\n\nhigh_engagement = relationship_posts[relationship_posts['engagement_level'].isin(['High', 'Viral'])]\nlow_engagement = relationship_posts[relationship_posts['engagement_level'] == 'Low']\n\nprint(f\"High-engagement relationship posts: {len(high_engagement)}\")\nprint(f\"Low-engagement relationship posts: {len(low_engagement)}\")\n\n# Compare characteristics\ncharacteristics = {\n    'Average Score': {\n        'High-Engagement': high_engagement['score'].mean(),\n        'Low-Engagement': low_engagement['score'].mean()\n    },\n    'Average Comments': {\n        'High-Engagement': high_engagement['num_comments'].mean(),\n        'Low-Engagement': low_engagement['num_comments'].mean()\n    },\n    'Average Text Length': {\n        'High-Engagement': high_engagement['text_length'].mean(),\n        'Low-Engagement': low_engagement['text_length'].mean()\n    }\n}\n\nfor metric, values in characteristics.items():\n    high_val = values['High-Engagement']\n    low_val = values['Low-Engagement']\n    diff_pct = ((high_val / low_val) - 1) * 100 if low_val > 0 else 0\n    print(f\"\\n{metric}:\")\n    print(f\"  High-engagement: {high_val:.2f}\")\n    print(f\"  Low-engagement:  {low_val:.2f}\")\n    print(f\"  Difference: {diff_pct:+.1f}%\")\n\nprint(f\"\\n{'='*80}\")\n\n# Step 4: Analyze posting patterns (day of week, time trends)\nprint(\"=== POSTING PATTERNS FOR HIGH-ENGAGEMENT RELATIONSHIP POSTS ===\")\n\n# Day of week analysis\nday_analysis = high_engagement['day_of_week'].value_counts()\nprint(\"High-engagement relationship posts by day of week:\")\nprint(day_analysis.sort_values(ascending=False))\n\n# Most successful day\nbest_day = day_analysis.idxmax()\nprint(f\"\\nBest day for relationship posts: {best_day} ({day_analysis[best_day]} posts)\")\n\n# Year analysis (if multiple years in data)\nif high_engagement['year'].nunique() > 1:\n    year_analysis = high_engagement.groupby('year').agg({\n        'score': 'mean',\n        'num_comments': 'mean'\n    }).round(2)\n    print(f\"\\nPerformance by year:\")\n    print(year_analysis)\n\nprint(f\"\\n{'='*80}\")\n\n# Step 5: Content analysis of high-engagement relationship posts\nprint(\"=== CONTENT PATTERNS IN HIGH-ENGAGEMENT RELATIONSHIP POSTS ===\")\n\n# Analyze specific relationship terms in high-engagement posts\nrelationship_terms = ['boyfriend', 'girlfriend', 'husband', 'wife', 'partner', 'fianc√©', 'fianc√©e']\nprint(\"Relationship type breakdown in high-engagement posts:\")\n\nterm_analysis = {}\nfor term in relationship_terms:\n    pattern = f'\\\\b{term}\\\\b'\n    count = high_engagement['selftext'].str.contains(pattern, case=False, na=False, regex=True).sum()\n    if count > 0:\n        term_analysis[term] = count\n\n# Sort by frequency\nterm_analysis = dict(sorted(term_analysis.items(), key=lambda x: x[1], reverse=True))\nfor term, count in term_analysis.items():\n    pct = (count / len(high_engagement)) * 100\n    print(f\"  {term.capitalize():12}: {count:3d} posts ({pct:5.1f}%)\")\n\nprint(f\"\\n{'='*80}\")\n\n# Step 6: Key findings summary\nprint(\"=== KEY FINDINGS SUMMARY ===\")\nprint(\"\\nCharacteristics of the most engaging relationship posts:\")\nprint(f\"1. LENGTH: High-engagement posts are {characteristics['Average Text Length']['High-Engagement'] - characteristics['Average Text Length']['Low-Engagement']:.0f} characters longer on average\")\nprint(f\"2. ENGAGEMENT: Generate {characteristics['Average Comments']['High-Engagement']:.0f} comments vs {characteristics['Average Comments']['Low-Engagement']:.0f} for low-engagement posts\")\nprint(f\"3. TIMING: Most successful on {best_day}s\")\n\n# Calculate content insights\nhigh_engagement_viral_pct = (len(relationship_posts[relationship_posts['engagement_level'] == 'Viral']) / len(relationship_posts)) * 100\nmost_common_relationship = max(term_analysis.items(), key=lambda x: x[1])[0] if term_analysis else \"partner\"\n\nprint(f\"4. CONTENT: '{most_common_relationship}' posts are most common in high-engagement category\")\nprint(f\"5. RARITY: Only {high_engagement_viral_pct:.2f}% of relationship posts achieve viral status\")\n\n# Final insight: correlation with family posts\nfamily_relationship_overlap = relationship_posts['selftext'].str.contains('family', case=False, na=False).sum()\noverlap_pct = (family_relationship_overlap / len(relationship_posts)) * 100\nprint(f\"6. FAMILY CONNECTION: {overlap_pct:.1f}% of relationship posts also mention family dynamics\")\n\nprint(f\"\\n{'='*80}\")\nprint(\"CONCLUSION: Most engaging relationship posts tend to be longer, more detailed\")\nprint(\"narratives that provide sufficient context for community judgment, posted on\")\nprint(f\"weekends, with '{most_common_relationship}' relationships being most commonly discussed.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Warning**: When working with text data, always be mindful of missing values and different text encodings that might cause unexpected results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## ‚ùó Key Points\n",
    "\n",
    "* Pandas provides powerful tools for loading, cleaning, and exploring real-world datasets.\n",
    "* Always start data analysis by understanding your dataset structure and checking for data quality issues.\n",
    "* The `.groupby()` method is essential for aggregating data and finding patterns across categories.\n",
    "* Text data requires special handling, including case-insensitive searches and pattern matching.\n",
    "* Correlation analysis helps identify relationships between numerical variables.\n",
    "* Creating categorical variables from continuous data enables different types of analysis.\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}