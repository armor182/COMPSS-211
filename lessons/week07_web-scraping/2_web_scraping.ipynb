{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with Beautiful Soup\n",
    "\n",
    "* * * \n",
    "\n",
    "### Icons used in this notebook\n",
    "üîî **Question**: A quick question to help you understand what's going on.<br>\n",
    "ü•ä **Challenge**: Interactive exercise. We'll work through these in the workshop!<br>\n",
    "‚ö†Ô∏è **Warning**: Heads-up about tricky stuff or common mistakes.<br>\n",
    "üí° **Tip**: How to do something a bit more efficiently or effectively.<br>\n",
    "\n",
    "### Learning Objectives\n",
    "1. [Understanding how websites work (HTML basics)](#h1)\n",
    "2. [Getting webpage content with Python](#h2)\n",
    "3. [Extracting specific information](#h3)\n",
    "4. [Collecting data from tables](#h4)\n",
    "5. [Building a simple dataset for analysis](#h5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Wikipedia?\n",
    "- It's free and open to use\n",
    "- Has tons of social science relevant data\n",
    "- Well-structured HTML that's easy to learn from\n",
    "- No login or API keys required!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Our Tools\n",
    "\n",
    "We need two main tools:\n",
    "- **requests**: This lets Python fetch webpages (like opening a website in your browser)\n",
    "- **beautifulsoup4**: This helps us read and understand the webpage content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the packages we need (only need to run once)\n",
    "%pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tools we'll use\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time  # We'll use this to be polite to Wikipedia's servers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='h1'></a>\n",
    "\n",
    "## Understanding HTML\n",
    "\n",
    "Websites are written in HTML, which uses **tags** to organize content. Think of tags like containers:\n",
    "\n",
    "- `<p>` = paragraph of text\n",
    "- `<h1>` = big heading\n",
    "- `<table>` = data table\n",
    "- `<a>` = link to another page\n",
    "\n",
    "Let's look at a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heading text: Welcome to Web Scraping\n",
      "Paragraph: This is a paragraph with some text.\n",
      "Paragraph: This is another paragraph!\n"
     ]
    }
   ],
   "source": [
    "# Here's what HTML looks like\n",
    "simple_html = \"\"\"\n",
    "<html>\n",
    "  <body>\n",
    "    <h1>Welcome to Web Scraping</h1>\n",
    "    <p>This is a paragraph with some text.</p>\n",
    "    <p>This is another paragraph!</p>\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Let's parse it with BeautifulSoup\n",
    "soup = BeautifulSoup(simple_html, 'html.parser')\n",
    "\n",
    "# Find the heading\n",
    "heading = soup.find('h1')\n",
    "print(\"Heading text:\", heading.text)\n",
    "\n",
    "# Find all paragraphs\n",
    "paragraphs = soup.find_all('p')\n",
    "for p in paragraphs:\n",
    "    print(\"Paragraph:\", p.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your First Wikipedia Scrape\n",
    "\n",
    "Let's start with something simple - getting basic information from a Wikipedia page about a major city.\n",
    "\n",
    "We'll scrape the Wikipedia page for **New York City**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Troubleshooting Tip:** If you get an error like `'NoneType' object has no attribute 'text'`, it means BeautifulSoup couldn't find the element you were looking for. This can happen if:\n",
    "- The webpage structure changed\n",
    "- Your internet connection blocked the request\n",
    "- The class or id name is different\n",
    "\n",
    "Always check if an element exists before trying to access its properties!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [403]>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Get the webpage (with headers this time!)\n",
    "url = \"https://en.wikipedia.org/wiki/New_York_City\"\n",
    "response = requests.get(url)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a list of common HTML response codes:\n",
    "\n",
    "- `200` OK ‚Üí Everything worked, page content is in .text or .content.\n",
    "- `301 / 302` Redirect ‚Üí The page moved to a new URL; requests follows redirects automatically.\n",
    "- `403` Forbidden ‚Üí Server blocked the request (often missing headers like User-Agent).\n",
    "- `404` Not Found ‚Üí The page doesn‚Äôt exist (or the URL pattern is wrong).\n",
    "- `429` Too Many Requests ‚Üí You‚Äôre being rate-limited; slow down or add delays.\n",
    "\n",
    "We're getting a `403` -- let's see what the reason is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please set a user-agent and respect our robot policy https://w.wiki/4wJS. See also T400119.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text  # Check for request errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's a User-Agent?\n",
    "\n",
    "A **user-agent** is like introducing yourself when you knock on someone's door. It tells the website:\n",
    "- Who you are (what browser/program you're using)\n",
    "- That you're a legitimate visitor, not a malicious bot\n",
    "\n",
    "Wikipedia requires this because they want to:\n",
    "- Prevent abuse from bots\n",
    "- Know who's accessing their site\n",
    "- Ensure people follow their guidelines\n",
    "\n",
    "Without a user-agent, Wikipedia will refuse your request (403 error)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up headers to identify ourselves to Wikipedia\n",
    "# This is required - Wikipedia blocks requests without a user-agent\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully connected to Wikipedia!\n",
      "Page title: Berkeley, California\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Get the webpage (with headers this time!)\n",
    "url = \"https://en.wikipedia.org/wiki/Berkeley,_California\"\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(\"Successfully connected to Wikipedia!\")\n",
    "else:\n",
    "    print(f\"Error: Got status code {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the title: it's a `h1` with a class name `firstHeading`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Parse the HTML\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Step 3: Get the page title\n",
    "soup.find('h1', class_='firstHeading')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first paragraph of a Wikipedia article usually contains a great summary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Berkeley (/Ààb…úÀêrkli/ BURK-lee) is a city on the eastern shore of San Francisco Bay in northern Alameda County, California, United States. It is named after the 18th-century Anglo-Irish bishop and philosopher George Berkeley. It borders the cities of Oakland and Emeryville to the south and the city of Albany and the unincorporated community of Kensington to the north. Its eastern border with Contra Costa County generally follows the ridge of the Berkeley Hills. The 2020 census recorded a population of 124,321.\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the first non-empty paragraph on the page\n",
    "paragraphs = soup.find_all('p')\n",
    "paragraphs[1].get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML Terminology (Cheat Sheet)\n",
    "\n",
    "- **Tag**: The basic building block in HTML.  \n",
    "  Example: `<table> ... </table>`\n",
    "\n",
    "- **Element**: A tag **plus** its content.  \n",
    "  Example: `<p>Hello</p>` is a paragraph element.\n",
    "\n",
    "- **Attribute**: Extra information about a tag, written inside the opening tag.  \n",
    "  Example: `<table class=\"wikitable\">` ‚Üí `class=\"wikitable\"` is an attribute.\n",
    "\n",
    "- **Class**: A *name* inside the `class` attribute.  \n",
    "  - An element can have **many class names**, separated by spaces.  \n",
    "  Example: `<table class=\"wikitable mw-collapsible\">`  \n",
    "  ‚Üí Classes = `wikitable`, `mw-collapsible`\n",
    "\n",
    "- **ID**: A unique identifier for one element.  \n",
    "  Example: `<div id=\"main\"> ... </div>`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü•ä Practice: Find Other Elements\n",
    "\n",
    "Now it's your turn! The Berkeley Wikipedia page has lots of interesting information. Let's practice finding different HTML elements.\n",
    "\n",
    "**Your Challenge:** \n",
    "1. Look for the **infobox** on the right side of the page (it contains quick facts)\n",
    "2. Try to extract the **Climate data** of Berkeley\n",
    "3. Find any **image captions** on the page\n",
    "4. Experiment with finding other elements!\n",
    "\n",
    "**Hints:**\n",
    "- The infobox usually has a class like `infobox` or `infobox geography`\n",
    "- Look for `<th>` (table header) and `<td>` (table data) tags within the infobox\n",
    "- Image captions are often in `<div>` tags with class `thumbcaption`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your practice code here!\n",
    "# Remember: we already have 'soup' from the NYC Wikipedia page above\n",
    "\n",
    "# 1. Get the infobox\n",
    "infobox = soup.find('table', class_='infobox')\n",
    "# Now try to find specific information within it\n",
    "\n",
    "# 2. Find the climate data table within the infobox\n",
    "soup.find('table', class_='wikitable collapsible')\n",
    "\n",
    "# 3. Try to find image captions\n",
    "# Hint: soup.find_all('div', class_='thumbcaption')\n",
    "\n",
    "# 4. Experiment with finding other elements!\n",
    "# What about links? soup.find_all('a')[:10]  # First 10 links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Data from Tables\n",
    "\n",
    "Wikipedia has lots of tables with useful data! Let's look at a page with demographic information.\n",
    "\n",
    "We'll scrape the **List of U.S. states by population**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Get the page with US state populations\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_U.S._states_and_territories_by_population\"\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Page loaded successfully!\")\n",
    "else:\n",
    "    print(f\"Error: Got status code {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table headers:\n",
      "0: State or territory\n",
      "1: Census population[8][9][a]\n",
      "2: Change,2010‚Äì2020[9][a]\n",
      "3: House seats[b]\n",
      "4: Pop. perelec. vote(2020)[c]\n",
      "5: Pop.perseat(2020)[a]\n",
      "6: % US(2020)\n",
      "7: % EC(2020)\n",
      "8: July 1, 2024 (est.)\n",
      "9: April 1, 2020\n"
     ]
    }
   ],
   "source": [
    "# Find the first table on the page (usually the main data table)\n",
    "table = soup.find('table', class_='wikitable')\n",
    "\n",
    "# Let's look at the table headers first\n",
    "headers_table = []\n",
    "for th in table.find_all('th')[:10]:  # Just first 10 headers\n",
    "    headers_table.append(th.text.strip())\n",
    "\n",
    "print(\"Table headers:\")\n",
    "for i, header in enumerate(headers_table):\n",
    "    print(f\"{i}: {header}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting State Population Data\n",
    "\n",
    "Now let's get the actual data - we'll collect state names and their populations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 US States by Population:\n",
      "----------------------------------------\n",
      "California: 39,431,263\n",
      "Texas: 31,290,831\n",
      "Florida: 23,372,215\n",
      "New York: 19,867,248\n",
      "Pennsylvania: 13,078,751\n",
      "Illinois: 12,710,158\n",
      "Ohio: 11,883,304\n",
      "Georgia: 11,180,878\n",
      "North Carolina: 11,046,024\n",
      "Michigan: 10,140,459\n"
     ]
    }
   ],
   "source": [
    "# Create lists to store our data\n",
    "states = []\n",
    "populations = []\n",
    "\n",
    "# Find all rows in the table\n",
    "rows = table.find_all('tr')[2:12]  # Skip header rows, get first 10 states\n",
    "\n",
    "for row in rows:\n",
    "    # Get all cells in this row\n",
    "    cells = row.find_all(['td', 'th'])\n",
    "    \n",
    "    if len(cells) > 3:  # Make sure we have enough cells\n",
    "        # State name is usually in the 2nd or 3rd cell\n",
    "        state_name = cells[0].text.strip()\n",
    "        # Population is usually in the 3rd or 4th cell\n",
    "        population = cells[1].text.strip()\n",
    "        \n",
    "        states.append(state_name)\n",
    "        populations.append(population)\n",
    "\n",
    "# Display what we collected\n",
    "print(\"Top 10 US States by Population:\")\n",
    "print(\"-\" * 40)\n",
    "for state, pop in zip(states, populations):\n",
    "    print(f\"{state}: {pop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Dataset - World Leaders\n",
    "\n",
    "Let's create a more complex example that would be useful for political science research.\n",
    "We'll scrape information about current world leaders!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page loaded!\n"
     ]
    }
   ],
   "source": [
    "# Get the list of current heads of state\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_current_heads_of_state_and_government\"\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"Page loaded!\")\n",
    "else:\n",
    "    print(f\"Error: Got status code {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of World Leaders:\n",
      "--------------------------------------------------\n",
      "Country: Afghanistan\n",
      "Leader: Supreme Leader¬†‚Äì Hibatullah Akhundzada\n",
      "--------------------------------------------------\n",
      "Country: Albania\n",
      "Leader: President¬†‚Äì Bajram Begaj\n",
      "--------------------------------------------------\n",
      "Country: Algeria\n",
      "Leader: President¬†‚Äì Abdelmadjid Tebboune\n",
      "--------------------------------------------------\n",
      "Country: Andorra\n",
      "Leader: Episcopal Co-Prince¬†‚Äì Josep-Llu√≠s Serrano Pentinat\n",
      "--------------------------------------------------\n",
      "Country: Angola\n",
      "Leader: President¬†‚Äì Jo√£o Louren√ßo\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Find tables with leader information\n",
    "tables = soup.find_all('table', class_='wikitable')\n",
    "\n",
    "# Let's focus on the first table we find\n",
    "if tables:\n",
    "    first_table = tables[0]\n",
    "    \n",
    "    # Extract first 5 rows as examples\n",
    "    leaders_data = []\n",
    "    rows = first_table.find_all('tr')[1:6]  # Skip header\n",
    "    \n",
    "    for row in rows:\n",
    "        cells = row.find_all(['td', 'th'])\n",
    "        if len(cells) >= 2:\n",
    "            # Try to extract country and leader name\n",
    "            country = cells[0].text.strip()\n",
    "            leader = cells[1].text.strip() if len(cells) > 1 else \"N/A\"\n",
    "            \n",
    "            # Clean up the text (remove extra whitespace and references like [1])\n",
    "            country = country.split('[')[0].strip()\n",
    "            leader = leader.split('[')[0].strip()\n",
    "            \n",
    "            leaders_data.append({\n",
    "                'country': country,\n",
    "                'leader': leader\n",
    "            })\n",
    "    \n",
    "    print(\"Sample of World Leaders:\")\n",
    "    print(\"-\" * 50)\n",
    "    for data in leaders_data:\n",
    "        print(f\"Country: {data['country'][:30]}\")\n",
    "        print(f\"Leader: {data['leader'][:50]}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Practical Example - Historical Events Timeline\n",
    "\n",
    "Let's scrape events from a specific year.\n",
    "We'll look at major events from 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the 2020 events page\n",
    "url = \"https://en.wikipedia.org/wiki/2020\"\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "January 1\n",
      "Croatia begins its term in the presidency of the European Union.[6]\n",
      "Flash floods struck Jakarta, Indonesia, killing 66 people in the worst flooding in over a decade.[7]\n",
      "January 2 ‚Äì The Royal Australian Air Force and Navy are deployed to New South Wales and Victoria to assist mass evacuation efforts amidst the 2019‚Äì20 Australian bushfire season.[8][9]\n",
      "January 3 ‚Äì  A United States drone strike at Baghdad International Airport kills ten people, including the intended target, an Iranian general Qasem Soleimani and Iraqi paramilitary leader Abu Mahdi al-Muhandis.[10]\n",
      "January 5\n",
      "Second Libyan Civil War: President Recep Tayyip Erdoƒüan announces the deployment of Turkish troops to Libya on behalf of the United Nations-backed Government of National Accord.[11]\n",
      "2019‚Äì20 Croatian presidential election: The second round of voting is held, and Zoran Milanoviƒá of the Social Democratic Party of Croatia defeats incumbent president Kolinda Grabar-Kitaroviƒá.[12]\n",
      "January 8\n",
      "Iran launches ballistic missiles at two Iraqi military bases hosting U.S. soldiers, injuring over 100 personnel.[13]\n",
      "Ukraine International Airlines Flight 752 was shot down by Iranian forces shortly after takeoff from Tehran Imam Khomeini International Airport, killing all 176 people on board.[14]\n"
     ]
    }
   ],
   "source": [
    "# Just find the Events section and grab the first few lists after it\n",
    "events_h2 = soup.find('h2', id='Events')\n",
    "first_ul = events_h2.parent.find_next_sibling('ul')\n",
    "\n",
    "# Get first 5 events from that list\n",
    "for i in first_ul.find_all('li', recursive=False)[:5]:\n",
    "    print(i.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Your Data\n",
    "\n",
    "Once you've scraped data, you'll want to save it for analysis!\n",
    "Let's save our state population data to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['January 2',\n",
       " 'The Royal Australian Air Force and Navy are deployed to New South Wales and Victoria to assist mass evacuation efforts amidst the 2019‚Äì20 Australian bushfire season.']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Extract events\n",
    "data = []\n",
    "for li in first_ul.find_all('li', recursive=False)[:20]:\n",
    "    text = li.text.split('[')[0]\n",
    "    if ' ‚Äì ' in text:\n",
    "        date, event = text.split(' ‚Äì ', 1)\n",
    "        data.append([date, event])\n",
    "\n",
    "# preview\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write CSV\n",
    "with open('events.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Date', 'Event'])\n",
    "    writer.writerows(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important: Web Scraping Ethics and Best Practices\n",
    "\n",
    "### Always Remember:\n",
    "\n",
    "1. **Check the website's rules**: Look for a `robots.txt` file (e.g., https://en.wikipedia.org/robots.txt)\n",
    "2. **Be polite**: Don't make too many requests too quickly\n",
    "3. **Give credit**: Always cite Wikipedia as your data source\n",
    "4. **Respect copyrights**: Wikipedia content is under Creative Commons license\n",
    "5. **Use APIs when available**: Wikipedia has an API that's better for large-scale data collection\n",
    "\n",
    "### Being a Good Web Scraper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped: Sociology\n",
      "Scraped: Psychology\n",
      "Scraped: Political science\n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "urls = [\n",
    "    \"https://en.wikipedia.org/wiki/Sociology\",\n",
    "    \"https://en.wikipedia.org/wiki/Psychology\",\n",
    "    \"https://en.wikipedia.org/wiki/Political_science\"\n",
    "]\n",
    "\n",
    "for url in urls:\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Wikipedia uses h1 for title\n",
    "    title = soup.find('h1').text\n",
    "    \n",
    "    print(f\"Scraped: {title}\")\n",
    "    time.sleep(1)  # Be polite!\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Build a Dataset of Universities\n",
    "Scrape the list of Ivy League universities and their founding years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "# Start with: url = \"https://en.wikipedia.org/wiki/Ivy_League\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Create a Timeline\n",
    "Pick a historical event page and extract a timeline of key dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "# Try: \"https://en.wikipedia.org/wiki/COVID-19_pandemic\"\n",
    "# or: \"https://en.wikipedia.org/wiki/Arab_Spring\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "üéâ **Congratulations!** You've learned the basics of web scraping!\n",
    "\n",
    "### What You've Learned:\n",
    "- How to fetch web pages with Python\n",
    "- How to parse HTML and extract specific information\n",
    "- How to work with tables and lists\n",
    "- How to save data for later analysis\n",
    "- Ethical web scraping practices\n",
    "\n",
    "### Where to Go From Here:\n",
    "1. **Practice with different Wikipedia pages** - try scraping data relevant to your research\n",
    "2. **Learn about APIs** - many sites offer cleaner ways to get data\n",
    "3. **Explore pandas** - a Python library great for analyzing scraped data\n",
    "4. **Try more complex scraping** - multiple pages, following links, etc.\n",
    "\n",
    "### Resources:\n",
    "- [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Wikipedia API](https://www.mediawiki.org/wiki/API:Main_page)\n",
    "- [Web Scraping Ethics](https://blog.apify.com/is-web-scraping-legal/)\n",
    "\n",
    "Remember: Web scraping is a powerful tool for social science research. Use it wisely and ethically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlab2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
