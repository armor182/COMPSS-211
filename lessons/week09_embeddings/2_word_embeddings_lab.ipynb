{
 "cells": [
  {
   "cell_type": "code",
   "source": "# Example: Document embeddings with Doc2Vec\n# from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n\n# # Prepare tagged documents\n# tagged_docs = [TaggedDocument(doc, [i]) for i, doc in enumerate(corpus)]\n\n# # Train Doc2Vec model\n# doc_model = Doc2Vec(\n#     tagged_docs,\n#     vector_size=100,\n#     window=5,\n#     min_count=5,\n#     workers=4,\n#     epochs=20\n# )\n\n# # Find similar documents\n# doc_id = 0  # Document to query\n# similar_docs = doc_model.dv.most_similar(doc_id, topn=5)\n# print(f\"Documents most similar to document {doc_id}:\")\n# for doc_id, similarity in similar_docs:\n#     print(f\"  Document {doc_id}: {similarity:.3f}\")\n#     print(f\"    Preview: {df.iloc[doc_id]['text_column'][:100]}...\")\n\n# # Infer vector for new document\n# new_doc = \"Your new document text here\".split()\n# new_vector = doc_model.infer_vector(new_doc)\n# similar_to_new = doc_model.dv.most_similar([new_vector], topn=3)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4. Document Embeddings with Doc2Vec\n\n**Challenge**: Move beyond word embeddings to represent entire documents as vectors.\n\n**Tasks:**\n- Implement Doc2Vec on your corpus\n- Compare document similarities\n- Build a document recommendation system\n- Cluster documents based on their embeddings\n- Create a semantic search engine for your documents\n\n**Applications:**\n- Find the most similar documents to a query document\n- Classify documents based on their embeddings\n- Track how document themes evolve over time",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Example: Working with multilingual embeddings\n# # If you have text in multiple languages\n# from langdetect import detect\n\n# # Detect language of documents\n# df['language'] = df['text_column'].apply(lambda x: detect(x) if len(x) > 20 else 'unknown')\n\n# # Separate by language\n# english_docs = df[df['language'] == 'en']['processed_text'].tolist()\n# spanish_docs = df[df['language'] == 'es']['processed_text'].tolist()\n\n# # Train separate models\n# model_en = Word2Vec(english_docs, vector_size=100, window=5, min_count=5)\n# model_es = Word2Vec(spanish_docs, vector_size=100, window=5, min_count=5)\n\n# # Compare similar words across languages\n# test_word_en = 'computer'\n# if test_word_en in model_en.wv:\n#     similar_en = model_en.wv.most_similar(test_word_en, topn=5)\n#     print(f\"English - similar to '{test_word_en}':\")\n#     for word, score in similar_en:\n#         print(f\"  {word}: {score:.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 3. Cross-lingual Word Embeddings\n\n**Challenge**: If you have multilingual data, explore cross-lingual embeddings.\n\n**Tasks:**\n- Train separate embeddings for different languages in your data\n- Use aligned embeddings to find translations\n- Compare semantic spaces across languages\n- Identify language-specific concepts\n- Build a simple cross-lingual similarity finder\n\n**Visualization ideas:**\n- Plot the same concepts in different language spaces\n- Show how well concepts align across languages\n- Identify culture-specific terms with no direct translation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Reduce topic overlap if needed\n# # If topics are too granular, merge similar ones\n# target_num_topics = 10  # Desired number of topics\n# topic_model.reduce_topics(documents, nr_topics=target_num_topics)\n\n# # Re-visualize after reduction\n# fig_reduced = topic_model.visualize_topics()\n# fig_reduced.show()\n\n# # Extract all documents from a specific topic\n# topic_of_interest = 2\n# topic_docs_indices = [i for i, t in enumerate(topics) if t == topic_of_interest]\n# df_topic_subset = df.iloc[topic_docs_indices].copy()\n# print(f\"Found {len(df_topic_subset)} documents in Topic {topic_of_interest}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualize topics with BERTopic\n# # Create interactive visualizations\n# fig_topics = topic_model.visualize_topics()\n# fig_topics.show()\n\n# # Visualize topic hierarchy\n# fig_hierarchy = topic_model.visualize_hierarchy()\n# fig_hierarchy.show()\n\n# # Find representative documents for a topic\n# topic_num = 0  # Choose your topic\n# representative_docs = topic_model.get_representative_docs(topic_num)\n# print(f\"\\nRepresentative documents for Topic {topic_num}:\")\n# for i, doc in enumerate(representative_docs[:3], 1):\n#     print(f\"\\n{i}. {doc[:200]}...\")  # First 200 chars",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Example: Topic modeling with BERTopic\n# Note: This may require installation: pip install bertopic\n\n# from bertopic import BERTopic\n\n# # Prepare your documents (use your cleaned text)\n# documents = df['text_column'].tolist()  # Replace with your text column\n\n# # Create and fit BERTopic model\n# topic_model = BERTopic(verbose=True)\n# topics, probabilities = topic_model.fit_transform(documents)\n\n# # Get topic information\n# topic_info = topic_model.get_topic_info()\n# print(f\"Number of topics found: {len(topic_info) - 1}\")  # -1 for outlier topic\n# print(\"\\nTop 5 topics by frequency:\")\n# print(topic_info.head(6))  # Including outlier topic (-1)\n\n# # Get top words for a specific topic\n# topic_words = topic_model.get_topic(0)  # Topic 0\n# print(f\"\\nTop words for Topic 0:\")\n# for word, score in topic_words[:10]:\n#     print(f\"  {word}: {score:.4f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 2. Topic Modeling with BERTopic\n\n**Challenge**: Use modern neural topic modeling to discover and analyze themes in your corpus.\n\n**Tasks:**\n- Install and configure BERTopic for your dataset\n- Extract topics using transformer-based embeddings\n- Compare topics to your word embedding clusters\n- Create interactive visualizations of topic distributions\n- Find representative documents for each topic\n\n**Analysis ideas:**\n- How do BERTopic themes compare to word embedding clusters?\n- Which topics are most prevalent in your dataset?\n- Can you track topic evolution over time (if temporal data)?\n- How do different documents get classified into topics?",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Example: Compare Skip-gram vs CBOW\n# from gensim.models import Word2Vec\n\n# # Skip-gram model (sg=1)\n# model_skipgram = Word2Vec(\n#     sentences=corpus,\n#     vector_size=100,\n#     window=5,\n#     min_count=5,\n#     sg=1,  # Skip-gram\n#     epochs=10,\n#     seed=42\n# )\n\n# # CBOW model (sg=0)\n# model_cbow = Word2Vec(\n#     sentences=corpus,\n#     vector_size=100,\n#     window=5,\n#     min_count=5,\n#     sg=0,  # CBOW\n#     epochs=10,\n#     seed=42\n# )\n\n# # Compare performance on analogies or similarity tasks\n# test_word = 'your_test_word'\n# if test_word in model_skipgram.wv and test_word in model_cbow.wv:\n#     print(f\"Skip-gram similar to '{test_word}':\")\n#     for word, score in model_skipgram.wv.most_similar(test_word, topn=5):\n#         print(f\"  {word}: {score:.3f}\")\n#     \n#     print(f\"\\nCBOW similar to '{test_word}':\")\n#     for word, score in model_cbow.wv.most_similar(test_word, topn=5):\n#         print(f\"  {word}: {score:.3f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 1. Advanced Word2Vec/FastText Parameters\n\nExperiment with different model architectures and training parameters:\n\n**Tasks:**\n- Compare Skip-gram vs CBOW architectures\n- Test different vector dimensions (50, 100, 200, 300)\n- Vary the context window size\n- Use hierarchical softmax vs negative sampling\n- Compare Word2Vec with FastText on out-of-vocabulary words\n\n**Questions to explore:**\n- How does model performance change with vector dimensionality?\n- Which architecture works better for your specific dataset?\n- How do rare words perform with different parameter settings?",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings Lab: Apply to Your Own Dataset\n",
    "\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### Lab Objectives\n",
    "\n",
    "* Train word embeddings on your chosen dataset\n",
    "* Explore semantic relationships specific to your domain\n",
    "* Analyze biases and patterns in your text corpus\n",
    "* Practice interpreting embedding results in context\n",
    "</div>\n",
    "\n",
    "### Getting Started\n",
    "\n",
    "Choose a text dataset that interests you. This could be:\n",
    "- News articles from a specific time period or topic\n",
    "- Social media posts (Twitter, Reddit, etc.)\n",
    "- Academic papers or book reviews\n",
    "- Product reviews or customer feedback\n",
    "- Historical documents or literature\n",
    "\n",
    "**Requirements**: Your dataset should have at least 1,000 documents with meaningful text content.\n",
    "\n",
    "### Sections\n",
    "1. [Data Loading and Exploration](#section1)\n",
    "2. [Text Preprocessing](#section2)\n",
    "3. [Training Word Embeddings](#section3)\n",
    "4. [Exploring Semantic Relationships](#section4)\n",
    "5. [Word Analogies in Your Domain](#section5)\n",
    "6. [Bias Analysis](#section6)\n",
    "7. [Visualization and Interpretation](#section7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Import the necessary libraries for text processing and embedding analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models import FastText, Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "## 1. Data Loading and Exploration\n",
    "\n",
    "Load your dataset and explore its basic properties. Understanding your data is crucial for effective preprocessing and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Your Dataset\n",
    "\n",
    "Load your chosen dataset. Identify which column(s) contain the text you want to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset here\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "# or df = pd.read_json('your_data.json')\n",
    "# or however you load your data\n",
    "\n",
    "# Display basic information about your dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Text Properties\n",
    "\n",
    "Examine the characteristics of your text data: length distribution, common words, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore your text column\n",
    "# Replace 'text_column' with the actual name of your text column\n",
    "\n",
    "# Calculate text lengths\n",
    "# df['text_length'] = df['text_column'].str.len()\n",
    "\n",
    "# Display statistics\n",
    "# print(f\"Average text length: {df['text_length'].mean():.0f} characters\")\n",
    "# print(f\"Median text length: {df['text_length'].median():.0f} characters\")\n",
    "\n",
    "# Show sample texts\n",
    "# print(\"\\nSample texts:\")\n",
    "# for i in range(3):\n",
    "#     print(f\"{i+1}. {df['text_column'].iloc[i][:200]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Data Distribution\n",
    "\n",
    "Create visualizations to understand your data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations relevant to your dataset\n",
    "# Examples:\n",
    "# - Text length histogram\n",
    "# - Distribution of categories/labels (if applicable)\n",
    "# - Time series of posts (if you have dates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "## 2. Text Preprocessing\n",
    "\n",
    "Clean and tokenize your text data. The preprocessing steps will depend on your specific dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Preprocessing Function\n",
    "\n",
    "Create a function to clean and tokenize your text. Consider what domain-specific terms or patterns you want to preserve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and tokenize text for embedding training.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Add domain-specific preprocessing here\n",
    "    # Examples:\n",
    "    # - Remove URLs: text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    # - Remove mentions: text = re.sub(r'@\\w+', '', text)\n",
    "    # - Handle special characters specific to your domain\n",
    "    \n",
    "    # Basic tokenization\n",
    "    tokens = re.findall(r\"\\b[a-z]+(?:'[a-z]+)?\\b|[a-z]+\", text)\n",
    "    \n",
    "    # Filter tokens based on your needs\n",
    "    # - Remove very short words: tokens = [t for t in tokens if len(t) > 2]\n",
    "    # - Keep domain-specific short terms\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Test your preprocessing function\n",
    "# sample_text = \"Your sample text here\"\n",
    "# print(f\"Original: {sample_text}\")\n",
    "# print(f\"Processed: {preprocess_text(sample_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Your Corpus\n",
    "\n",
    "Apply preprocessing to your entire dataset and prepare it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to your entire dataset\n",
    "# corpus = df['text_column'].apply(preprocess_text).tolist()\n",
    "\n",
    "# Remove empty documents\n",
    "# corpus = [doc for doc in corpus if len(doc) > 0]\n",
    "\n",
    "# Display corpus statistics\n",
    "# print(f\"Total documents: {len(corpus)}\")\n",
    "# print(f\"Average tokens per document: {np.mean([len(doc) for doc in corpus]):.0f}\")\n",
    "# print(f\"\\nSample processed document:\")\n",
    "# print(corpus[0][:20])  # First 20 tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "## 3. Training Word Embeddings\n",
    "\n",
    "Train a word embeddings model on your preprocessed corpus. Choose between Word2Vec and FastText based on your data characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Your Model\n",
    "\n",
    "**Word2Vec** vs **FastText**:\n",
    "- Use **FastText** if your text has many misspellings, abbreviations, or domain-specific terms\n",
    "- Use **Word2Vec** for cleaner, more formal text\n",
    "\n",
    "Adjust the parameters based on your corpus size and characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your embedding model\n",
    "print(\"Training word embedding model...\")\n",
    "\n",
    "# Choose one of these approaches:\n",
    "\n",
    "# Option 1: FastText (good for noisy text)\n",
    "# model = FastText(\n",
    "#     sentences=corpus,\n",
    "#     vector_size=100,      # Dimensionality of embeddings\n",
    "#     window=5,             # Context window size\n",
    "#     min_count=5,          # Ignore words appearing less than 5 times\n",
    "#     min_n=3,              # Min character n-gram\n",
    "#     max_n=6,              # Max character n-gram\n",
    "#     sg=1,                 # Use Skip-gram\n",
    "#     epochs=10,            # Training iterations\n",
    "#     seed=42\n",
    "# )\n",
    "\n",
    "# Option 2: Word2Vec (good for clean text)\n",
    "# model = Word2Vec(\n",
    "#     sentences=corpus,\n",
    "#     vector_size=100,      # Dimensionality of embeddings\n",
    "#     window=5,             # Context window size\n",
    "#     min_count=5,          # Ignore words appearing less than 5 times\n",
    "#     sg=1,                 # Use Skip-gram\n",
    "#     epochs=10,            # Training iterations\n",
    "#     seed=42\n",
    "# )\n",
    "\n",
    "# Print model statistics\n",
    "# print(f\"Model trained! Vocabulary size: {len(model.wv)}\")\n",
    "# print(f\"Vector dimensions: {model.wv.vector_size}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Your Model\n",
    "\n",
    "Verify that your model learned meaningful representations by testing some domain-relevant words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your model with relevant terms from your domain\n",
    "# test_words = ['word1', 'word2', 'word3']  # Replace with your domain terms\n",
    "\n",
    "# Check which words are in vocabulary\n",
    "# print(\"Words in vocabulary:\")\n",
    "# for word in test_words:\n",
    "#     if word in model.wv:\n",
    "#         print(f\"✓ '{word}' - vector shape: {model.wv[word].shape}\")\n",
    "#     else:\n",
    "#         print(f\"✗ '{word}' - not in vocabulary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "## 4. Exploring Semantic Relationships\n",
    "\n",
    "Investigate how your model captures relationships between domain-specific terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Similar Words\n",
    "\n",
    "Explore what words your model considers similar to key terms in your domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore similar words for key terms in your domain\n",
    "# key_terms = ['term1', 'term2', 'term3']  # Replace with your terms\n",
    "\n",
    "# for term in key_terms:\n",
    "#     if term in model.wv:\n",
    "#         print(f\"\\nMost similar to '{term}':\")\n",
    "#         for similar_word, score in model.wv.most_similar(term, topn=5):\n",
    "#             print(f\"  {similar_word}: {score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Semantic Similarities\n",
    "\n",
    "Compute similarities between related concepts in your domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarities between domain-specific term pairs\n",
    "# term_pairs = [('word1', 'word2'), ('word3', 'word4')]  # Your term pairs\n",
    "\n",
    "# print(\"Semantic similarities:\")\n",
    "# for term1, term2 in term_pairs:\n",
    "#     if term1 in model.wv and term2 in model.wv:\n",
    "#         similarity = model.wv.similarity(term1, term2)\n",
    "#         print(f\"{term1} ↔ {term2}: {similarity:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Similarity Heatmap\n",
    "\n",
    "Visualize relationships between multiple terms using a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a similarity heatmap for important terms in your domain\n",
    "# important_terms = ['term1', 'term2', 'term3', 'term4']  # Your terms\n",
    "\n",
    "# Filter to terms that exist in vocabulary\n",
    "# available_terms = [t for t in important_terms if t in model.wv]\n",
    "\n",
    "# Calculate similarity matrix\n",
    "# n_terms = len(available_terms)\n",
    "# similarity_matrix = np.zeros((n_terms, n_terms))\n",
    "\n",
    "# for i, term1 in enumerate(available_terms):\n",
    "#     for j, term2 in enumerate(available_terms):\n",
    "#         similarity_matrix[i, j] = model.wv.similarity(term1, term2)\n",
    "\n",
    "# Plot heatmap\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(similarity_matrix, \n",
    "#             xticklabels=available_terms,\n",
    "#             yticklabels=available_terms,\n",
    "#             cmap='YlOrRd',\n",
    "#             annot=True,\n",
    "#             fmt='.2f',\n",
    "#             cbar_kws={'label': 'Cosine Similarity'})\n",
    "# plt.title('Similarity Matrix of Key Terms')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "## 5. Word Analogies in Your Domain\n",
    "\n",
    "Test word analogies that are relevant to your specific domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Domain-Specific Analogies\n",
    "\n",
    "Think about relationships that might exist in your domain. What patterns would you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_analogy(model, positive, negative, description):\n",
    "    \"\"\"Test a word analogy and display results.\"\"\"\n",
    "    try:\n",
    "        result = model.wv.most_similar(positive=positive, negative=negative, topn=3)\n",
    "        print(f\"\\n{description}:\")\n",
    "        for word, score in result:\n",
    "            print(f\"  {word}: {score:.3f}\")\n",
    "    except KeyError:\n",
    "        print(f\"\\n{description}: Some words not in vocabulary\")\n",
    "\n",
    "# Test domain-specific analogies\n",
    "# Examples to adapt to your domain:\n",
    "# test_analogy(model, ['word1', 'word2'], ['word3'], 'word1 - word3 + word2')\n",
    "# test_analogy(model, ['word4', 'word5'], ['word6'], 'word4 - word6 + word5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Conceptual Dimensions\n",
    "\n",
    "Create vectors that represent conceptual differences and test them on other terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create conceptual vectors by subtracting related terms\n",
    "# For example: concept_vector = model.wv['positive_term'] - model.wv['negative_term']\n",
    "\n",
    "# Test how other words align with this conceptual dimension\n",
    "# test_terms = ['term1', 'term2', 'term3']  # Terms to test\n",
    "\n",
    "# for term in test_terms:\n",
    "#     if term in model.wv:\n",
    "#         similarity = np.dot(concept_vector, model.wv[term]) / (\n",
    "#             np.linalg.norm(concept_vector) * np.linalg.norm(model.wv[term]))\n",
    "#         print(f\"{term}: {similarity:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6'></a>\n",
    "## 6. Bias Analysis\n",
    "\n",
    "Investigate potential biases in your embeddings using semantic axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Semantic Axes\n",
    "\n",
    "Create semantic axes relevant to your domain. Think about what opposing concepts might exist in your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_semantic_axis(positive_words, negative_words, model):\n",
    "    \"\"\"Create a semantic axis from two sets of pole words.\"\"\"\n",
    "    # Get embeddings for available words\n",
    "    pos_vectors = [model.wv[w] for w in positive_words if w in model.wv]\n",
    "    neg_vectors = [model.wv[w] for w in negative_words if w in model.wv]\n",
    "    \n",
    "    if not pos_vectors or not neg_vectors:\n",
    "        return None\n",
    "    \n",
    "    # Calculate means and return difference\n",
    "    pos_mean = np.mean(pos_vectors, axis=0)\n",
    "    neg_mean = np.mean(neg_vectors, axis=0)\n",
    "    return pos_mean - neg_mean\n",
    "\n",
    "def project_on_axis(word, model, axis):\n",
    "    \"\"\"Project a word onto a semantic axis.\"\"\"\n",
    "    if word not in model.wv or axis is None:\n",
    "        return None\n",
    "    word_vector = model.wv[word]\n",
    "    return np.dot(word_vector, axis) / (np.linalg.norm(word_vector) * np.linalg.norm(axis))\n",
    "\n",
    "# Define semantic axes relevant to your domain\n",
    "# Example: positive vs negative sentiment\n",
    "# positive_terms = ['good', 'great', 'excellent', 'amazing']\n",
    "# negative_terms = ['bad', 'terrible', 'awful', 'horrible']\n",
    "# sentiment_axis = create_semantic_axis(positive_terms, negative_terms, model)\n",
    "\n",
    "# Define other axes relevant to your domain\n",
    "# formal_terms = ['formal', 'professional', 'official']\n",
    "# informal_terms = ['casual', 'informal', 'relaxed']\n",
    "# formality_axis = create_semantic_axis(formal_terms, informal_terms, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Words Against Your Axes\n",
    "\n",
    "Project domain-relevant terms onto your semantic axes to reveal patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test how different terms project onto your semantic axes\n",
    "# test_words = ['word1', 'word2', 'word3', 'word4']  # Domain-relevant terms\n",
    "\n",
    "# print(\"Projections onto semantic axis:\")\n",
    "# print(\"(Positive values = closer to positive pole, Negative = closer to negative pole)\")\n",
    "# print(\"-\" * 60)\n",
    "\n",
    "# projections = {}\n",
    "# for word in test_words:\n",
    "#     projection = project_on_axis(word, model, sentiment_axis)  # Use your axis\n",
    "#     if projection is not None:\n",
    "#         projections[word] = projection\n",
    "#         print(f\"{word:15} {projection:+.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Bias Patterns\n",
    "\n",
    "Create visualizations to show how terms cluster along your semantic dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot showing projections\n",
    "# if projections:\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     words = list(projections.keys())\n",
    "#     values = list(projections.values())\n",
    "#     colors = ['green' if v > 0 else 'red' for v in values]\n",
    "    \n",
    "#     plt.barh(words, values, color=colors)\n",
    "#     plt.xlabel('Semantic Axis Projection')\n",
    "#     plt.title('Word Projections on Semantic Axis')\n",
    "#     plt.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section7'></a>\n",
    "## 7. Visualization and Interpretation\n",
    "\n",
    "Create comprehensive visualizations to understand your embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Embedding Visualization\n",
    "\n",
    "Use PCA or t-SNE to visualize your embeddings in 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select important terms from your domain for visualization\n",
    "# visualization_terms = ['term1', 'term2', 'term3']  # Your important terms\n",
    "\n",
    "# Get vectors for available terms\n",
    "# available_viz_terms = [t for t in visualization_terms if t in model.wv]\n",
    "# if available_viz_terms:\n",
    "#     term_vectors = np.array([model.wv[term] for term in available_viz_terms])\n",
    "\n",
    "#     # Reduce to 2D using PCA\n",
    "#     pca = PCA(n_components=2, random_state=42)\n",
    "#     coords_2d = pca.fit_transform(term_vectors)\n",
    "\n",
    "#     # Create visualization\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "#     plt.scatter(coords_2d[:, 0], coords_2d[:, 1], s=100, alpha=0.6)\n",
    "\n",
    "#     # Add labels\n",
    "#     for i, term in enumerate(available_viz_terms):\n",
    "#         plt.annotate(term, (coords_2d[i, 0], coords_2d[i, 1]), \n",
    "#                     xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "\n",
    "#     plt.title('2D Visualization of Domain Terms')\n",
    "#     plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "#     plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "#     plt.grid(True, alpha=0.3)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Dimensional Analysis\n",
    "\n",
    "If you created multiple semantic axes, create a 2D plot showing how terms position along two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 2D plot with two semantic axes\n",
    "# analysis_terms = ['term1', 'term2', 'term3']  # Terms to analyze\n",
    "\n",
    "# Calculate projections on both axes (if you have two)\n",
    "# axis1_projections = []\n",
    "# axis2_projections = []\n",
    "# plot_terms = []\n",
    "\n",
    "# for term in analysis_terms:\n",
    "#     proj1 = project_on_axis(term, model, your_first_axis)\n",
    "#     proj2 = project_on_axis(term, model, your_second_axis)\n",
    "    \n",
    "#     if proj1 is not None and proj2 is not None:\n",
    "#         axis1_projections.append(proj1)\n",
    "#         axis2_projections.append(proj2)\n",
    "#         plot_terms.append(term)\n",
    "\n",
    "# if plot_terms:\n",
    "#     plt.figure(figsize=(10, 8))\n",
    "#     plt.scatter(axis1_projections, axis2_projections, s=100, alpha=0.6)\n",
    "    \n",
    "#     for i, term in enumerate(plot_terms):\n",
    "#         plt.annotate(term, (axis1_projections[i], axis2_projections[i]),\n",
    "#                     xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "    \n",
    "#     plt.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "#     plt.axvline(x=0, color='gray', linestyle='--', alpha=0.5)\n",
    "#     plt.xlabel('First Semantic Dimension')\n",
    "#     plt.ylabel('Second Semantic Dimension')\n",
    "#     plt.title('Terms in 2D Semantic Space')\n",
    "#     plt.grid(True, alpha=0.3)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neighborhood Exploration\n",
    "\n",
    "Visualize neighborhoods around key terms to understand local semantic structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select anchor words and their neighborhoods\n",
    "# anchors = ['anchor1', 'anchor2', 'anchor3']  # Your key terms\n",
    "# points = {}\n",
    "\n",
    "# for anchor in anchors:\n",
    "#     if anchor in model.wv:\n",
    "#         points[anchor] = model.wv[anchor]\n",
    "#         # Add similar words to the anchor\n",
    "#         for neighbor, _ in model.wv.most_similar(anchor, topn=5):\n",
    "#             if neighbor not in points:\n",
    "#                 points[neighbor] = model.wv[neighbor]\n",
    "\n",
    "# if points:\n",
    "#     words = list(points.keys())\n",
    "#     X = np.vstack([points[w] for w in words])\n",
    "\n",
    "#     # Use t-SNE for neighborhood visualization\n",
    "#     perp = min(30, max(5, len(words)//5))\n",
    "#     tsne = TSNE(n_components=2, perplexity=perp, random_state=42)\n",
    "#     X2 = tsne.fit_transform(X)\n",
    "\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "#     plt.scatter(X2[:, 0], X2[:, 1], alpha=0.7)\n",
    "#     for i, word in enumerate(words):\n",
    "#         plt.annotate(word, (X2[i, 0], X2[i, 1]), \n",
    "#                     xytext=(3, 3), textcoords='offset points', fontsize=9)\n",
    "#     plt.title('Semantic Neighborhoods in Your Domain')\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Discussion and Reflection\n",
    "\n",
    "Work with your group to discuss these questions. Be prepared to share your insights with the class:\n",
    "\n",
    "1. **Semantic Patterns**: What were the most interesting patterns you discovered in how words cluster in semantic space?\n",
    "\n",
    "2. **Bias and Assumptions**: What biases did you notice in the word embeddings? How might these affect analysis of social discourse?\n",
    "\n",
    "3. **Topic Differences**: How do different topics in your data differ in their semantic positioning?\n",
    "\n",
    "4. **Methodological Insights**: What are the strengths and limitations of using word embeddings for analyzing your genre of discourse?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion Notes\n",
    "\n",
    "Use this space to document your group's findings and insights:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most interesting semantic patterns:**\n",
    "- [Write your observations here]\n",
    "\n",
    "**Biases discovered:**\n",
    "- [Document any biases you found]\n",
    "\n",
    "**Topic differences:**\n",
    "- [Note how different topics cluster or separate]\n",
    "\n",
    "**Methodological insights:**\n",
    "- Strengths: [What worked well?]\n",
    "- Limitations: [What were the challenges or limitations?]\n",
    "- Future directions: [What would you explore next?]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 🚀 Stretch Goals\n\nFor students who complete the lab early and want to explore further:",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}