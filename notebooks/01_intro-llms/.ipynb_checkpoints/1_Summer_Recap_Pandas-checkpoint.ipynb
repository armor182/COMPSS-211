{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Summer Recap: Data Analysis with Pandas\n\n* * * \n\n<div class=\"alert alert-success\">  \n    \n### Learning Objectives \n    \n* Review fundamental pandas operations for data manipulation and analysis.\n* Apply data cleaning techniques to real-world social science datasets.\n* Practice exploratory data analysis using descriptive statistics and basic visualizations.\n* Demonstrate ability to filter, group, and aggregate data using pandas methods.\n* Evaluate LLM-generated code for accuracy and best practices.\n</div>\n\n### Icons Used in This Notebook\nüîî **Question**: A quick question to help you understand what's going on.<br>\nü•ä **Challenge**: Interactive excersise. We'll work through these in the workshop!<br>\nüí° **Tip**: How to do something a bit more efficiently or effectively.<br>\n‚ö†Ô∏è **Warning:** Heads-up about tricky stuff or common mistakes.<br>\nü§ñ **AI Generated**: Code generated by an LLM that we'll test and debug.<br>\n\n### Sections\n1. [Data Loading and Initial Exploration](#section1)\n2. [Data Cleaning and Basic Operations](#section2)\n3. [Exploratory Data Analysis](#section3)\n4. [Text Analysis Fundamentals](#section4)\n5. [Working with LLM-Generated Code](#section5)",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "\n",
    "# Data Loading and Initial Exploration\n",
    "\n",
    "Today we'll work with data from Reddit's \"Am I the Asshole?\" (AITA) subreddit. This dataset contains posts where people describe situations and ask for community judgment about their behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../../../data/aita_top_submissions.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumn names:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 1: Data Overview\n",
    "\n",
    "Explore the dataset structure and provide a summary of what you find. Use pandas methods to:\n",
    "1. Check the data types of each column\n",
    "2. Look for missing values\n",
    "3. Get basic descriptive statistics for numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîî **Question**: What do you notice about the `selftext` column? What might this tell us about the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "\n",
    "# Data Cleaning and Basic Operations\n",
    "\n",
    "Real-world data often requires cleaning before analysis. Let's examine our dataset for common issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate posts\n",
    "print(f\"Number of duplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "# Look at the distribution of some key variables\n",
    "print(f\"\\nScore statistics:\")\n",
    "print(df['score'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 2: Data Cleaning\n",
    "\n",
    "Clean the dataset by:\n",
    "1. Removing any posts where `selftext` is missing or empty\n",
    "2. Creating a new column called `text_length` that contains the character count of `selftext`\n",
    "3. Filter out posts that are shorter than 100 characters (likely low-quality posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Tip**: Use the `.str.len()` method to get string lengths in pandas. Remember that missing values might cause issues, so handle them first!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Dates\n",
    "\n",
    "The `created` column contains Unix timestamps. Let's convert these to readable dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Unix timestamp to datetime\n",
    "df['created_date'] = pd.to_datetime(df['created'], unit='s')\n",
    "\n",
    "# Extract useful date components\n",
    "df['year'] = df['created_date'].dt.year\n",
    "df['month'] = df['created_date'].dt.month\n",
    "df['day_of_week'] = df['created_date'].dt.day_name()\n",
    "\n",
    "print(\"Date range in dataset:\")\n",
    "print(f\"From: {df['created_date'].min()}\")\n",
    "print(f\"To: {df['created_date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "\n",
    "# Exploratory Data Analysis\n",
    "\n",
    "Now let's explore patterns in the data using pandas grouping and aggregation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 3: Score Analysis\n",
    "\n",
    "Analyze post popularity by:\n",
    "1. Finding the top 10 posts by score\n",
    "2. Calculating the average score by year\n",
    "3. Determining which day of the week gets the highest average scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment Engagement Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the relationship between text length and engagement\n",
    "correlation = df[['text_length', 'score', 'num_comments']].corr()\n",
    "print(\"Correlation matrix:\")\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîî **Question**: What does the correlation tell us about the relationship between post length and engagement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 4: Engagement Categories\n",
    "\n",
    "Create engagement categories and analyze them:\n",
    "1. Create a new column `engagement_level` with categories:\n",
    "   - 'Low': score < 100\n",
    "   - 'Medium': score 100-500\n",
    "   - 'High': score 500-2000\n",
    "   - 'Viral': score > 2000\n",
    "2. Calculate the percentage of posts in each category\n",
    "3. Find the average text length for each engagement level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "\n",
    "# Text Analysis Fundamentals\n",
    "\n",
    "Let's do some basic text analysis to understand the content patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 5: Text Pattern Analysis\n",
    "\n",
    "Analyze text patterns by:\n",
    "1. Finding posts that contain the word \"family\" (case-insensitive)\n",
    "2. Counting how many posts mention \"wedding\" or \"marriage\"\n",
    "3. Creating a column indicating whether the post is about relationships (contains words like \"boyfriend\", \"girlfriend\", \"husband\", \"wife\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° **Tip**: Use the `.str.contains()` method with pandas to search for text patterns. The `case=False` parameter makes the search case-insensitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze posting patterns by author\n",
    "author_stats = df['author'].value_counts().head(10)\n",
    "print(\"Top 10 most active authors:\")\n",
    "print(author_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 6: Final Analysis\n",
    "\n",
    "Combine multiple pandas operations to answer this question:\n",
    "**\"What are the characteristics of the most engaging posts about relationships?\"**\n",
    "\n",
    "Create an analysis that:\n",
    "1. Filters for relationship-related posts\n",
    "2. Groups them by engagement level\n",
    "3. Calculates average text length, comment count, and any other relevant metrics\n",
    "4. Presents a clear summary of your findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Warning**: When working with text data, always be mindful of missing values and different text encodings that might cause unexpected results."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "<a id='section5'></a>\n\n# Working with LLM-Generated Code\n\nNow let's explore how Large Language Models can assist with coding tasks. We'll generate some code using an LLM, test it, and discover common pitfalls and best practices.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Generating Code with ChatGPT\n\nLet's ask ChatGPT to help us create a function that analyzes sentiment patterns in our AITA dataset. Here's the prompt we'll use:\n\n**Prompt to ChatGPT:**\n*\"Write a Python function that takes a pandas DataFrame with a 'selftext' column and creates a simple sentiment analysis. The function should count positive and negative words using predefined word lists, calculate a sentiment score for each post, and return a new DataFrame with sentiment columns added.\"*",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## ü§ñ AI Generated Code\n\nBelow is the code generated by ChatGPT. Let's run it and see what happens:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Generated by ChatGPT - Let's test this code!\ndef analyze_sentiment(df):\n    \"\"\"\n    Analyze sentiment of posts in a DataFrame.\n    \n    Args:\n        df: pandas DataFrame with 'selftext' column\n    \n    Returns:\n        DataFrame with added sentiment columns\n    \"\"\"\n    # Define positive and negative words\n    positive_words = ['good', 'great', 'awesome', 'excellent', 'fantastic', 'wonderful', \n                     'amazing', 'perfect', 'best', 'love', 'happy', 'joy', 'pleased']\n    \n    negative_words = ['bad', 'terrible', 'awful', 'horrible', 'worst', 'hate', 'angry', \n                     'sad', 'upset', 'mad', 'furious', 'disgusting', 'annoying']\n    \n    # Create a copy to avoid modifying original\n    result_df = df.copy()\n    \n    # Initialize sentiment columns\n    result_df['positive_count'] = 0\n    result_df['negative_count'] = 0\n    result_df['sentiment_score'] = 0.0\n    \n    # Process each row\n    for idx, row in result_df.iterrows():\n        text = row['selftext'].lower()\n        \n        # Count positive words\n        pos_count = sum(1 for word in positive_words if word in text)\n        neg_count = sum(1 for word in negative_words if word in text)\n        \n        # Calculate sentiment score\n        total_words = len(text.split())\n        sentiment_score = (pos_count - neg_count) / total_words\n        \n        # Update DataFrame\n        result_df.loc[idx, 'positive_count'] = pos_count\n        result_df.loc[idx, 'negative_count'] = neg_count\n        result_df.loc[idx, 'sentiment_score'] = sentiment_score\n    \n    return result_df\n\n# Test the function\nprint(\"Testing AI-generated sentiment analysis function...\")\nsentiment_df = analyze_sentiment(df)\nprint(\"Function completed successfully!\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "üîî **Question**: Did the code run successfully? If you got an error, what do you think went wrong?",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Debugging the AI Code\n\nLet's investigate what went wrong and fix the issues step by step:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Let's check what's in our dataset first\nprint(\"Checking for missing selftext values:\")\nprint(f\"Missing values: {df['selftext'].isna().sum()}\")\nprint(f\"Total rows: {len(df)}\")\n\n# Look at a few selftext examples\nprint(\"\\nFirst few selftext values:\")\nfor i in range(3):\n    print(f\"Row {i}: {repr(df.iloc[i]['selftext'])}\")\n    print(f\"Type: {type(df.iloc[i]['selftext'])}\")\n    print(\"---\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## ü•ä Challenge 7: Fix the AI Code\n\nThe AI-generated code has several issues. Can you identify and fix them?\n\n**Issues to look for:**\n1. What happens if `selftext` contains missing values (NaN)?\n2. What happens if `selftext` is not a string?\n3. Are there performance issues with this approach?\n4. Are there edge cases in the sentiment calculation?\n\nWrite an improved version of the function below:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# YOUR CODE HERE - Fix the AI-generated function\n",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## A Better Implementation\n\nHere's an improved version that handles the edge cases:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def analyze_sentiment_improved(df):\n    \"\"\"\n    Improved sentiment analysis function that handles edge cases.\n    \n    Args:\n        df: pandas DataFrame with 'selftext' column\n    \n    Returns:\n        DataFrame with added sentiment columns\n    \"\"\"\n    # Define positive and negative words\n    positive_words = set(['good', 'great', 'awesome', 'excellent', 'fantastic', 'wonderful', \n                         'amazing', 'perfect', 'best', 'love', 'happy', 'joy', 'pleased'])\n    \n    negative_words = set(['bad', 'terrible', 'awful', 'horrible', 'worst', 'hate', 'angry', \n                         'sad', 'upset', 'mad', 'furious', 'disgusting', 'annoying'])\n    \n    # Create a copy to avoid modifying original\n    result_df = df.copy()\n    \n    # Handle missing values first\n    result_df['selftext'] = result_df['selftext'].fillna('')\n    \n    # Convert to string type to handle any non-string values\n    result_df['selftext'] = result_df['selftext'].astype(str)\n    \n    # Vectorized approach using pandas string methods\n    # Convert to lowercase\n    text_lower = result_df['selftext'].str.lower()\n    \n    # Count positive words\n    pos_pattern = '|'.join(positive_words)\n    result_df['positive_count'] = text_lower.str.count(pos_pattern)\n    \n    # Count negative words  \n    neg_pattern = '|'.join(negative_words)\n    result_df['negative_count'] = text_lower.str.count(neg_pattern)\n    \n    # Calculate word count (handle empty strings)\n    word_counts = text_lower.str.split().str.len().fillna(0)\n    \n    # Calculate sentiment score (avoid division by zero)\n    sentiment_numerator = result_df['positive_count'] - result_df['negative_count']\n    result_df['sentiment_score'] = np.where(\n        word_counts > 0, \n        sentiment_numerator / word_counts, \n        0\n    )\n    \n    return result_df\n\n# Test the improved function\nprint(\"Testing improved sentiment analysis function...\")\ntry:\n    sentiment_df_improved = analyze_sentiment_improved(df)\n    print(\"Function completed successfully!\")\n    print(f\"Added columns: {['positive_count', 'negative_count', 'sentiment_score']}\")\n    \n    # Show some results\n    print(\"\\nSample results:\")\n    print(sentiment_df_improved[['positive_count', 'negative_count', 'sentiment_score']].head())\n    \nexcept Exception as e:\n    print(f\"Error occurred: {e}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Performance Comparison\n\nLet's compare the performance of the original AI code vs our improved version:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import time\n\n# Create a small test dataset for timing\ntest_df = df.head(50).copy()\n\n# Test original function (if it works)\nprint(\"Testing original AI function performance...\")\ntry:\n    start_time = time.time()\n    result1 = analyze_sentiment(test_df)\n    original_time = time.time() - start_time\n    print(f\"Original function took: {original_time:.4f} seconds\")\nexcept Exception as e:\n    print(f\"Original function failed: {e}\")\n    original_time = None\n\n# Test improved function\nprint(\"\\nTesting improved function performance...\")\nstart_time = time.time()\nresult2 = analyze_sentiment_improved(test_df)\nimproved_time = time.time() - start_time\nprint(f\"Improved function took: {improved_time:.4f} seconds\")\n\nif original_time:\n    speedup = original_time / improved_time\n    print(f\"\\nSpeedup: {speedup:.2f}x faster\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## LLM Coding Guidelines\n\nBased on our experience with the AI-generated code, let's establish some guidelines for working with LLMs:\n\n### ‚úÖ **DO:**\n- **Provide clear context** and specify desired output format\n- **Test the code immediately** after generation  \n- **Check for edge cases** like missing values, empty strings, wrong data types\n- **Verify performance** - AI often uses inefficient approaches\n- **Document AI assistance** in comments (e.g., `# Generated with ChatGPT assistance`)\n- **Understand the code** before using it in your projects\n- **Ask for explanations** if you don't understand parts of the generated code\n\n### ‚ùå **DON'T:**\n- **Ask for too much at once** - break complex tasks into smaller parts\n- **Blindly copy-paste** without understanding the code\n- **Skip testing** - always run and verify the output\n- **Ignore error handling** - AI often misses edge cases\n- **Forget to document** AI usage (academic integrity requirement)\n- **Use AI output** that leads to plagiarism or incorrect work",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Course Policy on AI Use\n\n### üìã **Academic Integrity Guidelines**\n\n**You MAY use LLMs as coding assistants IF:**\n- You **document their use** clearly (in code comments or assignment submissions)\n- You **personally verify and understand** the solution\n- You can **explain how the code works** when asked\n- You **test the code thoroughly** and fix any bugs\n\n**Examples of acceptable documentation:**\n```python\n# Used ChatGPT to help write this sentiment analysis function\n# Modified the original output to handle edge cases\ndef my_function():\n    pass\n```\n\n**You MAY NOT:**\n- Use AI assistance **without acknowledgment** \n- Submit AI-generated code that you **don't understand**\n- Use AI output that leads to **plagiarism or incorrect work**\n- Claim AI-generated work as **entirely your own**\n\n‚ö†Ô∏è **Remember**: Understanding the code is more important than having perfect code. Using LLMs can speed up development, but only if you comprehend what they produce!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## ü•ä Challenge 8: Evaluate AI Output\n\nNow it's your turn! Ask ChatGPT (or another LLM) to generate code for one of these tasks:\n\n1. **Create a function** that finds the most common words in AITA post titles\n2. **Generate code** to create a simple visualization of post scores over time  \n3. **Write a function** that categorizes posts by topic based on keywords\n\n**Instructions:**\n1. Copy your prompt and the AI's response into the cells below\n2. Test the generated code\n3. Document any bugs or improvements needed\n4. Fix the issues and explain what you learned",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "**Your prompt to the LLM:**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# YOUR AI-GENERATED CODE HERE\n# Remember to add a comment acknowledging AI assistance!\n",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Issues found and fixes made:**",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# YOUR IMPROVED VERSION HERE",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "<div class=\"alert alert-success\">\n\n## ‚ùó Key Points\n\n* Pandas provides powerful tools for loading, cleaning, and exploring real-world datasets.\n* Always start data analysis by understanding your dataset structure and checking for data quality issues.\n* The `.groupby()` method is essential for aggregating data and finding patterns across categories.\n* Text data requires special handling, including case-insensitive searches and pattern matching.\n* Correlation analysis helps identify relationships between numerical variables.\n* Creating categorical variables from continuous data enables different types of analysis.\n* **LLMs can accelerate coding but require careful testing and understanding of generated code.**\n* **Always document AI assistance and verify that generated code handles edge cases properly.**\n\n</div>",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## ‚ùó Key Points\n",
    "\n",
    "* Pandas provides powerful tools for loading, cleaning, and exploring real-world datasets.\n",
    "* Always start data analysis by understanding your dataset structure and checking for data quality issues.\n",
    "* The `.groupby()` method is essential for aggregating data and finding patterns across categories.\n",
    "* Text data requires special handling, including case-insensitive searches and pattern matching.\n",
    "* Correlation analysis helps identify relationships between numerical variables.\n",
    "* Creating categorical variables from continuous data enables different types of analysis.\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}