{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Text Analysis Fundamentals: Solutions\n",
    "\n",
    "This notebook contains solutions for both text preprocessing and bag-of-words representation challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Text Preprocessing Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Use pandas to import Tweets\n",
    "csv_path = '../../../data/airline_tweets.csv'\n",
    "tweets = pd.read_csv(csv_path, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¥Š Challenge 1: Preprocessing with Multiple Steps\n",
    "\n",
    "So far we've learned a few preprocessing operations, let's put them together in a function! This function would be a handy one to refer to if you happen to work with some messy English text data, and you want to preprocess it with a single function. \n",
    "\n",
    "The example text data for challenge 1 has been read in. Write a function to:\n",
    "- Lowercase the text\n",
    "- Remove punctuation marks\n",
    "- Remove extra whitespace characters\n",
    "\n",
    "Feel free to recycle the codes we've used above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge1_path = '../../../data/example1.txt'\n",
    "\n",
    "with open(challenge1_path, 'r') as file:\n",
    "    challenge1 = file.read()\n",
    "    \n",
    "print(challenge1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def remove_punct(text):\n",
    "    '''Remove punctuation marks in input text'''\n",
    "    \n",
    "    # Select characters not in puncutaion\n",
    "    no_punct = []\n",
    "    for char in text:\n",
    "        if char not in punctuation:\n",
    "            no_punct.append(char)\n",
    "\n",
    "    # Join the characters into a string\n",
    "    text_no_punct = ''.join(no_punct)   \n",
    "    \n",
    "    return text_no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a pattern in regex\n",
    "blankspace_pattern = r'\\s+'\n",
    "\n",
    "# Write a replacement for the pattern identfied\n",
    "blankspace_repl = ' '\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    # Step 1: Lowercase the input text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Step 2: Use remove_punct to remove puncutuation marks\n",
    "    text = remove_punct(text)\n",
    "\n",
    "    # Step 3: Remove extra whitespace characters\n",
    "    text = re.sub(blankspace_pattern, blankspace_repl, text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "    \n",
    "clean_text(challenge1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¥Š Challenge 2: Remove Stop Words\n",
    "\n",
    "We have known how `nltk` and `spaCy` work as NLP packages. We've also demostrated how to identify stop words with each package. \n",
    "\n",
    "Let's write **two** functions to remove stop words from our text data. \n",
    "\n",
    "- Complete the function for stop words removal using `nltk`\n",
    "    - The starter code requires two arguments: the raw text input and a list of predefined stop words\n",
    "- Complete the function for stop words removal using `spaCy`\n",
    "    - The starter code requires one argument: the raw text input\n",
    " \n",
    "A friendly reminder before we dive in: both functions take raw text as inputâ€”that's a signal to perform tokenization on the raw text first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "def remove_stopword_nltk(raw_text, stopword):\n",
    "    \n",
    "    # Step 1: Tokenization with nltk\n",
    "    tokens = word_tokenize(raw_text)\n",
    "    \n",
    "    # Step 2: Filter out tokens in the stop word list\n",
    "    text = [token for token in tokens if token not in stopword]\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def remove_stopword_spacy(raw_text):\n",
    "\n",
    "    # Step 1: Apply the nlp pipeline\n",
    "    doc = nlp(raw_text)\n",
    "    \n",
    "    # Step 2: Filter out tokens in the stop word list\n",
    "    text = [token.text for token in doc if token.is_stop is False]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tweets['text'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopword_nltk(text, stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopword_spacy(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¥Š Challenge 3: Find the Word Boundary\n",
    "\n",
    "Now that we know BERT tokenization would often return subwords. Let's try a few more examples! \n",
    "\n",
    "Does the result make sense to you? What do you think is the correct word boundary to split the following words into subwords? \n",
    "\n",
    "Also feel free to read more about limitations of the WordPiece algorithm. For instance, [this blog post](https://medium.com/@rickbattle/weaknesses-of-wordpiece-tokenization-eb20e37fec99) dives into why would it fail, and [this one](https://tinkerd.net/blog/machine-learning/bert-tokenization/#demo-bert-tokenizer) introduces the mechanism underlying the algoritm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT tokenizer in\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize the tokenizer \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(string):\n",
    "    '''Tokenzie the input string with BERT'''\n",
    "    tokens = tokenizer.tokenize(string)\n",
    "    return print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abbreviations\n",
    "get_tokens('dlab')\n",
    "\n",
    "# OOV\n",
    "get_tokens('covid')\n",
    "\n",
    "# Prefix\n",
    "get_tokens('huggable')\n",
    "\n",
    "# Digits\n",
    "get_tokens('378')\n",
    "\n",
    "# YOUR EXAMPLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Bag-of-Words Representation Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to import tweets\n",
    "tweets_path = '../../../data/airline_tweets.csv'\n",
    "tweets = pd.read_csv(tweets_path, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¥Š Challenge 4: Apply a Text Cleaning Pipeline\n",
    "\n",
    "Write a function called `preprocess` that performs the following steps on a text input:\n",
    "\n",
    "* Step 1: Lowercase the text input.\n",
    "* Step 2: Replace the following patterns with placeholders:\n",
    "    * URLs &rarr; ` URL `\n",
    "    * Digits &rarr; ` DIGIT `\n",
    "    * Hashtags &rarr; ` HASHTAG `\n",
    "    * Tweet handles &rarr; ` USER `\n",
    "* Step 3: Remove extra blankspace.\n",
    "\n",
    "Here are some hints to guide you through this challenge:\n",
    "\n",
    "* For Step 1, recall from Part 1 that a string method called [`.lower()`](https://docs.python.org/3.11/library/stdtypes.html#str.lower) can be usd to convert text to lowercase. \n",
    "* We have integrated Step 2 into a function called `placeholder`. Run the cell below to import it into your notebook, and you can use it just like any other functions.\n",
    "* For Step 3, we have provided the regex pattern for identifying whitespace characters as well as the correct replacement for extract whitespace. \n",
    "\n",
    "Run your `preprocess` function on `example_tweet` (three cells below) to check if it works. If it does, apply it to the entire `text` column in the tweets dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blankspace_pattern = r'\\s+'\n",
    "blankspace_repl = ' '\n",
    "\n",
    "def preprocess(text):\n",
    "    '''Create a preprocess pipeline that cleans the tweet data.'''\n",
    "\n",
    "    # Step 1: Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Step 2: Replace patterns with placeholders\n",
    "    text = placeholder(text)\n",
    "\n",
    "    # Step 3: Remove extra whitespace characters\n",
    "    text = re.sub(blankspace_pattern, blankspace_repl, text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tweet = 'lol @justinbeiber and @BillGates are like soo 2000 #yesterday #amiright saw it on https://twitter.com #yolo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the example tweet\n",
    "print(example_tweet)\n",
    "print(f\"{'='*50}\")\n",
    "print(preprocess(example_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the text column and assign the preprocessed tweets to a new column\n",
    "tweets['text_processed'] = tweets['text'].apply(lambda x: preprocess(x))\n",
    "tweets['text_processed'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¥Š Challenge 5: Lemmatize the Text Input\n",
    "\n",
    "Recall from Part 1 that we introduced using `spaCy` to perform lemmatization, i.e., to \"recover\" the base form of a word. This process will reduce vocabulary size by keeping word variations minimalâ€”a smaller vocabularly may help improve model performance in sentiment classification. \n",
    "\n",
    "Now let's implement lemmatization on our tweet data and use the lemmatized text to create a third DTM. \n",
    "\n",
    "Complete the function `lemmatize_text`. It requires a text input and returns the lemmas of all tokens. \n",
    "\n",
    "Here are some hints to guide you through this challenge:\n",
    "\n",
    "- Step 1: initialize a list to hold lemmas\n",
    "- Step 2: apply the `nlp` pipeline to the input text\n",
    "- Step 3: iterate over tokens in the processed text and retrieve the lemma of the token\n",
    "    - HINT: lemmatization is one of the linguistic annotations that the `nlp` pipeline automatically does for us. We can use `token.lemma_` to access the annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to lemmatize text\n",
    "def lemmatize_text(text):\n",
    "    '''Lemmatize the text input with spaCy annotations.'''\n",
    "\n",
    "    # Step 1: Initialize an empty list to hold lemmas\n",
    "    lemma = []\n",
    "\n",
    "    # Step 2: Apply the nlp pipeline to input text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Step 3: Iterate over tokens in the text to get the token lemma\n",
    "    for token in doc:\n",
    "        lemma.append(token.lemma_)\n",
    "\n",
    "    # Step 4: Join lemmas together into a single string\n",
    "    text_lemma = ' '.join(lemma)\n",
    "    \n",
    "    return text_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to an example tweet\n",
    "print(tweets.iloc[101][\"text_processed\"])\n",
    "print(f\"{'='*50}\")\n",
    "print(lemmatize_text(tweets.iloc[101]['text_processed']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a while!\n",
    "tweets['text_lemmatized'] = tweets['text_processed'].apply(lambda x: lemmatize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the preprocessed tweet\n",
    "print(tweets['text_processed'].iloc[101])\n",
    "print(f\"{'='*50}\")\n",
    "# Print the lemmatized tweet\n",
    "print(tweets['text_lemmatized'].iloc[101])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¥Š Challenge 6: Words with Highest Mean TF-IDF scores\n",
    "\n",
    "We have obtained tf-idf values for each term in each document. But what do these values tell us about the sentiments of tweets? Are there any words that are  particularly informative for positive/negative tweets? \n",
    "\n",
    "To explore this, let's gather the indices of all positive/negative tweets and calculate the mean tf-idf scores of words appear in each category. \n",
    "\n",
    "We've provided the following starter code to guide you:\n",
    "- Subset the `tweets` dataframe according to the `airline_sentiment` label and retrieve the index of each subset (`.index`). Assign the index to `positive_index` or `negative_index`.\n",
    "- For each subset:\n",
    "    - Retrieve the td-idf representation \n",
    "    - Take the mean tf-idf values across the subset using `.mean()`\n",
    "    - Sort the mean values in the descending order using `.sort_values()`\n",
    "    - Get the top 10 terms using `.head()`\n",
    "\n",
    "Next, run `pos.plot` and `neg.plot` to plot the words with the highest mean tf-idf scores for each subset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tfidf vectorizer\n",
    "vectorizer = TfidfVectorizer(lowercase=True,\n",
    "                             stop_words='english',\n",
    "                             min_df=2,\n",
    "                             max_df=0.95,\n",
    "                             max_features=None)\n",
    "\n",
    "# Fit and transform \n",
    "tf_dtm = vectorizer.fit_transform(tweets['text_lemmatized'])\n",
    "\n",
    "# Create a tf-idf dataframe\n",
    "tfidf = pd.DataFrame(tf_dtm.todense(),\n",
    "                     columns=vectorizer.get_feature_names_out(),\n",
    "                     index=tweets.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the boolean masks \n",
    "positive_index = tweets[tweets['airline_sentiment'] == 'positive'].index\n",
    "negative_index = tweets[tweets['airline_sentiment'] == 'negative'].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the following two lines\n",
    "pos = tfidf.loc[positive_index].mean().sort_values(ascending=False).head(10)\n",
    "neg = tfidf.loc[negative_index].mean().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos.plot(kind='barh', \n",
    "         xlim=(0, 0.18),\n",
    "         color='cornflowerblue',\n",
    "         title='Top 10 terms with the highest mean tf-idf values for positive tweets');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg.plot(kind='barh', \n",
    "         xlim=(0, 0.18),\n",
    "         color='darksalmon',\n",
    "         title='Top 10 terms with the highest mean tf-idf values for negative tweets');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}