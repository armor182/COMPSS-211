{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Demo: Working with Language Models in Google Colab\n",
    "\n",
    "* * * \n",
    "\n",
    "<div class=\"alert alert-success\">  \n",
    "    \n",
    "### Learning Objectives \n",
    "    \n",
    "* Set up Google Colab for GPU-accelerated language model work\n",
    "* Download and run a small language model (Gemma 2B) from Hugging Face\n",
    "* Understand key concepts: tokenization, probability distributions, temperature effects\n",
    "* Demonstrate controlled text generation for computational social science applications\n",
    "* Learn practical considerations for model deployment and cost management\n",
    "\n",
    "</div>\n",
    "\n",
    "### Icons Used in This Notebook\n",
    "üîî **Question**: A quick question to help you understand what's going on.<br>\n",
    "ü•ä **Challenge**: Interactive exercise. We'll work through these in the workshop!<br>\n",
    "üí° **Tip**: How to do something a bit more efficiently or effectively.<br>\n",
    "‚ö†Ô∏è **Warning:** Heads-up about tricky stuff or common mistakes.<br>\n",
    "üé¨ **Demo**: Showing off something more advanced ‚Äì so you know what Python can be used for!<br>\n",
    "\n",
    "### Sections\n",
    "1. [Colab Setup and GPU Configuration](#setup)\n",
    "2. [Installing Requirements and Loading Models](#install)\n",
    "3. [Basic Text Generation](#generation)\n",
    "4. [Understanding Tokenization](#tokenization)\n",
    "5. [Probability Distributions and Temperature](#probability)\n",
    "6. [Controlled Generation for Social Science](#controlled)\n",
    "7. [Practical Considerations](#practical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "\n",
    "# Google Colab Setup and GPU Configuration\n",
    "\n",
    "**Why Google Colab for Language Models?**\n",
    "\n",
    "In computational social science, we often need to choose the right tool for the task. Today we're switching to Google Colab because:\n",
    "\n",
    "- **GPU Access**: Language models require significant computational power\n",
    "- **Memory**: Modern LLMs need substantial RAM (8GB+ for small models)\n",
    "- **Ease of Setup**: No local installation headaches\n",
    "- **Cost**: Free tier provides reasonable access for experimentation\n",
    "- **Collaboration**: Easy sharing and reproducibility\n",
    "\n",
    "This is a common pattern in computational social science - matching computational resources to research needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé¨ Demo: Colab Orientation (5 minutes)\n",
    "\n",
    "**Step 1: Enable GPU Runtime**\n",
    "1. Go to `Runtime` ‚Üí `Change runtime type`\n",
    "2. Set `Hardware accelerator` to `GPU`\n",
    "3. Choose `T4 GPU` (free tier) or `V100` (if available)\n",
    "4. Click `Save`\n",
    "\n",
    "**Step 2: Verify GPU Access**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Check your runtime settings!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Check System Resources**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check system resources\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "print(f\"CPU cores: {psutil.cpu_count()}\")\n",
    "print(f\"RAM: {psutil.virtual_memory().total / 1e9:.1f} GB\")\n",
    "print(f\"Available RAM: {psutil.virtual_memory().available / 1e9:.1f} GB\")\n",
    "print(f\"Disk space: {psutil.disk_usage('/').total / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='install'></a>\n",
    "\n",
    "# Installing Requirements and Loading Models\n",
    "\n",
    "üí° **Tip**: In Colab, package installations persist only for the current session. Each time you restart, you'll need to reinstall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers accelerate torch\n",
    "!pip install matplotlib seaborn pandas numpy\n",
    "!pip install gradio  # For interactive demos later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Gemma 2B Model from Hugging Face\n",
    "\n",
    "We'll use **Gemma 2B**, a small but capable language model from Google. It's perfect for educational purposes because:\n",
    "- Small enough to run on free Colab GPUs\n",
    "- Modern architecture (based on Transformer)\n",
    "- Good performance for its size\n",
    "- Well-documented and actively maintained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Model selection - using Gemma 2B Instruct for better instruction following\n",
    "model_name = \"google/gemma-2b-it\"  # \"it\" stands for \"instruction tuned\"\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "print(\"This may take a few minutes on first run...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load model with appropriate settings for Colab\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # Use half precision to save memory\n",
    "    device_map=\"auto\",          # Automatically map to available GPU\n",
    "    trust_remote_code=True      # Required for some models\n",
    ")\n",
    "\n",
    "print(f\"‚úì Model loaded successfully!\")\n",
    "print(f\"Model device: {model.device}\")\n",
    "print(f\"Model dtype: {model.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Warning**: If you get memory errors, try these solutions:\n",
    "1. Restart runtime and try again\n",
    "2. Use a smaller model like `microsoft/DialoGPT-small`\n",
    "3. Reduce batch size in generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='generation'></a>\n",
    "\n",
    "# Basic Text Generation\n",
    "\n",
    "Now let's generate text about social issues to see how the model behaves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Text Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_length=100, temperature=0.7, num_return_sequences=1):\n",
    "    \"\"\"\n",
    "    Generate text using the loaded model\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input text to continue\n",
    "        max_length: Maximum length of generated text\n",
    "        temperature: Controls randomness (0.0 = deterministic, 1.0+ = creative)\n",
    "        num_return_sequences: Number of different completions to generate\n",
    "    \"\"\"\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode outputs\n",
    "    generated_texts = []\n",
    "    for output in outputs:\n",
    "        text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        # Remove the original prompt from the output\n",
    "        text = text[len(prompt):] if text.startswith(prompt) else text\n",
    "        generated_texts.append(text.strip())\n",
    "    \n",
    "    return generated_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Multiple Completions for Social Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Social science prompt\n",
    "prompt = \"The impact of social media on political polarization\"\n",
    "\n",
    "print(f\"Prompt: '{prompt}'\\n\")\n",
    "print(\"Generating 3 different completions...\\n\")\n",
    "\n",
    "# Generate multiple completions\n",
    "completions = generate_text(\n",
    "    prompt, \n",
    "    max_length=150, \n",
    "    temperature=0.8, \n",
    "    num_return_sequences=3\n",
    ")\n",
    "\n",
    "for i, completion in enumerate(completions, 1):\n",
    "    print(f\"Completion {i}:\")\n",
    "    print(f\"'{completion}'\\n\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîî **Question**: What do you notice about the different completions? How do they vary in content and style?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge: Try Different Social Science Prompts\n",
    "\n",
    "Experiment with different prompts related to your research interests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different social science prompts\n",
    "prompts = [\n",
    "    \"Climate change attitudes vary across demographic groups because\",\n",
    "    \"The relationship between education and voting behavior\",\n",
    "    \"Economic inequality affects social cohesion by\",\n",
    "    \"Gender representation in leadership positions\"\n",
    "]\n",
    "\n",
    "# YOUR CODE HERE: Choose a prompt and generate text\n",
    "chosen_prompt = prompts[0]  # Change this index\n",
    "\n",
    "completions = generate_text(chosen_prompt, max_length=120, temperature=0.7)\n",
    "print(f\"Prompt: {chosen_prompt}\")\n",
    "print(f\"Completion: {completions[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tokenization'></a>\n",
    "\n",
    "# Understanding Tokenization\n",
    "\n",
    "Tokenization is the process of breaking text into smaller units (tokens) that the model can process. Understanding tokenization is crucial for working with language models effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé¨ Demo: How \"Social Science\" Becomes Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how different texts get tokenized\n",
    "examples = [\n",
    "    \"social science\",\n",
    "    \"computational social science\",\n",
    "    \"artificial intelligence\",\n",
    "    \"democratization\",\n",
    "    \"anti-establishment\"\n",
    "]\n",
    "\n",
    "print(\"Tokenization Examples:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for text in examples:\n",
    "    # Tokenize the text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_ids = tokenizer.encode(text)\n",
    "    \n",
    "    print(f\"Text: '{text}'\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(f\"Token IDs: {token_ids}\")\n",
    "    print(f\"Number of tokens: {len(tokens)}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subword Tokenization\n",
    "\n",
    "Modern language models use **subword tokenization** (like BPE - Byte Pair Encoding). This approach:\n",
    "- Handles out-of-vocabulary words better\n",
    "- Creates a manageable vocabulary size\n",
    "- Balances between character-level and word-level processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the vocabulary\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(f\"Vocabulary size: {vocab_size:,} tokens\")\n",
    "\n",
    "# Special tokens\n",
    "special_tokens = tokenizer.special_tokens_map\n",
    "print(f\"\\nSpecial tokens: {special_tokens}\")\n",
    "\n",
    "# Let's see some example tokens from the vocabulary\n",
    "print(\"\\nSample vocabulary (first 20 tokens):\")\n",
    "for i in range(20):\n",
    "    token = tokenizer.decode([i])\n",
    "    print(f\"ID {i}: '{token}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Length and Text Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship between text length and token count\n",
    "texts = [\n",
    "    \"AI\",\n",
    "    \"The quick brown fox\",\n",
    "    \"Social media platforms influence political discourse\",\n",
    "    \"Computational social science combines traditional social science methods with computational tools and big data analytics\"\n",
    "]\n",
    "\n",
    "print(\"Text Length vs Token Count:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for text in texts:\n",
    "    char_count = len(text)\n",
    "    word_count = len(text.split())\n",
    "    token_count = len(tokenizer.encode(text))\n",
    "    \n",
    "    print(f\"Text: '{text}'\")\n",
    "    print(f\"Characters: {char_count}, Words: {word_count}, Tokens: {token_count}\")\n",
    "    print(f\"Tokens per word: {token_count/word_count:.2f}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='probability'></a>\n",
    "\n",
    "# Probability Distributions and Temperature\n",
    "\n",
    "Language models work by predicting probability distributions over the vocabulary for the next token. Understanding these probabilities helps us control generation behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé¨ Demo: Show Top-k Tokens and Their Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "def get_next_token_probabilities(prompt, top_k=10):\n",
    "    \"\"\"\n",
    "    Get the top-k most likely next tokens and their probabilities\n",
    "    \"\"\"\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits[0, -1, :]  # Last token predictions\n",
    "    \n",
    "    # Convert to probabilities\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    \n",
    "    # Get top-k tokens\n",
    "    top_probs, top_indices = torch.topk(probs, top_k)\n",
    "    \n",
    "    # Decode tokens\n",
    "    results = []\n",
    "    for prob, idx in zip(top_probs, top_indices):\n",
    "        token = tokenizer.decode([idx.item()])\n",
    "        results.append({\n",
    "            'token': repr(token),  # repr to show whitespace/special chars\n",
    "            'probability': prob.item(),\n",
    "            'percentage': prob.item() * 100\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example: What comes after \"Climate change is\"\n",
    "prompt = \"Climate change is\"\n",
    "top_tokens = get_next_token_probabilities(prompt, top_k=15)\n",
    "\n",
    "print(f\"Top next tokens for: '{prompt}'\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "df = pd.DataFrame(top_tokens)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature Effects on Generation\n",
    "\n",
    "**Temperature** controls the randomness of generation:\n",
    "- **Low temperature (0.1-0.3)**: More deterministic, conservative\n",
    "- **Medium temperature (0.7-1.0)**: Balanced creativity and coherence  \n",
    "- **High temperature (1.5+)**: More random, creative, potentially incoherent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate temperature effects\n",
    "prompt = \"Social media algorithms\"\n",
    "temperatures = [0.1, 0.7, 1.2]\n",
    "\n",
    "print(f\"Temperature Effects on: '{prompt}'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\nTemperature: {temp}\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    # Generate 3 samples at this temperature\n",
    "    for i in range(3):\n",
    "        completion = generate_text(prompt, max_length=80, temperature=temp, num_return_sequences=1)[0]\n",
    "        print(f\"Sample {i+1}: {completion[:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Probability Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Compare probability distributions for different prompts\n",
    "prompts = [\n",
    "    \"Democracy is\",\n",
    "    \"Artificial intelligence will\",\n",
    "    \"The research shows that\"\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i, prompt in enumerate(prompts):\n",
    "    top_tokens = get_next_token_probabilities(prompt, top_k=8)\n",
    "    \n",
    "    tokens = [item['token'].replace(\"'\", \"\").replace('\"', '') for item in top_tokens]\n",
    "    probs = [item['percentage'] for item in top_tokens]\n",
    "    \n",
    "    axes[i].bar(range(len(tokens)), probs)\n",
    "    axes[i].set_title(f\"'{prompt}'\")\n",
    "    axes[i].set_xlabel(\"Next Token\")\n",
    "    axes[i].set_ylabel(\"Probability (%)\")\n",
    "    axes[i].set_xticks(range(len(tokens)))\n",
    "    axes[i].set_xticklabels(tokens, rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='controlled'></a>\n",
    "\n",
    "# Controlled Generation for Social Science\n",
    "\n",
    "For computational social science applications, we often want to control generation to study specific phenomena or generate data with particular characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé¨ Demo: Sentiment-Controlled Generation\n",
    "\n",
    "Let's demonstrate how to bias generation toward positive or negative sentiment by manipulating the probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sentiment word lists (simplified for demo)\n",
    "positive_words = [\n",
    "    \"great\", \"excellent\", \"wonderful\", \"amazing\", \"fantastic\", \n",
    "    \"good\", \"positive\", \"beneficial\", \"helpful\", \"successful\"\n",
    "]\n",
    "\n",
    "negative_words = [\n",
    "    \"terrible\", \"awful\", \"horrible\", \"bad\", \"negative\", \n",
    "    \"harmful\", \"dangerous\", \"problematic\", \"concerning\", \"disappointing\"\n",
    "]\n",
    "\n",
    "def controlled_generation(prompt, sentiment=\"neutral\", strength=2.0, max_length=100):\n",
    "    \"\"\"\n",
    "    Generate text with sentiment control by biasing token probabilities\n",
    "    \n",
    "    Args:\n",
    "        prompt: Input text\n",
    "        sentiment: 'positive', 'negative', or 'neutral'\n",
    "        strength: How much to bias (higher = stronger bias)\n",
    "        max_length: Maximum generation length\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Get token IDs for sentiment words\n",
    "    if sentiment == \"positive\":\n",
    "        target_words = positive_words\n",
    "    elif sentiment == \"negative\":\n",
    "        target_words = negative_words\n",
    "    else:\n",
    "        target_words = []\n",
    "    \n",
    "    target_token_ids = []\n",
    "    for word in target_words:\n",
    "        # Get token IDs for each word (some words might be multiple tokens)\n",
    "        tokens = tokenizer.encode(word, add_special_tokens=False)\n",
    "        target_token_ids.extend(tokens)\n",
    "    \n",
    "    # Custom generation function with logit manipulation\n",
    "    generated = inputs['input_ids'].clone()\n",
    "    \n",
    "    for _ in range(max_length - len(generated[0])):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(generated)\n",
    "            logits = outputs.logits[0, -1, :]\n",
    "            \n",
    "            # Bias logits toward sentiment words\n",
    "            if target_token_ids:\n",
    "                for token_id in target_token_ids:\n",
    "                    if token_id < len(logits):\n",
    "                        logits[token_id] += strength\n",
    "            \n",
    "            # Sample next token\n",
    "            probs = F.softmax(logits / 0.8, dim=-1)  # Temperature = 0.8\n",
    "            next_token = torch.multinomial(probs, 1)\n",
    "            \n",
    "            # Append to generated sequence\n",
    "            generated = torch.cat([generated, next_token.unsqueeze(0)], dim=1)\n",
    "            \n",
    "            # Stop if we hit EOS token\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    # Decode result\n",
    "    result = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "    return result[len(prompt):].strip()\n",
    "\n",
    "# Test controlled generation\n",
    "base_prompt = \"The new government policy on education is\"\n",
    "\n",
    "print(f\"Base prompt: '{base_prompt}'\\n\")\n",
    "\n",
    "sentiments = [\"neutral\", \"positive\", \"negative\"]\n",
    "for sentiment in sentiments:\n",
    "    print(f\"{sentiment.upper()} generation:\")\n",
    "    completion = controlled_generation(base_prompt, sentiment=sentiment, strength=3.0)\n",
    "    print(f\"{completion}\\n\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Challenge: Create a Bias Detection Tool\n",
    "\n",
    "Create a function to detect potential biases in model outputs by generating multiple completions and analyzing patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_detection(prompt_template, num_samples=10):\n",
    "    \"\"\"\n",
    "    Generate multiple completions to detect systematic biases\n",
    "    \n",
    "    Args:\n",
    "        prompt_template: Template with {} for substitution\n",
    "        num_samples: Number of completions to generate\n",
    "    \"\"\"\n",
    "    # Test different demographic groups\n",
    "    groups = [\"men\", \"women\", \"young people\", \"older adults\", \"students\", \"professionals\"]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for group in groups:\n",
    "        prompt = prompt_template.format(group)\n",
    "        completions = []\n",
    "        \n",
    "        for _ in range(num_samples):\n",
    "            completion = generate_text(prompt, max_length=80, temperature=0.8)[0]\n",
    "            completions.append(completion)\n",
    "        \n",
    "        results[group] = completions\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example: Test for potential bias in job-related statements\n",
    "prompt_template = \"{} are typically good at\"\n",
    "\n",
    "print(f\"Testing prompt template: '{prompt_template}'\\n\")\n",
    "print(\"Generating 5 samples per group...\\n\")\n",
    "\n",
    "bias_results = bias_detection(prompt_template, num_samples=5)\n",
    "\n",
    "for group, completions in bias_results.items():\n",
    "    print(f\"Group: {group}\")\n",
    "    for i, completion in enumerate(completions, 1):\n",
    "        print(f\"  {i}. {completion[:60]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîî **Question**: What patterns do you notice in the completions for different groups? Are there any concerning biases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='practical'></a>\n",
    "\n",
    "# Practical Considerations\n",
    "\n",
    "When working with language models in computational social science research, several practical considerations are crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Issues and Solutions\n",
    "\n",
    "### GPU Memory Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current GPU memory usage\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated() / 1e9\n",
    "    cached = torch.cuda.memory_reserved() / 1e9\n",
    "    total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    \n",
    "    print(f\"GPU Memory Status:\")\n",
    "    print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"  Cached: {cached:.2f} GB\")\n",
    "    print(f\"  Total: {total:.2f} GB\")\n",
    "    print(f\"  Available: {total - cached:.2f} GB\")\n",
    "    \n",
    "    if cached > total * 0.8:\n",
    "        print(\"‚ö†Ô∏è Warning: GPU memory usage is high!\")\n",
    "        print(\"Consider: reducing batch size, using smaller models, or clearing cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Management Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory management functions\n",
    "def clear_gpu_cache():\n",
    "    \"\"\"Clear GPU cache to free memory\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"‚úì GPU cache cleared\")\n",
    "\n",
    "def get_model_size(model):\n",
    "    \"\"\"Calculate model size in parameters\"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Model size:\")\n",
    "    print(f\"  Total parameters: {total_params:,}\")\n",
    "    print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"  Size (approx): {total_params * 2 / 1e9:.2f} GB\")  # fp16 = 2 bytes per param\n",
    "\n",
    "get_model_size(model)\n",
    "clear_gpu_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loading Errors and Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test different models with error handling\n",
    "def test_model_loading(model_names):\n",
    "    \"\"\"Test loading different models and report what works\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for model_name in model_names:\n",
    "        try:\n",
    "            print(f\"Testing {model_name}...\")\n",
    "            test_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            test_model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name, \n",
    "                torch_dtype=torch.float16,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "            \n",
    "            # Test generation\n",
    "            inputs = test_tokenizer(\"Hello\", return_tensors=\"pt\").to(test_model.device)\n",
    "            outputs = test_model.generate(**inputs, max_length=20)\n",
    "            \n",
    "            results.append((model_name, \"‚úì Success\", \"\"))\n",
    "            \n",
    "            # Clean up\n",
    "            del test_model, test_tokenizer\n",
    "            clear_gpu_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            results.append((model_name, \"‚úó Failed\", str(e)[:100]))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Alternative models to try if Gemma doesn't work\n",
    "alternative_models = [\n",
    "    \"microsoft/DialoGPT-small\",    # Very small, should work on most systems\n",
    "    \"gpt2\",                       # Classic GPT-2, reliable\n",
    "    \"distilgpt2\"                  # Smaller version of GPT-2\n",
    "]\n",
    "\n",
    "print(\"If you're having issues with Gemma, try these alternatives:\")\n",
    "print(\"\\nAlternative Models:\")\n",
    "for model_name in alternative_models:\n",
    "    print(f\"  - {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rate Limiting and API Considerations\n",
    "\n",
    "When using hosted models (like OpenAI's API), you need to consider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "def rate_limited_generation(prompts, delay_range=(1, 3)):\n",
    "    \"\"\"\n",
    "    Generate text for multiple prompts with rate limiting\n",
    "    \n",
    "    Args:\n",
    "        prompts: List of prompts to process\n",
    "        delay_range: (min, max) seconds to wait between requests\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, prompt in enumerate(prompts):\n",
    "        print(f\"Processing prompt {i+1}/{len(prompts)}: {prompt[:50]}...\")\n",
    "        \n",
    "        try:\n",
    "            # Generate text\n",
    "            completion = generate_text(prompt, max_length=100, temperature=0.7)[0]\n",
    "            results.append({\n",
    "                'prompt': prompt,\n",
    "                'completion': completion,\n",
    "                'status': 'success'\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                'prompt': prompt,\n",
    "                'completion': None,\n",
    "                'status': f'error: {str(e)[:100]}'\n",
    "            })\n",
    "        \n",
    "        # Add delay between requests (except for last one)\n",
    "        if i < len(prompts) - 1:\n",
    "            delay = random.uniform(*delay_range)\n",
    "            print(f\"Waiting {delay:.1f} seconds...\")\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "test_prompts = [\n",
    "    \"The impact of technology on democracy\",\n",
    "    \"Social inequality in modern societies\",\n",
    "    \"Climate change policy effectiveness\"\n",
    "]\n",
    "\n",
    "print(\"Demo: Rate-limited batch processing\")\n",
    "batch_results = rate_limited_generation(test_prompts, delay_range=(0.5, 1.0))\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "for result in batch_results:\n",
    "    print(f\"Status: {result['status']}\")\n",
    "    if result['completion']:\n",
    "        print(f\"Output: {result['completion'][:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use Local vs Cloud Computing\n",
    "\n",
    "### Local Computing (Your Computer)\n",
    "**Pros:**\n",
    "- Full control over data and processing\n",
    "- No usage limits or costs\n",
    "- Good for small models and development\n",
    "\n",
    "**Cons:**\n",
    "- Limited computational resources\n",
    "- Requires good hardware for larger models\n",
    "- Setup and maintenance overhead\n",
    "\n",
    "### Cloud Computing (Colab, AWS, etc.)\n",
    "**Pros:**\n",
    "- Access to powerful GPUs\n",
    "- Scalable resources\n",
    "- No hardware investment\n",
    "\n",
    "**Cons:**\n",
    "- Usage limits and costs\n",
    "- Data privacy considerations\n",
    "- Internet dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_costs(num_prompts, avg_tokens_per_prompt=100, tokens_per_second=50):\n",
    "    \"\"\"\n",
    "    Estimate computational costs for a research project\n",
    "    \"\"\"\n",
    "    total_tokens = num_prompts * avg_tokens_per_prompt\n",
    "    processing_time = total_tokens / tokens_per_second\n",
    "    \n",
    "    # Rough cost estimates (as of 2024)\n",
    "    colab_pro_cost = 0.0017 * (processing_time / 3600)  # $0.0017/hour for Colab Pro\n",
    "    openai_gpt4_cost = total_tokens * 0.00003  # $0.03 per 1K tokens for GPT-4\n",
    "    \n",
    "    print(f\"Cost Estimation for Research Project:\")\n",
    "    print(f\"  Number of prompts: {num_prompts:,}\")\n",
    "    print(f\"  Total tokens: {total_tokens:,}\")\n",
    "    print(f\"  Processing time: {processing_time/3600:.2f} hours\")\n",
    "    print(f\"\\nEstimated Costs:\")\n",
    "    print(f\"  Local/Colab Free: $0 (if within limits)\")\n",
    "    print(f\"  Colab Pro: ${colab_pro_cost:.2f}\")\n",
    "    print(f\"  OpenAI GPT-4: ${openai_gpt4_cost:.2f}\")\n",
    "    print(f\"\\nRecommendation:\")\n",
    "    if num_prompts < 100:\n",
    "        print(\"  Use free Colab for small experiments\")\n",
    "    elif num_prompts < 1000:\n",
    "        print(\"  Consider Colab Pro or local setup\")\n",
    "    else:\n",
    "        print(\"  Budget for cloud computing or invest in local hardware\")\n",
    "\n",
    "# Example cost estimation\n",
    "estimate_costs(500, avg_tokens_per_prompt=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Size Trade-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison table\n",
    "model_comparison = {\n",
    "    \"Model\": [\"DistilGPT-2\", \"GPT-2\", \"Gemma-2B\", \"Gemma-7B\", \"GPT-3.5\", \"GPT-4\"],\n",
    "    \"Parameters\": [\"82M\", \"117M\", \"2B\", \"7B\", \"175B\", \"1T+\"],\n",
    "    \"Memory (GB)\": [\"0.3\", \"0.5\", \"4\", \"14\", \"350+\", \"?\"],\n",
    "    \"Quality\": [\"Basic\", \"Good\", \"Very Good\", \"Excellent\", \"Excellent\", \"State-of-art\"],\n",
    "    \"Speed\": [\"Fast\", \"Fast\", \"Medium\", \"Slow\", \"API\", \"API\"],\n",
    "    \"Use Case\": [\"Development\", \"Experiments\", \"Research\", \"Production\", \"Production\", \"Production\"]\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "df_comparison = pd.DataFrame(model_comparison)\n",
    "print(\"Model Size Trade-offs:\")\n",
    "print(df_comparison.to_string(index=False))\n",
    "\n",
    "print(\"\\nGuidelines:\")\n",
    "print(\"- Start small for proof-of-concept\")\n",
    "print(\"- Scale up based on research needs\")\n",
    "print(\"- Consider computational budget\")\n",
    "print(\"- Evaluate quality vs. cost trade-offs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•ä Final Challenge: Design Your Research Pipeline\n",
    "\n",
    "Design a computational pipeline for a social science research question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def design_research_pipeline(research_question, num_conditions, samples_per_condition):\n",
    "    \"\"\"\n",
    "    Design a research pipeline for systematic text generation studies\n",
    "    \"\"\"\n",
    "    total_samples = num_conditions * samples_per_condition\n",
    "    \n",
    "    print(f\"Research Pipeline Design\")\n",
    "    print(f\"Research Question: {research_question}\")\n",
    "    print(f\"Number of conditions: {num_conditions}\")\n",
    "    print(f\"Samples per condition: {samples_per_condition}\")\n",
    "    print(f\"Total samples needed: {total_samples}\")\n",
    "    \n",
    "    # Estimate resources\n",
    "    estimate_costs(total_samples)\n",
    "    \n",
    "    print(f\"\\nPipeline Steps:\")\n",
    "    print(f\"1. Design prompt templates\")\n",
    "    print(f\"2. Generate systematic variations\")\n",
    "    print(f\"3. Batch process with rate limiting\")\n",
    "    print(f\"4. Quality control and filtering\")\n",
    "    print(f\"5. Analysis and interpretation\")\n",
    "    \n",
    "    return {\n",
    "        'total_samples': total_samples,\n",
    "        'estimated_time_hours': total_samples / 100,  # Rough estimate\n",
    "        'recommended_model': 'Gemma-2B' if total_samples < 1000 else 'API-based'\n",
    "    }\n",
    "\n",
    "# Example research design\n",
    "pipeline = design_research_pipeline(\n",
    "    research_question=\"How do language models represent different social groups in workplace contexts?\",\n",
    "    num_conditions=10,  # Different demographic groups\n",
    "    samples_per_condition=50  # Completions per group\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## ‚ùó Key Points\n",
    "\n",
    "* **Tool Selection**: Choose computational resources (local vs. cloud) based on research needs and constraints\n",
    "* **Tokenization**: Understanding how text becomes tokens is crucial for working with language models effectively\n",
    "* **Probability Control**: Temperature and other parameters control the creativity/randomness of generation\n",
    "* **Controlled Generation**: We can bias models toward specific outcomes for research purposes\n",
    "* **Practical Limits**: GPU memory, rate limits, and costs are real constraints in research\n",
    "* **Research Design**: Systematic approaches to text generation enable rigorous computational social science\n",
    "* **Bias Detection**: Language models can exhibit biases that researchers need to identify and account for\n",
    "* **Scalability**: Start small, validate approaches, then scale up based on research requirements\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "**For Your Research:**\n",
    "1. Identify specific research questions that could benefit from language model analysis\n",
    "2. Design small pilot studies to test approaches\n",
    "3. Consider ethical implications of generated content\n",
    "4. Plan for computational resources and costs\n",
    "\n",
    "**Technical Learning:**\n",
    "1. Explore fine-tuning models on domain-specific data\n",
    "2. Learn about prompt engineering techniques\n",
    "3. Study bias detection and mitigation methods\n",
    "4. Experiment with different model architectures\n",
    "\n",
    "**Resources:**\n",
    "- Hugging Face Transformers documentation\n",
    "- Google Colab tutorials and best practices\n",
    "- Academic papers on computational social science applications\n",
    "- Online courses on natural language processing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}